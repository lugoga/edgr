[
["index.html", "A glimpse of GIS and Remote Sensing with R Prerequisites", " A glimpse of GIS and Remote Sensing with R Masumbuko Semba 2019-06-10 Prerequisites This book assumes you have some knowledge of R. If you’ve never used R before, or need a refresher, start with our Introduction to R for Geospatial Data lesson.This book also assumes you have some knowledge of geospatial data types and common file formats. If you have never worked with geospatial data before, or need a refresher, start with our Introduction to Geospatial Concepts lesson. R and RStudio are the two main pieces of software that we are going to use. R is the programming language and RStudio is a modern IDE for it. You can use R without RStudio; but you cannot use RStudio without R. If you wish to install R and RStudio, Installation is simple, but operating system dependent. To download and install R for Windows, follow this link. For macOS, follow this one. If you run a GNU+Linux distribution, you can install R using the system’s package manager. On Ubuntu, install r-base. For RStudio, look for your operating system here. Packages are additional pieces of code that can be installed from within R with the install.packages() function. These packages extend R’s capabilities significantly, and are probably one of the main reasons R is so popular. We will install the packages that we need for this book in the coming chapters. "],
["sources.html", "Sources", " Sources https://geocompr.robinlovelace.net/adv-map.html#inset-maps https://mgimond.github.io/Spatial/good-map-making-tips.html https://r4ds.had.co.nz/graphics-for-communication.html "],
["note-to-the-reader.html", "Note to the reader", " Note to the reader This book is still being written. New content are still being added "],
["what-is-r.html", "What is R?", " What is R? R is programming language that focuses on applications in statistics and presenting the results in graphics. "],
["who-is-this-book-for.html", "Who is this book for?", " Who is this book for? Different field can benefit from using this book. "],
["intro.html", "Chapter 1 Introduction 1.1 Why learn computer Programming? 1.2 Learning to program 1.3 Programming in R", " Chapter 1 Introduction An increase in complexity and scale of environemental data both from satellite and insitu observations. This means scientists are increasingly required to develop data skills needed to design reproducible workflows for the simulation, collection, organization, pocessing, analysis and presentation of the results. However, to gain that skills requires some knowledge of coding using one or more computer languages. Coding also known as scripting, makes your work explicitly described, and transparent and completely reproducible. ## [conflicted] Will prefer dplyr::filter over any other package ## [conflicted] Will prefer dplyr::select over any other package 1.1 Why learn computer Programming? The benefit of learning to program are numerous as it fosters creativity, reasoning and ploblem solving. Programming makes data-analysis more efficient, accurate and transparent—opens new doors for new analyses that would not be practical or possible without programming. Most experiments for instance are often carried out on computers, so programming is vital for designing, creating and implementing experiments. The advent in computer technology plays an important role in research, and there are many kinds of proprietary softwared developed specifically to deal with data ana anlysis in research. For example most statistians and researchers analyse their data with using proprietary software like Microsoft Excel, SPSS, SAS and many others. These kind of canned software are widely used and useful. They are generally user-friendely with graphic user interface (GUI)1 require lillte or no programming knowledge, and you can complete analytical tasks in relatively small amount of time. Unfortunate the click-and drag interface you interact with in the GUI is a result of code hidden to user with these software. So you can only accomplish the taks based on how the software is program to do and incapable of doing some task that it was not designed for. You will sometimes find that the software is simply incapable of doing a particular kind of statistical anlysis or running a particular kind of experiment. At that juncture, it becomes apparent for researcher and scientists to learn programming, which help to solve solutions to unique and emerging research problems directly. Learning how to program and create your own code allows control over every detail of analyzing data. This level of control is invaluable for creating flexible and customizable data analysis, and for being confident in the output that decision makers depends. Another advantage of learning to program worth mentioning is time-saving. Becoming fluent in programming enables you to handle research data in relatively short time. This is because with programming language skills, you automate most of the tasks with scripts or code, which save you copious amounts of time in analysing your data and open new way of exploring and visualizing the data you are working on. This make you understand the data in a different way. Finally, computer programming is a valuable skill in general and may open doors in the larger workforce in this digital age. 1.2 Learning to program Learning programming is a skill and requires an initial investment. It takes practice, time, and effort. There is no easy way out. There are many layers to individual programming languages, and there are many programming languages out there to learn that could be useful. Befere you begin to learn to code, it is important to recognize that the underlying skill of computer programming is problem-solving—the ability to sovle new problem yourself with computer. From the perspective of applying computer programming techniques to solve problems in science and other fields, there are three major aspects. These are: Understaing the issues that need to be addressed Understanding the tools available at your disposal Applying the tools to solve the problem In most cases people know the issues and want to solve them problems, the biggest and vital componet that is often missing are the tools. Recognize the tools is one way but understand how to use the tools to the solve the proleme is the most difficult step. This is what they call the chicken and egg problem. What should you learn first? Should you learn about how organize codes at the beginning or after you have learn some basic aspects of the language? Should you learn random tidbits of programming languages before learning how to apply those tidbits to solve a problem? This is a dilemma for most of us, because we real unsure of the base and where to start. The major porting of this book will deal with the understanding the tool— the basic building blocks of any programming languages. Fortunate, most programming languages use the same basci concept forming the building bloacks for making all sort of programs. However, this book will specifically focus on R language. We believe that iving examples codes will make you understand the tools available to you and this has spill over effect as it will help you to: understand the conceptual tools write codes to implement the tools using the specific syntax2 to solve the problems or address the issues you are facing. Coding is writing a recipe for solving a problem. More specifically, it is writing the solution to a problem in a highly detailed manner that forces a computer to follow the directions to solve the problem. scientists have dubbed these code-based recipes algorithms3. Learning how to create algorithms involves first ldeterminging the problem and then converting the solution into an ordered steps that solve the problem. These ordered steps of instructions make use of the tools or building blocks of the programming language. 1.3 Programming in R R is an easy-to-learn programming language that has some reaaly useful feature for beginning programmer. The code is quite easy to read and it has an interactive console which you can enter your commands and see them run. R like any programming languages involves the same basic building blocks. Once you master the building block in R, you can switch to other languages with little difficulties. Because our goal is to learn how to code to analyze data and produce graphics, we will begin learning R, which is well-suited to this purpose. First, we will learn how to work with basic programming concepts in R , then we will learn how to handle and analyse data in R. 1.3.1 What is R? R is an open source programming language well suited for data analysis and visualization (R Core Team 2019). It is a powerful language used in mathematical operations, data-processing, analysis and graphical display of data. Like other statistical software, R provides a statistical framework and terminal–based interface for users to parse commands for data ingestion, manipulation and graphics. (???) pointed out that R is a vehicle for newly developing methods of interactive data analysis. R have rapidly developed and extended its capability to a large collection of packages provided by researchers and volunteers. R can be downloaded from https://www.r-project.org and available for all thre major operating systems—Windows, Mac and Unix/Linux. R is an interpreted language—the expression specified in this languages executes line by line similar to other languages like python or ruby rather than compiling the source code an executable chunk as in C++. One of the power of R language is its dynamic—infers the data types of the variables based on the context. We do not need to declare variables separately. R is considered as esoteric because its syntax are easily understood by people from different fields. The R programming usage has become one-stop solution to data analysis. R was created in 1993 and has evolved into a stable programming language. It has become a de facto standard for data analysis both in academic and industry sectors. R has its roots in the statistics community, being created by statistians for statistics. Many of its core tools are directed toward statistics. Being an open-source language, R has many advantage oover other commercial statistical platform like MATLAB, SAS and SPSS. The big rip for using R is its ecosystem of packages. 1.3.2 Installing R There is an active community of R developers which regularly releases a new version R. Each version has a name assigned to it. Except for major resease, the version of R are usually backward compatible in terms of their funcitonality. Installing R is fairly straightforward. R is available on most computing platform like Windows. Linux and Mac. It is highly recommended to install a pre–compiled bindary distribution for your operating system. Follow this instructions to install R Go to https://cran.r-project.org/ Click download latest version of R for Mac or Windows. For Windows users click Base and download the installer. For Mac users select the file R-3.x.x.pkg that aligns with yur OS version. After you download the installer, double-click its icon and the compiled binary executes and install R in your machine. Follow the instruction to install R in default location. Once you have installed R, you should now have an icon on your Windows or Mac desktop labelled R. If you are using Ubuntu, in the Applications menu, you should see a new group named Programming with the application R. After the installation of R, the software is launched by Double-click the icon and you should see a window that look similar to figure 1.1 popup. The default desktop layout include the console, which provide the interface between the software and the user, i.e., it accepts R commands typed after the prompt sign &gt;. Figure 1.1: The graphical user interface of R Let’s enter some commands at the prompt, beggining with the following: print(\"Programming is fun!, sometimes challenging and occasionnaly frustrating\"). Make sure the text you typed inside the print() are either double or single quoted. Press Enter on your keyboard when you’ve finished typing the line. print(&quot;Programming is fun!, sometimes challenging and occasionnaly frustrating&quot;) ## [1] &quot;Programming is fun!, sometimes challenging and occasionnaly frustrating&quot; If you have entered the text correctly, the word should appear in the console and the prompt should reappear to let you know that the console is ready to accept more commands. The version and common packages used in this book can be accessed by simply passing a commands devtools::session_info() in the console. you can also check the version of R installed in your computer with the base version() function version _ platform x86_64-w64-mingw32 arch x86_64 os mingw32 system x86_64, mingw32 status major 3 minor 5.3 year 2019 month 03 day 11 svn rev 76217 language R version.string R version 3.5.3 (2019-03-11) nickname Great Truth By default, R stores all of your R–related files in the startup folder named R. You can check the path of the current working directory with the getwd() function. getwd() ## [1] &quot;E:/bookdown/spatil_r&quot; Alternatively, you can create a personal working directory in which to read your files into R and also to export the files generated from processes in R. You should then define the path of the working directory using the setwd() function. setwd(&quot;E:/bookdown/spatil_r/&quot;) To see which files are in the working directory that you just defined or the default R working directory use the list.files() function list.files() 1.3.3 What is RStudio? You have seen that R comes with a console that allows to program interactively. However, you need to install RStudio— an integrated development environment (IDE) for R programming. RStudio makes writing R codes easier and in a more user-friendly environment (???). RStudio is a free and open-source integrated development environment (IDE) for R (???). You must have already installed R in your machine before you install Rstudio. You can download the software from the R-studio website at this link http:/www.rstudio.com. Rstudio work in the four operating systems—Windows, Mac, Ubuntu and Fedora. To obtain the reference infomation for citing RStudio in publication by simply typing RStudio.version() command in the console. 1.3.4 Basic Features of RStudio After the installation is complete, open Rstudio and a window like the one shown in figure (Figure 1.2 should open. Across the top is a standard menu bar with typicall menu items. Below the standard menu, the program is divided into different panels. The four fundamental panels in Rstudio that we will look are editor, console, worksapce and plots. Figure 1.2: The graphical user interface of RStudio showing the four main windows source editor and data viewer: The top left corner contains the script editor. This a simple text editor for writing and editing R scripts and markdown documents. Several tabs can be opened in the editor window at once, with each tab representing a different document. These files can be saved from the editor into the working directory. The editor window is also used to view data frame similar to Excel spreadsheet. Source editor is recommended in most programming work because of its efficient way to manage scripts and Rmarkdown document for reproducible work. environment (workspace): The top right panel contains several tabs but I introduce the workspace and history tabs. The workspace list all the objects that are currently loaded in R’s memory. You can use the ls() function to check the list all files in the environment.You can also view the object in the workspace (environment) by clicking them or run View() function in console. history tab provides a record of the recent commands executed in the console, scripts or rmarkdown documents. R consoles: The bottom left window is the command line widely known as console. this window is similar to console in base R. This is used to directly enter commands into R. There are only few thing you need to know to become familiar with console. Click in the console and you should see a blicking caretappear next to the greater than sign &gt;, reffered as prompt. This is where you will type R’comands to executes one command at a time. For example, in a console type the 2+3 and click enter; 7+8 ## [1] 15 Once you have entered commands here, press enter to execute the command and you will get the answer 25. The console is useful for entering single lines of code and running them. Oftentime this occurs when you are exploring or testing a certain task with a combinations of functions and objects. But because coding is all about creating scripts that automate the analytical process, you will find that very rare you use the the console but rather write the code in script or markdown files. files, plots, packages, help, and viewer: The bottom-right window has five tabs for files, plots, package, help and viewer. The files tab allows browsing of the computers file directory. The plots tab will show recent plots and figures drawn with graphic functions in R. The packages tabl list all packages installed in R and provide tools for download new package and update packages. The help tab is an invaluable tab, it help to search for functions and see examples of how they are used. The viewer tab is like the plot tab that both are used to show recent plots and figures. However, while the plots is for static plot, the viewer is for animations and interactive plots and figures. An important concept in R is the current working directory. This is the file folder that R points to by default. By default the software stores all of your R-related files in the startup folder, which is the Document folder. Alternatively, you can create a personal working directory in which to store your R-related files with the setwd() function. This is especially important when reading in data to R. The current working directory should be set to the folder containing the data to be loaded into R. 1.3.5 Rstudio editor You should be making use of Rstudio’s text edditor to code in R. It is important to practice writing good code that are easy to read and understand what they mean. In short make sure your code is easy to read and understand what it does. This will help you understand your own code later, and help other people understand your code when shared or when they ask you for a help. 1.3.6 Rmarkdown Rstudio’s text editor allows to create markdown files called rmarkdown. Markdown is a plain text that combine codes and text and ability to ouput different format like word, PDF and HTML format. This ability of combining text and code has made rmarkdown a powerful markup language in R. In general, with rmarkdown you can write document with titles, headings, paragraphs, insert figures, tables and equations. Another useful feature of rmarkdown document is the option to publish R projects in various file format such as HTML, Latex, PDF EPUB, Word and many others. This feature enables you to share your results with colleagues who may or may not have R software. The published document includes formated plain text in section, heading and paragraphs that comes with code chunk, figures and tables. 1.3.7 Use descriptive names While programming, you will often declare and assign names of the variables. R usually will not care what name you give to any variables. But declare a descriptive name is informative because you easily remember what it contains. Therefore, its is encouraged to give names that represent the meaning of the data stored in each variable. 1.3.8 Use comments While working with R script, the hash tag # set a comment and tell the script to skip the line with the hash tag. In rmarkdown, the hash tag are used to set headings and therefore you can specify the comment with hash tage in the chunk code. Comments are useful as they help you and other people about a particular code and what particular aspect it does. Therefore, it is always recommend to write clear and precise comments that can help you in the future when you want to understand what the code does and also can help others follows with little consultation if shared. 1.3.9 Getting Help in R Often, we get stuck whilee doing some analysis as either we do not know the correct function to use or its syntax. Its important for anyone who is new to R to knwo the right place to look for help. There are two ways to get help in R—built in help system and online. 1.3.9.1 Built in system R has tons of tools to get you up and run with R base functions, these include help.start() function that Start the hypertext (currently HTML) version of R’s online documentation. ?help.start() help() is the primary interface to access documentation of functions and datasets. A shortcut for help() is the ? help(&quot;metR::Adiabat&quot;) ?metR::Adiabat help.search() allows for searching the help system for documentation and return those matching the searched itme. The shortcut ofr help.search() function is ?? help.search(&quot;geom_contour&quot;) ??ggplot2::geom_abline demo is an interactive platform that demonstrate certain topics provided in R packages. Typing demo() in the chunk or console gives the list of available topics. demo() example() run the code and displays example of the specified topic that is available example(&quot;t.test&quot;) ## ## t.test&gt; require(graphics) ## ## t.test&gt; t.test(1:10, y = c(7:20)) # P = .00001855 ## ## Welch Two Sample t-test ## ## data: 1:10 and c(7:20) ## t = -5.4349, df = 21.982, p-value = 1.855e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -11.052802 -4.947198 ## sample estimates: ## mean of x mean of y ## 5.5 13.5 ## ## ## t.test&gt; t.test(1:10, y = c(7:20, 200)) # P = .1245 -- NOT significant anymore ## ## Welch Two Sample t-test ## ## data: 1:10 and c(7:20, 200) ## t = -1.6329, df = 14.165, p-value = 0.1245 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -47.242900 6.376233 ## sample estimates: ## mean of x mean of y ## 5.50000 25.93333 ## ## ## t.test&gt; ## Classical example: Student&#39;s sleep data ## t.test&gt; plot(extra ~ group, data = sleep) ## ## t.test&gt; ## Traditional interface ## t.test&gt; with(sleep, t.test(extra[group == 1], extra[group == 2])) ## ## Welch Two Sample t-test ## ## data: extra[group == 1] and extra[group == 2] ## t = -1.8608, df = 17.776, p-value = 0.07939 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.3654832 0.2054832 ## sample estimates: ## mean of x mean of y ## 0.75 2.33 ## ## ## t.test&gt; ## Formula interface ## t.test&gt; t.test(extra ~ group, data = sleep) ## ## Welch Two Sample t-test ## ## data: extra by group ## t = -1.8608, df = 17.776, p-value = 0.07939 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.3654832 0.2054832 ## sample estimates: ## mean in group 1 mean in group 2 ## 0.75 2.33 1.3.9.2 Package documentation Add on package in R can be accessed with different tools below; library allow access of the documentation of a package . Note that the package must be installed in your machine to access its documentation library(help = &quot;metR&quot;) vignette allows to view a specific package vignette, or list the available ones. All available vignette in the local machine can be accessed by writing vignette(). Note that this can tak a long time ?vignette() You can list vignettes from all attached packages in your working directory as; vignette(all = FALSE) And to access a vignette of a specific package, you simply type vignette( package = &quot;sf&quot;) You notice that the package sf has more than one vignette 1.3.9.3 Online Resources R bloggers contains several post covering on different topics in R. This is the place you should not miss to visi if you want help on topic such as R, data analysis, data visualization, data wrangling and machine learning. You can learn more about this platform here Stack overflow is very resourful place to obtain answers of some code or package you are facing challenges. There is high changes that some one else with similar challenges has already presented the matter and you can benefit from the answers provided. Twitter There is an active R commnity on Twitter comprised of experts who have dedicated their time and resources to help others. Use the hashtag #rstats if you are asking for help or guidance on Twitter. Rstudio community resemble stack overflow but differs in the sense you simply drop a questions related to Rstudio packages and services r4ds Is an online system that promote community to learn data science using R. You can learn more about the communit here Rstudio Resources offers learning resources like cheatsheets, webinars and blogs that deals with matter relating to R and Rstudio packages Reddit is another platform that help you find answers to the challenges you face in R Future learn coursera linkedIn udemy R weekly provide updates about R community. You can get information of new packages, blogs, conferences, workshops, tutorials and jobs References "],
["get-started.html", "Chapter 2 Getting started 2.1 Maths in R 2.2 Assignment operator 2.3 Setting Working Directory 2.4 Packages or Libraries 2.5 Understanding Data in R 2.6 Data Frame 2.7 tibbles 2.8 Exercise 2.9 Exercise", " Chapter 2 Getting started ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::filter over any other package ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::select over any other package In chapter 1 you installed and got a glimpse of R and Rstudio program. In this chapter you begin to do something with them. We will begin with some simple calculations and then move on to variables. We then move to functions and later to packages. We will learn the basic data types that are widely used in R and how to construct them This chapter provides example of foundational programming concepts in R. These include the basic tasks like importing data in R, manipulating data, visualizing the data and conduct explatory and inferencial statistics. These basics will provide building blocks for handling data, analysing data and make plots with R. In the examples, R codes are presented in the light gray chunk blocks. Inside these chunk blocks, lines that begin with two number signs (##) are the outputs of the preceding lines of codes that have been executed and lines without the number signs are are the code that generated the output. 2.1 Maths in R Before proceeding, we need to clear the workspace by typing; rm(list = ls()) Clearing the workspace before you begin a new project is an important task in R because it avoid name conflicts with previous projects. We can also clear figures from the plot windows by simply typing; graphics.off() Creating a vector files of numerical number of strings in R is easy. To combine more than one element in a vector simply use the c function, which stands for concatenate. age = c(45,75,65,85,45,32,57,65,52,45,65,32,89,45,66) which first assign age as the name of the object, then combine the list of elements in paretheses with c function. 2.1.1 R operators R provides standard arithmetic operators for addition , substraction, multiplication, division, and exponential. The basic symbols used by R to perform mathematical operations are called operators. Some of the mathematical operators are listed in table 2.1. Note that the forward slash (/) is used for division because it’s similar to the division line that you would use when writing a fraction. require(magrittr) ## Loading required package: magrittr ## ## Attaching package: &#39;magrittr&#39; ## The following objects are masked from &#39;package:testthat&#39;: ## ## equals, is_less_than, not data.frame(Symbol = c(&quot;+&quot;, &quot;-&quot;, &quot;*&quot;, &quot;/&quot;), Operation = c(&quot;Addition&quot;, &quot;Substraction&quot;,&quot;Multiplication&quot;,&quot;Division&quot;)) %&gt;% knitr::kable(format = &quot;html&quot;, align = &quot;l&quot;, caption = &quot;Basic R operators&quot;) Table 2.1: Basic R operators Symbol Operation Addition Substraction Multiplication / Division Because of the convinient, we will use Rstudio console, just write an expression 2 + 3 and click Enter 2+3 ## [1] 5 As we expected, R returns the answer as 5. Unlike other programming languages, coding in R does not need to terminate the expression or lines with a semicolon. 10-4 ## [1] 6 23*2 ## [1] 46 8/2 ## [1] 4 5^2 ## [1] 25 5%%3 ## [1] 2 5%%5 ## [1] 0 2.1.2 Precedence R can be used to express complicated mathematical formulars. For anyone unfamiliar with writing formulas on computers, it is important to recognize that R will make assumptions about which part of the formular to compute first. This order of operations is reffered as predecence. Multiplication and division have a higher order than addition and substraction. For example guess what is the answer for the mathematical expression in the chunk below. 5 - 3 + 8 / 2 * 3 The answer should be 14. According precedence, the number 8 is first divided by 2 to get 4 as product which is multiplied by 3 to obtain 12 and then 5 is added to get 17, which is substracted by 3 to get 14 as final result. We use parentheses in a R to control the order of operations like in the expression below. Can you make a mental calcuation and provide the answer before you click enter for the expression in the chunk below? 5 - (3 + 8) / 2 * 3 The result of the expression is -11.5 and not 14 because the parentheses tell R to do the operation in the parentheses firs and then do the operation outside the parentheses. 2.2 Assignment operator We often use an assignment operator to assing the value of an expression to a variable. R has two assignment operators—the conventional assignment operator =, which is present in most programming languages, and the arrows &lt;- and -&gt; which are specific to R. The expression x = 5 assign the value 5 to x, likewise the expression x &lt;- 5 and 5 -&gt; x have the same effect. Throughout this book, we stick on the conventional assignment operator (=) 2.2.1 Variables We can create expression using variables. for instance, we assign the value 5 to the variable x and evaluate the square of x using the exponential ^ operator. x = 5 x^2 ## [1] 25 R has many types of variables that store different kinds of data in different ways. Example you can store a list of numbers or text days= c(&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;) speed = c(128, 158, 89) R has peculiar syntax when it comes to variable names. The dot character . has a completely different meaning as compared to other programming languages. In R, we can use . in the variable names, so x.1 and x_1 are perfectly valid. In practice, the dot operator is used as a visual separator in variable names, similar to underscore in most other programming languages. 2.2.2 Naming conventions Name must begin with a letter. Do not use number, dollar sign or underscore R is case sensitive; average and Average are two different objects in R Use descriptive names if the name is made of more than one word, used period (. or underscore to separate the words) 2.2.3 Functions Functions in R are first class objects, which means that they can be treated much like any other R object. Importantly, Functions can be passed as arguments to other functions Functions can be nested, so that you can define a function inside of another function The return value of a function is the last expression in the function body to be evaluated. R functions arguments can be matched positionally or by name. So the following calls to sd are all equivalent data = rnorm(25) sum(data) ## [1] 4.692747 sd(data) ## [1] 1.188329 mean(data) ## [1] 0.1877099 median(data) ## [1] 0.1583888 2.2.4 Essential functions R has many built-in functions that can be used fo a great variety of tasks. These can be suplemented with packages, which contains more functions bundled in one document. Here is a list of common and widely used functions + rep() — repeates a value some number of times to make a list + seq() — creates a sequence of values between a start and end number and spaced at certain interval + aggregate() — used to bin data by condition + table() — used to summarise categorical data + plot() — graphical plots of data + hist() — a function for plotting a histogram + boxplot() — a function for plotting boxplot + mean() — compute arithmetric mean + sd() — compute arithmetric standard deviation + sum() — compute the total of the set of elements + length() — function to count the number of elements in a vector 2.3 Setting Working Directory (???) defined a working directory as a folder in your computer or server where you stores the raw data, codes and output for that specific project. This folder is important in programming because it allows to read the data and write outputs to this working directory. In R you can set working directory with setwd() function and check whether you are in the right working directory with the getwd() function. getwd() setwd(&quot;./Data Manipulation/R_dege/&quot;) 2.4 Packages or Libraries R is made up of many user-written package. The base version of R allows user to get started in R, but the capabilities of base R are limited and additional packages are required for smooth performance of working with data. packages are collections of R functions, data, and compiled code in a well-defined format. A package bundles together code, data, documentation and tests and provide ana easy method to share with others. Until November 2018, there were 1300+ packages available for download on CRAN and countless more avaialble trought GitHub. The huge number of package has made R so successful and the chance is that some one has already created a package that can solve the problem you about to tackle and you can benefit from their work by downloading their package. 2.4.1 Installing packages R comes with a standard set of packages. Others are available for download and installation. The primary of stable package is the CRAN. In R you can install a package from CRAN with an install.packages(\"packagename\") function that allows you to install the package you want to use into R. In have already installed the ggplot2 packages in my machine, so if you want to install in your machine you can simply uncomment the chunk below by removing the hash tag (#) ## install.packages(&quot;ggplot2&quot;) ## install.packages(&quot;dplyr&quot;) ## install.packages(&quot;lubridate&quot;) ## install.packages(&quot;factoextra&quot;) ## install.packages(&quot;readxl&quot;) ## install.packages(&quot;kableextra&quot;) ## install.packages(&quot;haven&quot;) ## install.packages(&quot;readr&quot;) 2.4.2 Loading packages Once package is downloaded and installed in your computer, you have to them into the session to access its functions and resources of the package. Yu can load the packages you want ot use with ether library() or required() function. require(dplyr) library(readr) require(lubridate) library(readxl) require(haven) library(ggplot2) require(kableExtra) 2.5 Understanding Data in R Clearing the workspace is always recommended before working on a new R project to avoid name conflicts with provious projects. We can also clear all figures using graphics.off()' function and clear the console with a combinantion ofCTRL+L`. It is a good code practise that a new R project start with the code in the chunk below: rm(list = ls()) graphics.off() 2.5.1 Data Types R is a flexible language that allows to work with different kind of data format (???). This inluced integer, numeric, character, complex, dates and logical. The default data type or class in R is double precision—numeric. In a nutshell, R treats all kind of data into five categories but we deal with only four in this book. Before proceeding, we need to clear the workspace by typing rm(list = ls()) after the prompt in the in a console. Integers:Integer values do not have decimal places. They are commonly used for counting or indexing. aa = c(20,68,78,50) You can check if the data is integer with is.integer() and can convert numeric value to an integer with as.integer() is.integer(aa) ## [1] FALSE You can query the class of the object with the class() to know the class of the object class(aa) ## [1] &quot;numeric&quot; Although the object bb is integer as confirmed with as.integer() function, the class() ouput the answer as numeric. This is because the defaul type of number in r is numeric. However, you can use the function as.integer() to convert numeric value to integer class(as.integer(aa)) ## [1] &quot;integer&quot; Numeric: The numeric class holds the set of real numbers — decimal place numbers. The numeric class is more general than the integer class, and inclused the integer numbers. These could be any number (whole or decimal number). You can check if the data is integer with is.integer() bb = c(12.5, 45.68, 2.65) class(bb) ## [1] &quot;numeric&quot; is.numeric(bb) ## [1] TRUE Strings: These collection of characters. This often are text data like names. You can check if the data is integer with is.character() kata = c(&quot;Dege&quot;, &quot;Mchikichini&quot;, &quot;Mwembe Mdogo&quot;, &quot;Cheka&quot;) class(kata) ## [1] &quot;character&quot; Factor: These are strings from finite set of values. For example, we might wish to store a variable that records gender of people. You can check if the data is factor with is.factor() and use as.factor() to convert string to factor sex = c(&quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;) sex = as.factor(sex) class(sex) ## [1] &quot;factor&quot; levels(sex) ## [1] &quot;Female&quot; &quot;Male&quot; Often times we need to know the possible groups that are in the factor data. This can be achieved with the levels() function levels(sex) ## [1] &quot;Female&quot; &quot;Male&quot; levels(kata) ## NULL Often we wish to take a continuous numerical vector and transform it into a factor. The function cut() takes a vector of numerical data and creates a factor based on your give cut-points. Let us make a fictional income of 508 people with rnorm() function. income = rnorm(n = 508, mean = 500, sd = 80) hist(income, col = &quot;green&quot;, main = &quot;&quot;, las = 1, xlab = &quot;Individual Income&quot;) Figure 2.1: Income distribution #mosaic::plotDist(dist = &quot;norm&quot;, mean = 500, sd = 80) We can now breaks the distribution into groups and make a simple plot as shown in figure 2.2, where those with income less than 400 were about 50, followed with a group with income range between 400 and 500 of about 200 and 250 people receive income above 500 group = cut(income, breaks = c(300,400,500,800), labels = c(&quot;Below 400&quot;, &quot;400-500&quot;, &quot;Above 500&quot;)) is.factor(group) ## [1] TRUE levels(group) ## [1] &quot;Below 400&quot; &quot;400-500&quot; &quot;Above 500&quot; barplot(table(group), las = 1, horiz = FALSE, col = c(&quot;blue&quot;, &quot;red&quot;, &quot;blue&quot;), ylab = &quot;Frequency&quot;, xlab = &quot;Group of Income&quot;) Figure 2.2: Barplot of grouped income data = data.frame(group, income) Logicals: This is a special case of a factor that can only take on the values TRUE and FALSE. R is case-sensitive, therefore you must always capitalize TRUE and FALSE in function in R. Date and time 2.5.2 Vectors Ofen times we want to store a set of numbers in once place. One way to do this is using the vectors in R. Vectors store severl numbers– a set of numbers in one container. let us look on the example below id = c(1,2,3,4,5) people = c(158,659,782,659,759) street = c(&quot;Dege&quot;, &quot;Mchikichini&quot;, &quot;Mwembe Mdogo&quot;, &quot;Mwongozo&quot;, &quot;Cheka&quot;) Notice that the c() function, which is short for concatenate wraps the list of numbers. The c() function combines all numbers together into one container. Notice also that all the individual numbers are separated with a comma. The comma is reffered to an an item-delimiter. It allows R to hold each of the numbers separately. This is vital as without the item-delimiter, R will treat a vector as one big, unsperated number. 2.5.3 Indexing the element One advantage of vector is that you can extract individual element in the vector object by indexing, which is accomplished using the square bracket as illustrated below. id[5] ## [1] 5 people[5] ## [1] 759 street[5] ## [1] &quot;Cheka&quot; Apart from extracting single element, indexing allows to extract a range of element in a vector. This is extremely important because it allows to subset a portion of data in a vector. A colon operator is used to extract a range of data street[2:4] ## [1] &quot;Mchikichini&quot; &quot;Mwembe Mdogo&quot; &quot;Mwongozo&quot; 2.5.4 Adding and Replacing an element in a vector It is possible to add element of an axisting vecor. Here ia an example id[6] = 6 people[6] = 578 street[6] = &quot;Mwongozo&quot; Sometimes you may need to replace an element from a vector, this can be achieved with indexing people[1] = 750 2.5.5 Number of elements in a vector Sometimes you may have a long vector and want to know the numbers of elements in the object. R has length() function that allows you to query the vector and print the answer length(people) ## [1] 6 2.6 Data Frame data.frame is very much like a simple Excel spreadsheet where each column represents a variable type and each row represent observations. Perhaps the easiest way to create a data frame is to parse vectors in data.frame() function. # create vectors Name = c(&#39;Bob&#39;,&#39;Jeff&#39;,&#39;Mary&#39;) Score = c(90, 75, 92) dt = data.frame(Name, Score) Table 2.2 show the the data frame created by fusing the two vectors together. Table 2.2: Variables in the data frame Name Score Bob 90 Jeff 75 Mary 92 Because the columns have meaning and we have given them column names, it is desirable to want to access an element by the name of the column as opposed to the column number.In large Excel spreadsheets I often get annoyed trying to remember which column something was. The $sign and []are used in R to select variable from the data frame. dt$Name ## [1] Bob Jeff Mary ## Levels: Bob Jeff Mary dt[,1] ## [1] Bob Jeff Mary ## Levels: Bob Jeff Mary dt$Score ## [1] 90 75 92 dt[,2] ## [1] 90 75 92 R has build in dataset that we can use for illustration. For example, (???) created a longley dataset, which is data frame with 7 economic variables observed every year from 1947 ti 1962 (Table 2.3). We can add the data in the workspace with data() function data(longley) longley %&gt;% kable(caption = &quot;Longleys&#39; Economic dataset&quot;, align = &quot;c&quot;, row.names = F) %&gt;% column_spec(1:7, width = &quot;3cm&quot;) Table 2.3: Longleys’ Economic dataset GNP.deflator GNP Unemployed Armed.Forces Population Year Employed 83.0 234.289 235.6 159.0 107.608 1947 60.323 88.5 259.426 232.5 145.6 108.632 1948 61.122 88.2 258.054 368.2 161.6 109.773 1949 60.171 89.5 284.599 335.1 165.0 110.929 1950 61.187 96.2 328.975 209.9 309.9 112.075 1951 63.221 98.1 346.999 193.2 359.4 113.270 1952 63.639 99.0 365.385 187.0 354.7 115.094 1953 64.989 100.0 363.112 357.8 335.0 116.219 1954 63.761 101.2 397.469 290.4 304.8 117.388 1955 66.019 104.6 419.180 282.2 285.7 118.734 1956 67.857 108.4 442.769 293.6 279.8 120.445 1957 68.169 110.8 444.546 468.1 263.7 121.950 1958 66.513 112.6 482.704 381.3 255.2 123.366 1959 68.655 114.2 502.601 393.1 251.4 125.368 1960 69.564 115.7 518.173 480.6 257.2 127.852 1961 69.331 116.9 554.894 400.7 282.7 130.081 1962 70.551 Sometimes you may need to create set of values and store them in vectors, then combine the vectors into a data frame. Let us see how this can be done. First create three vectors. One contains id for ten individuals, the second vector hold the time each individual signed in the attendane book and the third vector is the distance of each individual from office. We can concatenate the set of values to make vectors. id = c(1,2,3,4,5,6,7,8,9,10) time = lubridate::ymd_hms(c(&quot;2018-11-20 06:35:25 EAT&quot;, &quot;2018-11-20 06:52:05 EAT&quot;, &quot;2018-11-20 07:08:45 EAT&quot;, &quot;2018-11-20 07:25:25 EAT&quot;, &quot;2018-11-20 07:42:05 EAT&quot;, &quot;2018-11-20 07:58:45 EAT&quot;, &quot;2018-11-20 08:15:25 EAT&quot;, &quot;2018-11-20 08:32:05 EAT&quot;, &quot;2018-11-20 08:48:45 EAT&quot;, &quot;2018-11-20 09:05:25 EAT&quot;), tz = &quot;&quot;) distance = c(20, 85, 45, 69, 42, 52, 6, 45, 36, 7) Once we have the vectors that have the same length dimension, we can use the function data.frame() to combine the the three vectors into one data frame shown in table 2.4 arrival = data.frame(id, time, distance) Table 2.4: The time employees enter into the office with the distance from their residential areas to the office IDs Time Distance 1 2018-11-20 06:35:25 20 2 2018-11-20 06:52:05 85 3 2018-11-20 07:08:45 45 4 2018-11-20 07:25:25 69 5 2018-11-20 07:42:05 42 6 2018-11-20 07:58:45 52 7 2018-11-20 08:15:25 6 8 2018-11-20 08:32:05 45 9 2018-11-20 08:48:45 36 10 2018-11-20 09:05:25 7 2.7 tibbles A tibble is a modern data.frame, which keep keep what has proven to be effective and throw out what is not. Tibbles are data.frames that are llazy and surly—they do not change variable names or types and never do partial matching. Tibbles have an enhanced print_method, which makes them easier to use with large dataset containing diffeent objects with large number of variables. A tibble is created with tibble() function from tibble package (Müller and Wickham 2019) data.tb = tibble::tibble(time, distance) tibble::is_tibble(data.tb);class(data.tb) ## [1] TRUE ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; You can also use the as_tibble() to convert data.frame to tibble ## create a data.frame data.df = data.frame(time, distance) ## create a tibble data.tbl = tibble::as_tibble(data.df) ## printout tibble::is_tibble(data.df); tibble::is_tibble(data.tb) ## [1] FALSE ## [1] TRUE 2.7.1 Column Names Unlike data.frame that is strict on column names, tibble need not be valid R variable names. They can contain unusual characters like a space or a smiley but be enclosed in ticks. tibble::tibble(id = 1:10, &quot;:&quot; = rep(c(&quot;NE&quot;, &quot;SE&quot;), each = 5), &quot;-&quot; = rnorm(10, 25,2), chl = rnorm(10,.25,.01)) ## # A tibble: 10 x 4 ## id `:` `-` chl ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 NE 27.0 0.252 ## 2 2 NE 26.0 0.241 ## 3 3 NE 27.9 0.259 ## 4 4 NE 23.3 0.248 ## 5 5 NE 26.9 0.265 ## 6 6 SE 27.7 0.249 ## 7 7 SE 22.1 0.234 ## 8 8 SE 28.1 0.253 ## 9 9 SE 24.2 0.233 ## 10 10 SE 24.0 0.243 However, when we use data.frame()instead of tibble() with the same arguments, we notice that the data.frame() function has modified the column names. data.frame(id = 1:10, &quot;:&quot; = rep(c(&quot;NE&quot;, &quot;SE&quot;), each = 5), &quot;-&quot; = rnorm(10, 25,2), chl = rnorm(10,.25,.01)) ## id X. X..1 chl ## 1 1 NE 29.88740 0.2439263 ## 2 2 NE 24.24104 0.2598289 ## 3 3 NE 28.40826 0.2487499 ## 4 4 NE 23.94467 0.2414008 ## 5 5 NE 24.32014 0.2318529 ## 6 6 SE 26.16394 0.2545717 ## 7 7 SE 24.41430 0.2538367 ## 8 8 SE 27.20533 0.2528488 ## 9 9 SE 23.76710 0.2521412 ## 10 10 SE 22.90105 0.2294084 2.7.2 Add rows and columns tibble allows you to add column and rows. Let us add the unique ID in the data.tb we created earlier using add_column() function. This function allows us to specify wher we want to place the column we will create. for this case we want the id variable to be the first, hence we specify .before = 1 argument. tibble::add_column(.data = data.tb, id = 1:10, .before = 1) ## # A tibble: 10 x 3 ## id time distance ## &lt;int&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 1 2018-11-20 06:35:25 20 ## 2 2 2018-11-20 06:52:05 85 ## 3 3 2018-11-20 07:08:45 45 ## 4 4 2018-11-20 07:25:25 69 ## 5 5 2018-11-20 07:42:05 42 ## 6 6 2018-11-20 07:58:45 52 ## 7 7 2018-11-20 08:15:25 6 ## 8 8 2018-11-20 08:32:05 45 ## 9 9 2018-11-20 08:48:45 36 ## 10 10 2018-11-20 09:05:25 7 We can also add row(s) in tibble with the add_row() function and specify at which row location we want to insert the rows. For instance, in the code below, we command the rows to be inserted from row 5 tibble::add_row(.data = data.tb, time = seq(lubridate::dmy(120514),lubridate::dmy(150514), by = &quot;day&quot;), distance = rnorm(4, 23,6), .before = 5) ## # A tibble: 14 x 2 ## time distance ## &lt;dttm&gt; &lt;dbl&gt; ## 1 2018-11-20 06:35:25 20 ## 2 2018-11-20 06:52:05 85 ## 3 2018-11-20 07:08:45 45 ## 4 2018-11-20 07:25:25 69 ## 5 2014-05-12 03:00:00 25.9 ## 6 2014-05-13 03:00:00 21.4 ## 7 2014-05-14 03:00:00 20.3 ## 8 2014-05-15 03:00:00 24.7 ## 9 2018-11-20 07:42:05 42 ## 10 2018-11-20 07:58:45 52 ## 11 2018-11-20 08:15:25 6 ## 12 2018-11-20 08:32:05 45 ## 13 2018-11-20 08:48:45 36 ## 14 2018-11-20 09:05:25 7 2.8 Exercise Create a vector of character strings with six elements test &lt;- c(&#39;red&#39;,&#39;red&#39;,&#39;blue&#39;,&#39;yellow&#39;,&#39;blue&#39;,&#39;green&#39;) and then Transform the test vector just you created into a factor. Use the levels() command to determine the levels (and order) of the factor you just created. Transform the factor you just created into integers. Comment on the relationship between the integers and the order of the levels you found in part (b). Use some sort of comparison to create a vector that identifies which factor elements are the red group. Suppose we vectors that give a students name, their GPA, and their major. We want to come up with a list of forestry students with a GPA of greater than 3.0. Name &lt;- c(&#39;Adam&#39;,&#39;Benjamin&#39;,&#39;Caleb&#39;,&#39;Daniel&#39;,&#39;Ephriam&#39;, &#39;Frank&#39;,&#39;Gideon&#39;) GPA &lt;- c(3.2, 3.8, 2.6, 2.3, 3.4, 3.7, 4.0) Major &lt;- c(&#39;Math&#39;,&#39;Forestry&#39;,&#39;Biology&#39;,&#39;Forestry&#39;,&#39;Forestry&#39;,&#39;Math&#39;,&#39;Forestry&#39;) Create a vector of TRUE/FALSE values that indicate whether the students GPA is greater than 3.0. Create a vector of TRUE/FALSE values that indicate whether the students’ major is forestry. Create a vector of TRUE/FALSE values that indicates if a student has a GPA greater than 3.0 and is a forestry major. Convert the vector of TRUE/FALSE values in part (c) to integer values using the as.numeric() function. Which numeric value corresponds to TRUE? Sum (using the sum() function) the vector you created to count the number of students with GPA &gt; 3.0 and are a forestry major. 2.9 Exercise Create a data.frame named my.trees that has the following columns: Girth = c(8.3, 8.6, 8.8, 10.5, 10.7, 10.8, 11.0) Height= c(70, 65, 63, 72, 81, 83, 66) Volume= c(10.3, 10.3, 10.2, 16.4, 18.8, 19.7, 15.6) Extract the third observation (i.e. the third row) Extract the Girth column referring to it by name (don’t use whatever order you placed the columns in). Print out a data frame of all the observations except for the fourth observation. (i.e. Remove the fourth observation/row.) References "],
["get-familiar-with-r-markdown.html", "Chapter 3 Get familiar with R Markdown 3.1 R Markdown 3.2 Text Formating 3.3 Hyperlinks 3.4 Images 3.5 Tables 3.6 Cross-referencing 3.7 Bibliographies 3.8 Create .Rmd FIle 3.9 Rendering Markdown 3.10 Structure of an Rmarkdown FIle 3.11 Inserting code chunk 3.12 Chunk name 3.13 Chunk options 3.14 Inline code", " Chapter 3 Get familiar with R Markdown One of the key task of scientist is communicate your analysis and result to the different group of people. The typical data analysis workflow looks like this: you go out and collect data and you organize it in a file or spreadsheet or database. Then interact with R using scripts to run some analyses, perhaps saving some intermediate results along the way or maybe always working on the raw data. You visualize and create some plots or tables of relevant summaries of the data. And then you go and write a report about the results in a text editor or word processor and fine tune report or document. To accomplish these task you must use various tools. That era has gone and in R there are several ways of writing documents that are used both as automated analysis scripts as well as for generating reports. The most popular of these approaches is R Markdown—for writing these documents) and knitr—or running the analysis and generating the reports. The two packages can generate report that has code, results, plots and text in various format. This chapter introduces R Markdown, a simple programming syntax that can be used to describe text formatting and structure by adding special characters to the text. Being comfortable with this simple syntax to describe text rendering will help you document your code. 3.1 R Markdown In interactive data analysis, the code and not the report or presentation that is the source of the results. Therefore, the document should also be based on code. This can be accomplished with R Markdown, which produces documents that are generated by code, reproducible and easy to maintain. R Markdown is a file format for easy–to–write dynamic documents from plain text. It provide a unified authoring framework—combining code alongside its outputs (graphs, tables, etc), with comment that explain about the code. R Markdown documents are fully reproducible and rendered in several output documents—Html, PDF, Word files; presentation— Html slideshows, PDF beamer and MS powerpoints, and more. R Markdown documents uses the Markdown package, which also depends on markdown syntax. Markdown is a very simple markup language which provides syntax that is used to describe the format and structure of text documents. With only a small handful of options, Markdown allows you to apply formatting to your text and creating documents with headers, images, links and more while maintaining the original plain text easy to read. R Markdown is easy to use, allows others to reproduce your work and has powerful features like producing multiple output formats. With R Markdown, you can spend more time workin on your code and less time maintaining and typeseting the report—this translate that with R Markdown you can share more work with less effort than used before with previous tools. This is one powerful of using R Markwons as it make scientists more effective. An R mMrkdown documents essentially servers three purpose—communication, collaboration, and modern–day lab environment, which captures what you did and what you were thinking. From a communication angle it enables decision makers focus more on the results of the analysis insted of lines of code. But, because it include the code, it serve as a means to share and collaborate your work with others. To work with rmarkdown in Rstudio, you need the latest version of R and Rstudio. You also need rmarkdown and knitr packages installed in your machine. If you have not installed them in your machine, you can install the packages using the code written as; install.packages(&quot;knitr&quot;) install.packages(&quot;rmarkdown&quot;) You do not need to load the two packages, because Rstudio explicitly does that automatically when needed. The output format of an R Markdown files is a plain text file with an extension of .Rmd. This file contain a mixutre of three types of content including a YAML header, R code and plain text mixed with text formatting as shown in figure 3.1. Figure 3.1: Rmarkdonw syntax 3.2 Text Formating As we mentioned previously, R Markdonw is based on the Markdown syntax. Markdown is used to declare text formatting options. You do this by adding special symbols (punctuation) around the text you wish to “mark.”You can see some of the Markdown syntax tha rmarkdown use to format plain text shown in table 3.1. First, there are section header. You can specify differen level of headings—chapters, sections, subsections etc with the hash tag (#) at the begining of the heading. To have unorderlist, you use an asterisk at the begining and a space before the text. You can have sublist by simply indenting the asterisks. You ought to tab the indented line so there is a space between where the text start and the outer lever and where the bullet is at the next level. If you prefer, you can use the dash (-) instead of (*). To have numbered list you simply use the numbers instad of the *. Never mind even if you mix the number, pandoc will handle that and arrange the number in ascending order. data.frame(Syntax = c(&quot;#heading1&quot;,&quot;##heading2&quot;,&quot;###heading3&quot;,&quot;* text *&quot;, &quot;** text**&quot;, &quot;`code`&quot;, &quot;~~ text ~~&quot;, &quot;+ text&quot;, &quot;1. text&quot;, &quot;&gt; text&quot;, &quot;---text&quot;, &quot;--text&quot;, &quot;2^&quot;, &quot;x^&quot;, &quot;x_&quot;), Format = c(&quot;single hash tag for First level Heading&quot;,&quot;Two hash tags for second level Heading&quot;,&quot;Three hash tages for third level Heading&quot;,&quot;Emphasize *italize* using asterisks&quot;, &quot;Emphasize bold with two asteriks&quot;, &quot;`Code` text with back ticks&quot;, &quot;strike through with two tilde&quot;, &quot;Ordered list&quot;, &quot;Numbered list&quot;, &quot;Quote&quot;, &quot;Em dash with three dash&quot;, &quot;En dash with two dash&quot;, &quot; Power with ^ symbol&quot;, &quot;Superscript with single ^&quot;, &quot;Underscript with Underscore&quot;)) %&gt;% kableExtra::kable(format = &quot;html&quot;, caption = &quot;Common syntax for Markdown formatting&quot;, align = &quot;l&quot;) %&gt;% kableExtra::column_spec(column = 1, width = &quot;4cm&quot;)%&gt;% kableExtra::column_spec(column = 2, width = &quot;8cm&quot;) Table 3.1: Common syntax for Markdown formatting Syntax Format #heading1 single hash tag for First level Heading ##heading2 Two hash tags for second level Heading ###heading3 Three hash tages for third level Heading text * Emphasize italize using asterisks ** text** Emphasize bold with two asteriks code Code text with back ticks ~~ text ~~ strike through with two tilde text Ordered list text Numbered list &gt; text Quote —text Em dash with three dash –text En dash with two dash 2^ Power with ^ symbol x^ Superscript with single ^ x_ Underscript with Underscore 3.3 Hyperlinks R Markdown Providing hyperlinks in documentation is a great way to reference other resources on the web. You turn text into a hyperlink in Markdown by surrounding the text in square brackets [], and placing the URL to link to immediately after that in parentheses (). For example [here](www.masumbuko-semba.netlify.com/2018-02-25/sst/) 3.4 Images R Markdown also supports the rendering of images in your documents, which allows you to include diagrams, charts, and pictures in your documentation. The syntax for including images is similar to that for hyperlinks, except with an exclamation point ! before the link to indicate that it should be shown as an image: [!picture](path/toimage). 3.5 Tables While syntax for tables isn’t supported in all Markdown environments, tables can be shown on GitHub and in many other rendering engines. Tables are useful for organizing content, though they are somewhat verbose to express in markup syntax. Its complex to create table with the Markdown syntax, however, you can use either knitr::kable() or kableExtra::kable() to render data frames into Markdown table. This tools also has function to customize the table for generating as Html or Latex PDF document. Later on, I will show you how to make table from data frame with kableExtra::kable() 3.6 Cross-referencing Markdown and R Markdown do not support cross references and citation. However, you can combine R Markdown with bookdown package to achieve that. As shown in figure 3.5, I added the bookdown::html_document2 in the YAML (Figure 3.2) header to let bookdown function of cross-reference possible in R Markdown document. knitr::include_graphics(&quot;./images/yaml.png&quot;) Figure 3.2: Formatted YAML header to accomodate cross-reference and bibliograpy 3.7 Bibliographies You notice also that I added the bibliography: book.bib in the YAML header of the R Markdown document. This handle citation and allows the writer to insert citation. To make the citation done automatically, you must have listed your file in Biblex format (.bib) as shown in figure 3.3 as I did here and call the index with the key. To add a bibliography, you simply add the bibliography::name to YAML header as shown in figure 3.2. The name should be the file name of the Bibtex, and the Bibtex file should have an .bib extension. knitr::include_graphics(&quot;./images/bib_screenshot.png&quot;) Figure 3.3: Formating reference in Bibtex format To cite an article from the bibliography, you use [@dplyr] where dplyr is the indentifier key used in the Bibtex document of the list of reference. You can cite more than one paper inside the square brackets separate by a semicolon (Wickham et al. 2019; Wickham 2017; ???). To supress the author name(s) in the ciatio, say whey you have mentioned the names already of the reference in the text, you put - before the @, so you write Hadley Wickham (2016) the author of….. Depending on the journal or the citation style you prefer the most, you can specify the style of citation by adding csl: * tag in the YAML header, where * stands for the citation style, for instance, plos_one.csl. 3.8 Create .Rmd FIle To create a new R Markdown document, first open Rstudio and go to the File menu, choose New File and then R Markdown. Now RStudio will bring up a window where you can decide which kind of document you want to make and add some information, such as title and author name. It doesn’t matter so much what you do here; you can change it later. Rstudio will always choose html as the desired output document format . Type the title and author name, then click OK. The result is a new file with some boilerplate text in it, as shown in Figure 2-1. At the top of the file, between two lines containing just — is some meta-information for the document, and after the second — is the actual text. It consists of a mix of text, formatted in the Markdown language, and R code. Save the file go to the File menu, choose Save As and choose the name of the file you want to save in your working directory. The file should have and R Markdonw (.Rmd) extension. 3.9 Rendering Markdown To render R Markdonw document in Rstudio, you use the knitr package—a General-Purpose Package for Dynamic Report Generation in R (Xie 2015). In the toolbar above below the standard menu bar, there is a menu option called Knit, clikc the arrow pointing down just after the Knit and click Knit to HTML. The knitr package will render the R Markdown document into an HTML document and open it in Rstudio’ viewer pane as shwon in figure 3.4. knitr::include_graphics(&quot;./images/rmarkdown4.png&quot;) Figure 3.4: Rendered Markdown file as HTML The rendered HTML file is also saved in your local directory bearing a same name to that given to R Markdown document. 3.10 Structure of an Rmarkdown FIle The .Rmd file has three main parts Header: The text at the top of the document, written in YAML format. Markdown sections: plain text that describe the workflow written using the markdown syntax Code chunks: block of R code that can be runa and also be rendered using knitr to generate an interactive output of your desire. 3.11 Inserting code chunk To execute command in .Rmd document, you need to insert a chunk. You can insert these code block for R commands in three ways Using the combination of keyboard shortcuts Cmd|Ctrl + Alt + I You can insert with the button icon in the editor toolbar You can manually insert the chunk by typing three back ticks followed with r inside the braces```{r} and finish with three back ticks ``` You choose the approach that you feel comfortable, but I recommend to learn the keyboard shortcut, which will save you a lot of time in the long run and serve you from fatique. 3.12 Chunk name 3.13 Chunk options rmarkdon has several options that help you control how the R commands are executed in chunk block. There are almost sixty options offered by Knitr for you to control the code chunks. Some of the most commonly used options includes; eval = FALSE: prevents code from execution. And since the code will not run, you expect no result. This is important especially when you want to highlight the function of certain commands, but they should not run include = FALSE: allows the code to run but should not show the code results in the final document. There process that alwys output message or notification, use this option to prevent them echo = FALSE: This options run the code and display the result but prevent the code from appearing in the document. This is useful for writing document intended to people who have no interest of seeing the underlying R code message = FALSE: or warning = FALSE prevent both the message and the warning from appearing in the final document. results = \"hide\" hides printed outputs fig.show = \"hide\" hides plots error = TRUE allows the rendering of the document to continue in the next code chunk even if there is an error in the code chunk being evaluated. 3.14 Inline code You can insert R code chunk directly in the text by surrounding the code with a pair of backticks and the letter r like r. This simply means you typle a single back tick r with a letterr` without the brace blacket and followed by a sing back tick. Inline code are useful especially when you want to R code executed but only display the properties you examined in the text. To sum up, figure 3.5 display the raw R Markdown document with the formatted text, code, and syntax on (left panel) and the generated HTML document, which is clean and nice for reporting. knitr::include_graphics(&quot;./images/rmarkdown5.png&quot;) Figure 3.5: A R Mardown document (left) and rendered HTML document (R) References "],
["dataTypes.html", "Chapter 4 Understanding Data in R 4.1 Data Frame 4.2 Matrix 4.3 Arrays 4.4 Dealing with Misiing Values", " Chapter 4 Understanding Data in R ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::filter over any other package ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::select over any other package In chapter 2, we interactively made some basic calculations with RStudio and you learned about variables, functions, packages and how to install the packages in to the workspace. In this chapter we focus on lower level data types that R handles. This includes understanding how to manage the numeric type ( integer vs. double) and strings. The series of data called vectors and tabular format of data storage called data frames. But before we move further, let’s us clean our working environment by clicking a combination of Ctrl+L. Clearing the workspace is always recommended before working on a new R project to avoid name conflicts with provious projects. We can also clear all figures using graphics.off() function. It is a good code practise that a new R project start with the code in the chunk below: rm(list = ls()) graphics.off() 4.0.1 Data Types R is a flexible language that allows to work with different kind of data format (???). This inluced integer, numeric, character, complex, dates and logical. The default data type or class in R is double precision—numeric. In a nutshell, R treats all kind of data into five categories but we deal with only four in this book. Before proceeding, we need to clear the workspace by typing rm(list = ls()) after the prompt in the in a console. Integers:Integer values do not have decimal places. They are commonly used for counting or indexing. aa = c(20,68,78,50) You can check if the data is integer with is.integer() and can convert numeric value to an integer with as.integer() is.integer(aa) ## [1] FALSE You can query the class of the object with the class() to know the class of the object class(aa) ## [1] &quot;numeric&quot; Although the object bb is integer as confirmed with as.integer() function, the class() ouput the answer as numeric. This is because the defaul type of number in r is numeric. However, you can use the function as.integer() to convert numeric value to integer class(as.integer(aa)) ## [1] &quot;integer&quot; Numeric: The numeric class holds the set of real numbers — decimal place numbers. The numeric class is more general than the integer class, and inclused the integer numbers. These could be any number (whole or decimal number). You can check if the data is integer with is.integer() bb = c(12.5, 45.68, 2.65) class(bb) ## [1] &quot;numeric&quot; is.numeric(bb) ## [1] TRUE Strings: In programming terms, we usually call text as string.This often are text data like names. countries = c(&quot;Kenya&quot;, &quot;Uganda&quot;, &quot;Rwanda&quot;, &quot;Tanzania&quot;) class(countries) ## [1] &quot;character&quot; We can be sure whether the object is a string with is.character() or check the class of the object with class(). Factor: These are strings from finite set of values. For example, we might wish to store a variable that records gender of people. You can check if the data is factor with is.factor() and use as.factor() to convert string to factor sex = c(&quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;) sex = as.factor(sex) class(sex) ## [1] &quot;factor&quot; Often times we need to know the possible groups that are in the factor data. This can be achieved with the levels() function levels(sex) ## [1] &quot;Female&quot; &quot;Male&quot; levels(countries) ## NULL Often we wish to take a continuous numerical vector and transform it into a factor. The function cut() takes a vector of numerical data and creates a factor based on your give cut-points. Let us make a fictional income of 508 people with rnorm() function. income = rnorm(n = 508, mean = 500, sd = 80) hist(income, col = &quot;green&quot;, main = &quot;&quot;, las = 1, xlab = &quot;Individual Income&quot;) Figure 4.1: Income distribution #mosaic::plotDist(dist = &quot;norm&quot;, mean = 500, sd = 80) We can now breaks the distribution into groups and make a simple plot as shown in figure 4.2, where those with income less than 400 were about 50, followed with a group with income range between 400 and 500 of about 200 and 250 people receive income above 500 group = cut(income, breaks = c(300,400,500,800), labels = c(&quot;Below 400&quot;, &quot;400-500&quot;, &quot;Above 500&quot;)) is.factor(group) ## [1] TRUE levels(group) ## [1] &quot;Below 400&quot; &quot;400-500&quot; &quot;Above 500&quot; barplot(table(group), las = 1, horiz = FALSE, col = c(&quot;blue&quot;, &quot;red&quot;, &quot;blue&quot;), ylab = &quot;Frequency&quot;, xlab = &quot;Group of Income&quot;) Figure 4.2: Barplot of grouped income data = data.frame(group, income) Logicals: This is a special case of a factor that can only take on the values TRUE and FALSE. R is case-sensitive, therefore you must always capitalize TRUE and FALSE in function in R. Date and time 4.0.2 Vectors Ofen times we want to store a set of numbers in once place. One way to do this is using the vectors in R. Vectors store severl numbers– a set of numbers in one container. let us look on the example below id = c(1,2,3,4,5) people = c(158,659,782,659,759) street = c(&quot;Dege&quot;, &quot;Mchikichini&quot;, &quot;Mwembe Mdogo&quot;, &quot;Mwongozo&quot;, &quot;Cheka&quot;) Notice that the c() function, which is short for concatenate wraps the list of numbers. The c() function combines all numbers together into one container. Notice also that all the individual numbers are separated with a comma. The comma is reffered to an an item-delimiter. It allows R to hold each of the numbers separately. This is vital as without the item-delimiter, R will treat a vector as one big, unsperated number. 4.0.3 Indexing the element One advantage of vector is that you can extract individual element in the vector object by indexing, which is accomplished using the square bracket as illustrated below. id[5] ## [1] 5 people[5] ## [1] 759 street[5] ## [1] &quot;Cheka&quot; Apart from extracting single element, indexing allows to extract a range of element in a vector. This is extremely important because it allows to subset a portion of data in a vector. A colon operator is used to extract a range of data street[2:4] ## [1] &quot;Mchikichini&quot; &quot;Mwembe Mdogo&quot; &quot;Mwongozo&quot; 4.0.4 Adding and Replacing an element in a vector It is possible to add element of an axisting vecor. Here ia an example id[6] = 6 people[6] = 578 street[6] = &quot;Mwongozo&quot; Sometimes you may need to replace an element from a vector, this can be achieved with indexing people[1] = 750 4.0.5 Number of elements in a vector Sometimes you may have a long vector and want to know the numbers of elements in the object. R has length() function that allows you to query the vector and print the answer length(people) ## [1] 6 4.0.6 Generating sequence of vectors Numbers There are few R operators that are designed for creating vecor of non-random numbers. These functions provide multiple ways for generating sequences of numbers The colon : operator, explicitly generate regular sequence of numbers between the lower and upper boundary numbers specified. For example, generating number beween 0 and 10, we simply write; vector.seq = 0:10 vector.seq ## [1] 0 1 2 3 4 5 6 7 8 9 10 However, if you want to generate a vector of sequence number with specified interval, let say we want to generate number between 0 and 10 with interval of 2, then the seq() function is used regular.vector = seq(from = 0,to = 10, by = 2) regular.vector ## [1] 0 2 4 6 8 10 unlike the seq() function and : operator that works with numbers, the rep() function generate sequence of repeated numbers or strings to create a vector id = rep(x = 3, each = 4) station = rep(x = &quot;Station1&quot;, each = 4) id;station ## [1] 3 3 3 3 ## [1] &quot;Station1&quot; &quot;Station1&quot; &quot;Station1&quot; &quot;Station1&quot; The rep() function allows to parse each and times arguments. The each argument allows creation of vector that that repeat each element in a vector according to specified number. sampled.months = c(&quot;January&quot;, &quot;March&quot;, &quot;May&quot;) rep(x = sampled.months, each = 3) ## [1] &quot;January&quot; &quot;January&quot; &quot;January&quot; &quot;March&quot; &quot;March&quot; &quot;March&quot; &quot;May&quot; ## [8] &quot;May&quot; &quot;May&quot; But the times argument repeat the whole vector to specfied times rep(x = sampled.months, times = 3) ## [1] &quot;January&quot; &quot;March&quot; &quot;May&quot; &quot;January&quot; &quot;March&quot; &quot;May&quot; &quot;January&quot; ## [8] &quot;March&quot; &quot;May&quot; 4.0.7 Generating vector of normal distribution The central limit theorem that ensure the data is normal distributed is well known to statistician. R has a rnorm() function which makes vector of normal distributed values. For example to generate a vector of 40 sea surface temperature values from a normal distribution with a mean of 25, and standard deviation of 1.58, we simply type this expression in console; sst = rnorm(n = 40, mean = 25,sd = 1.58) sst ## [1] 23.13792 26.64994 27.77266 25.85643 25.34524 22.36000 27.71274 ## [8] 27.37440 25.76579 25.75880 27.09793 25.58109 24.04555 24.22041 ## [15] 22.91639 22.85039 24.25080 25.55899 22.93851 26.26927 27.33057 ## [22] 25.93791 25.36382 26.96936 25.67697 25.22632 26.57515 28.15970 ## [29] 23.92103 23.64244 24.73740 24.57313 25.00254 24.25200 23.33889 ## [36] 25.23467 24.96875 25.05506 26.52940 23.89199 4.0.8 Rounding off numbers There are many ways of rounding off numerical number to the nearest integers or specify the number of decimal places. the code block below illustrate the common way to round off: require(magrittr) chl = rnorm(n = 20, mean = .55, sd = .2) chl %&gt;% round(digits = 2) ## [1] 0.68 0.32 0.38 0.54 0.53 0.37 0.35 0.45 0.73 1.05 0.22 0.32 0.56 0.92 ## [15] 0.36 0.77 0.62 0.60 0.33 0.37 4.1 Data Frame data.frame is very much like a simple Excel spreadsheet where each column represents a variable type and each row represent observations. A data frame is the most common way of storing data in R and, generally, is the data structure most often used for data analyses. A data frame is a list of equal–length vectors with rows as records and columns as variables. This makes data frames unique in data storing as it can store different classes of objects in each column (i.e. numeric, character, factor, logic, etc). In this section, we will create data frames and add attributes to data frames. 4.1.1 Creating data frames Perhaps the easiest way to create a data frame is to parse vectors in a data.frame() function. For instance, in this case we create a simple data frame dt and assess its internal structure # create vectors Name = c(&#39;Bob&#39;,&#39;Jeff&#39;,&#39;Mary&#39;) Score = c(90, 75, 92) Grade = c(&quot;A&quot;, &quot;B&quot;, &quot;A&quot;) ## use the vectors to make a data frame dt = data.frame(Name, Score, Grade) ## assess the internal structure str(dt) ## &#39;data.frame&#39;: 3 obs. of 3 variables: ## $ Name : Factor w/ 3 levels &quot;Bob&quot;,&quot;Jeff&quot;,&quot;Mary&quot;: 1 2 3 ## $ Score: num 90 75 92 ## $ Grade: Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 2 1 Note how Variable Name in dt was converted to a column of factors . This is because there is a default setting in data.frame() that converts character columns to factors . We can turn this off by setting the stringsAsFactors = FALSE argument: ## use the vectors to make a data frame df = data.frame(Name, Score, Grade, stringsAsFactors = FALSE) df %&gt;% str() ## &#39;data.frame&#39;: 3 obs. of 3 variables: ## $ Name : chr &quot;Bob&quot; &quot;Jeff&quot; &quot;Mary&quot; ## $ Score: num 90 75 92 ## $ Grade: chr &quot;A&quot; &quot;B&quot; &quot;A&quot; Now the variable Name is of character class in the data frame. The inherited problem of data frame to convert character columns into a factor is resolved by introduction f advanced data frames called tibble, which provides sticker checking and better formating than the traditional data.frame. ## use the vectors to make a tibble tb = tibble(Name, Score, Grade) ## check the internal structure of the tibble tb%&gt;% glimpse() ## Observations: 3 ## Variables: 3 ## $ Name &lt;chr&gt; &quot;Bob&quot;, &quot;Jeff&quot;, &quot;Mary&quot; ## $ Score &lt;dbl&gt; 90, 75, 92 ## $ Grade &lt;chr&gt; &quot;A&quot;, &quot;B&quot;, &quot;A&quot; Table 4.1 show the the data frame created by fusing the two vectors together. Table 4.1: Variables in the data frame Name Score Grade Bob 90 A Jeff 75 B Mary 92 A Because the columns have meaning and we have given them column names, it is desirable to want to access an element by the name of the column as opposed to the column number.In large Excel spreadsheets I often get annoyed trying to remember which column something was. The $sign and []are used in R to select variable from the data frame. dt$Name ## [1] Bob Jeff Mary ## Levels: Bob Jeff Mary dt[,1] ## [1] Bob Jeff Mary ## Levels: Bob Jeff Mary dt$Score ## [1] 90 75 92 dt[,2] ## [1] 90 75 92 R has build in dataset that we can use for illustration. For example, (???) created a longley dataset, which is data frame with 7 economic variables observed every year from 1947 ti 1962 (Table 4.2). We can add the data in the workspace with data() function data(longley) longley %&gt;% kable(caption = &quot;Longleys&#39; Economic dataset&quot;, align = &quot;c&quot;, row.names = F) %&gt;% column_spec(1:7, width = &quot;3cm&quot;) Table 4.2: Longleys’ Economic dataset GNP.deflator GNP Unemployed Armed.Forces Population Year Employed 83.0 234.289 235.6 159.0 107.608 1947 60.323 88.5 259.426 232.5 145.6 108.632 1948 61.122 88.2 258.054 368.2 161.6 109.773 1949 60.171 89.5 284.599 335.1 165.0 110.929 1950 61.187 96.2 328.975 209.9 309.9 112.075 1951 63.221 98.1 346.999 193.2 359.4 113.270 1952 63.639 99.0 365.385 187.0 354.7 115.094 1953 64.989 100.0 363.112 357.8 335.0 116.219 1954 63.761 101.2 397.469 290.4 304.8 117.388 1955 66.019 104.6 419.180 282.2 285.7 118.734 1956 67.857 108.4 442.769 293.6 279.8 120.445 1957 68.169 110.8 444.546 468.1 263.7 121.950 1958 66.513 112.6 482.704 381.3 255.2 123.366 1959 68.655 114.2 502.601 393.1 251.4 125.368 1960 69.564 115.7 518.173 480.6 257.2 127.852 1961 69.331 116.9 554.894 400.7 282.7 130.081 1962 70.551 Sometimes you may need to create set of values and store them in vectors, then combine the vectors into a data frame. Let us see how this can be done. First create three vectors. One contains id for ten individuals, the second vector hold the time each individual signed in the attendane book and the third vector is the distance of each individual from office. We can concatenate the set of values to make vectors. id = c(1,2,3,4,5,6,7,8,9,10) time = ymd_hms(c(&quot;2018-11-20 06:35:25 EAT&quot;, &quot;2018-11-20 06:52:05 EAT&quot;, &quot;2018-11-20 07:08:45 EAT&quot;, &quot;2018-11-20 07:25:25 EAT&quot;, &quot;2018-11-20 07:42:05 EAT&quot;, &quot;2018-11-20 07:58:45 EAT&quot;, &quot;2018-11-20 08:15:25 EAT&quot;, &quot;2018-11-20 08:32:05 EAT&quot;, &quot;2018-11-20 08:48:45 EAT&quot;, &quot;2018-11-20 09:05:25 EAT&quot;), tz = &quot;&quot;) distance = c(20, 85, 45, 69, 42, 52, 6, 45, 36, 7) Once we have the vectors that have the same length dimension, we can use the function data.frame() to combine the the three vectors into one data frame shown in table 4.3 arrival = data.frame(id, time, distance) Table 4.3: The time employees enter into the office with the distance from their residential areas to the office IDs Time Distance 1 2018-11-20 06:35:25 20 2 2018-11-20 06:52:05 85 3 2018-11-20 07:08:45 45 4 2018-11-20 07:25:25 69 5 2018-11-20 07:42:05 42 6 2018-11-20 07:58:45 52 7 2018-11-20 08:15:25 6 8 2018-11-20 08:32:05 45 9 2018-11-20 08:48:45 36 10 2018-11-20 09:05:25 7 4.2 Matrix A matrix is defined as a collection of data elements arranged in a two–dimensional rectangular layout. R is very strictly when you make up a matrix as it must be with equal dimension—all columns in a matrix must be of the same length. Unlike data frame and list that can store numeric or character.etc in columns, matrix columns must be numeric or characters in a matrix file. 4.2.1 Creating Matrices The base R has a matrix() function that construct matrices column–wise. In other language, element in matrix are entered starting from the upper left corner and running down the columns. Therefore, one should take serious note of specifying the value to fill in a matrix and the number of rows and columns when using the matrix() function.For example in the code block below, we create an imaginary month sst value for five years and obtain an atomic vector of 60 observation. sst = rnorm(n = 60, mean = 25, 3) Once we have the atomic vector of sst value, we can convert it to matrix with the matrix() function. We put the observation as rows—months and the columns as years. Therefore, we have 12 rows and 5 years and the product of number of months and years we get 60—equivalent to our sst atomic vector we just created above. sst.matrix = matrix(data = sst, nrow = 12, ncol = 5) We then check whether we got the matrix with is.matrix() function is.matrix(sst);is.matrix(sst.matrix) ## [1] FALSE ## [1] TRUE sst ## [1] 30.71749 22.65786 23.70982 24.78016 27.43975 26.43239 24.61215 ## [8] 19.90383 22.15099 20.16220 24.03513 20.63225 25.60412 23.92288 ## [15] 23.31859 24.19946 23.87450 20.61185 26.70006 23.30603 21.51151 ## [22] 26.46118 27.48191 24.45768 29.00557 26.22302 24.48855 23.26612 ## [29] 29.94478 27.66465 24.02312 22.98475 27.52648 29.53158 23.20869 ## [36] 26.16872 24.43391 25.65599 32.01202 18.88049 26.62624 23.57498 ## [43] 24.60709 29.95364 26.23628 24.37927 21.27906 19.12633 30.60554 ## [50] 25.04912 28.15223 28.82997 26.26700 25.26655 22.95682 25.34013 ## [57] 30.08855 20.12601 25.90400 19.78301 We can check whether the dimension we just defined while creating this matrix is correct. This is done with the dim() function from base R. dim(sst.matrix) ## [1] 12 5 If you have large vector and you you want the matrix() function to figure out the number of columns, you simply define the nrow and tell the function that you do not want those element arranged by rows —i.e you want them in column-wise. That is done by parsing the argument byrow = FALSE inside the matrixt() function. sst.matrixby = sst %&gt;% matrix(nrow = 12, byrow = FALSE) 4.2.2 Adding attributes to Matrices Often times you may need to add additional attributes to the maxtrix—observation names, variable names and comments in the matrix. We can add columns, which are years from 2014 to 2018 years = 2014:2018 colnames(sst.matrix) = years sst.matrix ## 2014 2015 2016 2017 2018 ## [1,] 30.71749 25.60412 29.00557 24.43391 30.60554 ## [2,] 22.65786 23.92288 26.22302 25.65599 25.04912 ## [3,] 23.70982 23.31859 24.48855 32.01202 28.15223 ## [4,] 24.78016 24.19946 23.26612 18.88049 28.82997 ## [5,] 27.43975 23.87450 29.94478 26.62624 26.26700 ## [6,] 26.43239 20.61185 27.66465 23.57498 25.26655 ## [7,] 24.61215 26.70006 24.02312 24.60709 22.95682 ## [8,] 19.90383 23.30603 22.98475 29.95364 25.34013 ## [9,] 22.15099 21.51151 27.52648 26.23628 30.08855 ## [10,] 20.16220 26.46118 29.53158 24.37927 20.12601 ## [11,] 24.03513 27.48191 23.20869 21.27906 25.90400 ## [12,] 20.63225 24.45768 26.16872 19.12633 19.78301 and add the month for rows, which is January to December. Now the matrix has names for the rows—records and for columns—variables months = seq(from = dmy(010115), to = dmy(311215), by = &quot;month&quot;) %&gt;% month(abbr = TRUE, label = TRUE) rownames(sst.matrix) = months sst.matrix ## 2014 2015 2016 2017 2018 ## Jan 30.71749 25.60412 29.00557 24.43391 30.60554 ## Feb 22.65786 23.92288 26.22302 25.65599 25.04912 ## Mar 23.70982 23.31859 24.48855 32.01202 28.15223 ## Apr 24.78016 24.19946 23.26612 18.88049 28.82997 ## May 27.43975 23.87450 29.94478 26.62624 26.26700 ## Jun 26.43239 20.61185 27.66465 23.57498 25.26655 ## Jul 24.61215 26.70006 24.02312 24.60709 22.95682 ## Aug 19.90383 23.30603 22.98475 29.95364 25.34013 ## Sep 22.15099 21.51151 27.52648 26.23628 30.08855 ## Oct 20.16220 26.46118 29.53158 24.37927 20.12601 ## Nov 24.03513 27.48191 23.20869 21.27906 25.90400 ## Dec 20.63225 24.45768 26.16872 19.12633 19.78301 4.3 Arrays array(data = sst, dim = c(3,5,4)) ## , , 1 ## ## [,1] [,2] [,3] [,4] [,5] ## [1,] 30.71749 24.78016 24.61215 20.16220 25.60412 ## [2,] 22.65786 27.43975 19.90383 24.03513 23.92288 ## [3,] 23.70982 26.43239 22.15099 20.63225 23.31859 ## ## , , 2 ## ## [,1] [,2] [,3] [,4] [,5] ## [1,] 24.19946 26.70006 26.46118 29.00557 23.26612 ## [2,] 23.87450 23.30603 27.48191 26.22302 29.94478 ## [3,] 20.61185 21.51151 24.45768 24.48855 27.66465 ## ## , , 3 ## ## [,1] [,2] [,3] [,4] [,5] ## [1,] 24.02312 29.53158 24.43391 18.88049 24.60709 ## [2,] 22.98475 23.20869 25.65599 26.62624 29.95364 ## [3,] 27.52648 26.16872 32.01202 23.57498 26.23628 ## ## , , 4 ## ## [,1] [,2] [,3] [,4] [,5] ## [1,] 24.37927 30.60554 28.82997 22.95682 20.12601 ## [2,] 21.27906 25.04912 26.26700 25.34013 25.90400 ## [3,] 19.12633 28.15223 25.26655 30.08855 19.78301 This can be done with the indexing. For example, in the sst.matrix we just create, it has twelve rows representing monthly average and five columns representing years. We then obtain data for the six year and we want to add it into the matrix. Simply done with indexing sst.matrix[1:12,5] ## Jan Feb Mar Apr May Jun Jul Aug ## 30.60554 25.04912 28.15223 28.82997 26.26700 25.26655 22.95682 25.34013 ## Sep Oct Nov Dec ## 30.08855 20.12601 25.90400 19.78301 4.4 Dealing with Misiing Values Just as we can assign numbers, strings, list to a variable, we can also assign nothing to an object, or an empty value to a variable. IN R, an empty object is defined with NULL. Assigning a value oof NULL to an object is one way to reset it to its original, empty state. You might do this when you wanto to pre–allocate an object without any value, especially when you iterate the process and you want the outputs to be stored in the empty object. sst.container = NULL You can check whether the object is an empty with the is.null() function, which return a logical ouputs indicating whther is TRUE or FALSE is.null(sst.container) ## [1] TRUE You can also check for NULL in an if satement as well, as highlighted in the following example; if (is.null(sst.container)){ print(&quot;The object is empty and hence you can use to store looped outputs!!!&quot;) } ## [1] &quot;The object is empty and hence you can use to store looped outputs!!!&quot; And empty element (value) in object is represented with NA in R, and it is the absence of value in an object or variable. sst.sample = c(26.78, 25.98,NA, 24.58, NA) sst.sample ## [1] 26.78 25.98 NA 24.58 NA To identify missing values in a vector in R, use the is.na() function, which returns a logical vector with TRUE of the corresponding element(s) with missing value is.na(sst.sample) ## [1] FALSE FALSE TRUE FALSE TRUE and computing statistics of the variable with NA always will give out the NA ouputs mean(sst.sample); sd(sst.sample);range(sst.sample) ## [1] NA ## [1] NA ## [1] NA NA However, we can exclude missing value in these mathematical operations by parsing , na.rm = TRUE argument mean(sst.sample, na.rm = TRUE);sd(sst.sample, na.rm = TRUE);range(sst.sample, na.rm = TRUE) ## [1] 25.78 ## [1] 1.113553 ## [1] 24.58 26.78 you can also exclude the element with NA value using the `na.omit() sst.sample %&gt;% na.omit() ## [1] 26.78 25.98 24.58 ## attr(,&quot;na.action&quot;) ## [1] 3 5 ## attr(,&quot;class&quot;) ## [1] &quot;omit&quot; Finally is a NaN, which is closely related to NA, which is used to assign non-floating numbers. For example when we have the anomaly of sea surface temperature and we are interested to use sqrt() function to reduce the variability of the dataset. sst.anomaly = c(2.3,1.25,.8,.31,0,-.21) sqrt(sst.anomaly) ## Warning in sqrt(sst.anomaly): NaNs produced ## [1] 1.5165751 1.1180340 0.8944272 0.5567764 0.0000000 NaN We notice that the sqrt of -0.21 gives us a NaN elements. "],
["vector.html", "Chapter 5 Vector Data 5.1 Introduction 5.2 Numeric Vector 5.3 Integer vector 5.4 Character vector 5.5 Logical Vector 5.6 Vector Data 5.7 Reading vector data 5.8 Make shapefiles from Tabular data 5.9 Export simple feature as shapefile", " Chapter 5 Vector Data ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::filter over any other package ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::select over any other package 5.1 Introduction This chapter provides brief explanations of the fundamental vector model. You will get familiar with the theory behind vector model and the disciplines in which they predominate, before demonstrating its implementation in R. Vector is the most basic data structure in R. It is a sequence of elements of the same data type. if the elemenets are of different data types, they be coerced to a commontype that can accomodate all the elelements. Vector are generally created using the c() function widely called concatenate, though depeending on the type vector being created, other medhod. 5.2 Numeric Vector We create a numeric vector using a c() function but you can use any function that creates a sequence of numbers sst = c(25.4, 26, 28, 27.8, 29, 24.8, 22.3) We can use the is.vector() function to check if is is avector and class to check the data type is.vector(sst); class(sst) ## [1] TRUE ## [1] &quot;numeric&quot; 5.3 Integer vector Creating an integer vector is similar to numeric vector except that we need to instruct R to treat the data as integer and not numeric or double. To command R creating integer, we specify a suffix L to an element depth = c(5L, 10L, 15L, 20L, 25L,30L) is.vector(depth);class(depth) ## [1] TRUE ## [1] &quot;integer&quot; 5.4 Character vector A character vector may contain a single character , a word or a group of words. The elements must be enclosed with a single or double quotations mark. sites = c(&quot;Pemba Channel&quot;, &quot;Zanzibar Channnel&quot;, &quot;Pemba Channel&quot;) is.vector(sites); class(sites) ## [1] TRUE ## [1] &quot;character&quot; 5.5 Logical Vector A vector of logical values will either contain TRUE or FALSE or both presence = c(TRUE,TRUE, FALSE, TRUE, FALSE) is.vector(presence);class(presence) ## [1] TRUE ## [1] &quot;logical&quot; 5.6 Vector Data The geographic vector model is based on points located within a coordinate reference system (CRS). Points can represent self-standing features (e.g., the locations where research sample were taken ) or they can be linked together to form complex geometries like lines and polygons. Most point geometries contain only two dimensions with longitude and latitude together with the attribute information. However 3-dimensional points contain an additional \\(z\\) value— representing a thrid dimension—elevation, bathmetry etc. The standard and widely implemented spatial format for vector data is shapefile. shapefile format is popular geospatial vector data format for geographical information system (GIS) software.It is developed and maintained by Esri. Despite what its name may imply, a “single” shapefile is actually composed of at least three files, and as many as eight. Each file that makes up a “shapefile” has a common filename but different extension type. The list of files that define a “shapefile” are shown in table 5.1. Note that each file has a specific role in defining a shapefile. Table 5.1: Eight Common files that makes a shapefile Description Extension Attribute information .dbf Feature geometry .shp Feature geometry index .shx Attribute index .aih Attribute index .ain Coordinate system information .prj Spatial index file .sbn Spatial index file .sbx Until recent, shapefile format was the de facto form ofvector data basis for libraries such as GDAL. R has well-supported classes for storing spatial data and interfacing to the shapefile format, but has so far lacked a complete implementation of simple features, making conversions at times convoluted, inefficient or incomplete (Pebesma 2018). 5.6.1 Simple features Pebesma (2018) plainly described simple features as hierachical data model that present objects in the real world in computers, with emphasis on the spatial geometry of these objects. Out of 17, there are only seven seven simple feature types described in Table 5.2 that are commonly used. sf can represent common vector geometry types—points, lines, polygons and their respective ‘multi’ versions. sf also supports geometry collections, which can contain multiple geometry types in a single object. Table 5.2: Common simple features Type Description Point zero-dimensional geometry containing a single point Linestring sequence of points connected by straight, non-self intersecting line pieces; one-dimensional geometry Polygon geometry with a positive area (two-dimensional); sequence of points form a closed, non-self intersecting ring; the first ring denotes the exterior ring, zero or more subsequent rings denote holes in this exterior ring Multipoint set of points; a MULTIPOINT is simple if no two Points in the MULTIPOINT are equal Multilinestring Set of linestrings Multipolygon set of polygons Geometrycollection Set of geometries of any type with exception of geometrycollection These core geometry types are fully supported by the R package sf (Pebesma 2018). sf is a package providing a class system for geographic vector data (Lovelace, Nowosad, and Muenchow 2019) supersede, sp—methods for spatial data (Bivand, Pebesma, and Gomez-Rubio 2013). It also provides a consistent command-line interface to GEOS and GDAL, superseding rgdal— for data read/write (Bivand, Keitt, and Rowlingson 2019) and rgeos—for spatial operations (Bivand and Rundel 2018) packages 5.7 Reading vector data We will use the sf package to work with vector data in R [Pebesma (2018). Notice that the rgdal package automatically loads when sf is loaded. The sf package has the st_read() function that read different types of vector data to sf object. require(sf) ## Loading required package: sf ## Linking to GEOS 3.6.1, GDAL 2.2.3, PROJ 4.9.3 5.7.1 Reading shapefiles Shapefile is the widely used vector format in GIS software. The function st_read() import any type of shapefile into R. for example the chunk block below show how to import the sampling location that are in shapefile format into simple feature object in R’s worksapace. location = st_read(&quot;location.shp&quot;) ## Reading layer `location&#39; from data source `E:\\bookdown\\spatil_r\\location.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 18 features and 1 field ## geometry type: POINT ## dimension: XY ## bbox: xmin: 39.45336 ymin: -6.850945 xmax: 39.55239 ymax: -6.461915 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs location ## Simple feature collection with 18 features and 1 field ## geometry type: POINT ## dimension: XY ## bbox: xmin: 39.45336 ymin: -6.850945 xmax: 39.55239 ymax: -6.461915 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## First 10 features: ## name geometry ## 1 station 1 POINT (39.45336 -6.850945) ## 2 station 2 POINT (39.45336 -6.822652) ## 3 station 3 POINT (39.46751 -6.787286) ## 4 station 4 POINT (39.47458 -6.758993) ## 5 station 5 POINT (39.47812 -6.7307) ## 6 station 6 POINT (39.49226 -6.713016) ## 7 station 7 POINT (39.48519 -6.695333) ## 8 station 8 POINT (39.49226 -6.659967) ## 9 station 9 POINT (39.50641 -6.64582) ## 10 station 10 POINT (39.51702 -6.631674) When we print the this simple feature it tells us that it has 18 features that span between longitude 39.45336°E and 39.55239°E and latitude 6.850945°S and 6.461915°S with defined geographical coordinate system of WGS84. 5.7.2 Reading GPX file The st_read() function can also read files from GPS devices with the .gpx extension. track = st_read(&quot;Track-180911-063740.gpx&quot;, quiet = TRUE) ## Warning in evalq((function (..., call. = TRUE, immediate. = FALSE, ## noBreaks. = FALSE, : automatically selected the first layer in a data ## source containing more than one. track ## Simple feature collection with 1 feature and 24 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 39.44527 ymin: -6.907095 xmax: 39.44527 ymax: -6.907095 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## ele time magvar geoidheight name cmt ## 1 14.4 2018-09-11 07:42:07 NA NA Track Recording Stopped &lt;NA&gt; ## desc ## 1 Recording stopped at 33&#39;00&quot; because the user stopped it after 6.58km (0.50m gain). ## src link1_href link1_text link1_type link2_href link2_text link2_type ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## sym type fix sat hdop vdop pdop ageofdgpsdata dgpsid x_speed ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA NA NA NA NA NA 0.385527 ## geometry ## 1 POINT (39.44527 -6.907095) We can assess the geographical extent of the simple feature track with the st_bbox() function. track %&gt;% st_bbox() ## xmin ymin xmax ymax ## 39.445274 -6.907095 39.445274 -6.907095 And check the type of geographical coordinate system with st_crs() function track %&gt;% st_crs() ## Coordinate Reference System: ## EPSG: 4326 ## proj4string: &quot;+proj=longlat +datum=WGS84 +no_defs&quot; 5.8 Make shapefiles from Tabular data Sometimes the geographical information are in tabular form and you need to convert them into simple feature to work with spatial analysis and mapping. The sf package provide a st_as_sf() function that can make simple feature from the location information in the table. To illustrate this point, let us first load the file that contain the geographical information into the workspace. location = read_csv(&quot;kimbiji_kizimkazi_transect.csv&quot;) ## Parsed with column specification: ## cols( ## lon = col_double(), ## lat = col_double() ## ) Looking the internal structure of the location object we loaded, we find that there are eighteen observations and each observation has the longitude and latitude information. location %&gt;% glimpse() ## Observations: 18 ## Variables: 2 ## $ lon &lt;dbl&gt; 39.45336, 39.45336, 39.46751, 39.47458, 39.47812, 39.49226... ## $ lat &lt;dbl&gt; -6.850945, -6.822652, -6.787286, -6.758993, -6.730700, -6.... The file contain only the geographical information. We can add the column for station names. mutate() function from dplyr package add the third column. Because the station name should be sequentially numbered, the paste() function was used to do this. location = location %&gt;% mutate(name = paste(&quot;station&quot;, 1:18)) Once we know that the dataset contain the longitude and latitude information, we can use these spatial information to make simple feature object using the st_as_sf() from sf package location.sf = location %&gt;% st_as_sf(coords = c(&quot;lon&quot;, &quot;lat&quot;)) location.sf ## Simple feature collection with 18 features and 1 field ## geometry type: POINT ## dimension: XY ## bbox: xmin: 39.45336 ymin: -6.850945 xmax: 39.55239 ymax: -6.461915 ## epsg (SRID): NA ## proj4string: NA ## # A tibble: 18 x 2 ## name geometry ## &lt;chr&gt; &lt;POINT&gt; ## 1 station 1 (39.45336 -6.850945) ## 2 station 2 (39.45336 -6.822652) ## 3 station 3 (39.46751 -6.787286) ## 4 station 4 (39.47458 -6.758993) ## 5 station 5 (39.47812 -6.7307) ## 6 station 6 (39.49226 -6.713016) ## 7 station 7 (39.48519 -6.695333) ## 8 station 8 (39.49226 -6.659967) ## 9 station 9 (39.50641 -6.64582) ## 10 station 10 (39.51702 -6.631674) ## 11 station 11 (39.52056 -6.61399) ## 12 station 12 (39.52763 -6.578624) ## 13 station 13 (39.52763 -6.557404) ## 14 station 14 (39.5347 -6.539721) ## 15 station 15 (39.54178 -6.518501) ## 16 station 16 (39.54531 -6.497281) ## 17 station 17 (39.54531 -6.483135) ## 18 station 18 (39.55239 -6.461915) The coords parameter is given the latitude and longitude value columns–values used to locate the points associated with each record. We now have a simple featuere with 18 points. However, the simple feature lack the coordinate system. We can define the coordinate system for the simple feature with the st_set_crs() function and parse the epsg code of WGS84. location.sf = location.sf %&gt;% st_set_crs(4326) Let us check if the location.sf is indeed a spatial object location.sf ## Simple feature collection with 18 features and 1 field ## geometry type: POINT ## dimension: XY ## bbox: xmin: 39.45336 ymin: -6.850945 xmax: 39.55239 ymax: -6.461915 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## # A tibble: 18 x 2 ## name geometry ## &lt;chr&gt; &lt;POINT [°]&gt; ## 1 station 1 (39.45336 -6.850945) ## 2 station 2 (39.45336 -6.822652) ## 3 station 3 (39.46751 -6.787286) ## 4 station 4 (39.47458 -6.758993) ## 5 station 5 (39.47812 -6.7307) ## 6 station 6 (39.49226 -6.713016) ## 7 station 7 (39.48519 -6.695333) ## 8 station 8 (39.49226 -6.659967) ## 9 station 9 (39.50641 -6.64582) ## 10 station 10 (39.51702 -6.631674) ## 11 station 11 (39.52056 -6.61399) ## 12 station 12 (39.52763 -6.578624) ## 13 station 13 (39.52763 -6.557404) ## 14 station 14 (39.5347 -6.539721) ## 15 station 15 (39.54178 -6.518501) ## 16 station 16 (39.54531 -6.497281) ## 17 station 17 (39.54531 -6.483135) ## 18 station 18 (39.55239 -6.461915) let us check the class of the simple feature location.sf %&gt;% class() ## [1] &quot;sf&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Note the object has four class sf, tbl_df, tbl, and data_frame. The data frame contents was also carried over into the attributes table of the simple feature. There was only one attribute, name, other than lon and lat in the tabular data used to create this simple feature. Looking on the file clearly the projection is defined to WGS84. We can further transform the geographical coordinate system that is degree into the UTM, which is in metric. The function st_transform() from sf package handle transformation of coordinate system (Pebesma 2018). The epsg code for zone 37 south is 32737, which is parsed into the function. location.utm = location.sf %&gt;% st_transform(32737) location.utm ## Simple feature collection with 18 features and 1 field ## geometry type: POINT ## dimension: XY ## bbox: xmin: 550090.3 ymin: 9242705 xmax: 561079.7 ymax: 9285701 ## epsg (SRID): 32737 ## proj4string: +proj=utm +zone=37 +south +datum=WGS84 +units=m +no_defs ## # A tibble: 18 x 2 ## name geometry ## &lt;chr&gt; &lt;POINT [m]&gt; ## 1 station 1 (550090.3 9242705) ## 2 station 2 (550093.2 9245833) ## 3 station 3 (551660.2 9249741) ## 4 station 4 (552444.8 9252868) ## 5 station 5 (552838.7 9255995) ## 6 station 6 (554404.1 9257949) ## 7 station 7 (553624.3 9259904) ## 8 station 8 (554410 9263813) ## 9 station 9 (555975.3 9265376) ## 10 station 10 (557149.7 9266938) ## 11 station 11 (557542.7 9268893) ## 12 station 12 (558328.7 9272802) ## 13 station 13 (558331.2 9275147) ## 14 station 14 (559115.3 9277101) ## 15 station 15 (559899.8 9279446) ## 16 station 16 (560293.4 9281792) ## 17 station 17 (560295.1 9283356) ## 18 station 18 (561079.7 9285701) 5.9 Export simple feature as shapefile Once the simple feature is created, you might be interested to export as shapefile for use with other GIS software like QGIS and Esri ARCGIS. The sf package has a st_write() function that export simple feature from the workspace into shapefiles in the working directory. The chunk block below demonstrates the export of simple feature object location.sf into the location.shp in the working directory—denoted with ./ location.sf %&gt;% st_write(&quot;./location.shp&quot;) References "],
["introduction-to-tidyverse.html", "Chapter 6 Introduction to tidyverse 6.1 readr 6.2 dplyr 6.3 tidyr 6.4 ggplot2", " Chapter 6 Introduction to tidyverse While the base R packages includes many useful functions and data structures that you can use to accomplish a wide variety of data science task, the add–on tidyverse package supports a comprehensive data science workflow as illustrated in the diagram below. knitr::include_graphics(&quot;./images/data_science_model.png&quot;) Tidyverse is a coherent system of packages designed to address specific component of the workflow. Most of the package in the tidyverse were developed by Hadley Wickham (2017), and many other contributors. The core purpose of the tidyverse is to make statisticians and data scientists more productive by helping them through reproducible workflows that facilitate communication. In a nutshel, tidyverse focus about connecting the tools that make the coding workflows possible, through function that understand each other. It consists several packages for importing, tidying, transforming, exploring, and visualizing data. The packages in the tidyverse share a common philosophy of data and R programming, and are designed to work together naturally (Wickham and Grolemund 2016) In this chapter we will have a glimpse of the core packages that are in the tidyverse, then we will look at them separately in the coming chapters. The packages that are extensively used in the tidyverse include 6.1 readr The readr package provide a fast and friendly way to read tabular data into a structured data format. The package can read file from different format including comma seprated (.csv), tab delimited(.txt), white space seprated(.tsv) and more. Unlike R base, which import data as data.frame, readr package import and structure data in format called tibble. Tibbles are the tidyverse implementation of a data frame. They are similiar to data frame, but a bit more advanced version. Tibbles differs with data frame as they never convert data types of variables, they never change the names of the variables or create row names. Tibble also have a refined print out method that display the first ten rows and only variables that fit the size of the user screen. 6.2 dplyr The dplyr package is an important tidyverse component as it is aimed at manipulating the data in stored in tibbles. It includes seven primary functions that simplify data transformation and manipulation. This includes tasks such as filtering rows, selecting variables, create new variables from existing ones, ordering rows by either ascending or descending, summarise, aggregating, joining tibbles with key variable and much more. Understanding core functions of dplyr will help to significantly reduce the time you spend on the data wrangling. 6.3 tidyr The tidyr package reshape the data in a consistent format. Of many function it offers, the two widely used functions in this package are gather(), and spread(). The former function reformat the tibble data from wide form to long form, whereas the later reformat the long form to wide form. The core purpose of tidyr package is to ensure the tibble is in consistent format by ensuring tibble is in the form that: Each variable forms a column Each observation forms a row Each type of observational unit forms a table 6.4 ggplot2 ggplot2 is a package for creating graphics developed by Hadley Wickham (Wickham 2016). Unlike most other graphics packages in R, ggplot2 is based on Leland Wilkinson’s Grammar of Graphics. Grammar of Graphics is a term used to express the idea of creating independent layers that are combined into a graphical display. The building blocks used in ggplot2 to implement the Grammar of Graphics include data, aesthetics mapping, geometric objects, statistical transformation, scales, coordinate systems,labelling, themes, guides and faceting. These allows you to c onsistent syle for defining the garphics, a high level of abstraction for specifying plots, flexibility, a built–in themeing system for plot appearnace, mature and complete graphics system and access to many other extensions packages. This makes ggplot2 very powerful because you are not limited to a set of pre–specified graphics, but you can create new graphics that are precisely tailored for your problem. To make use of these package you must have installed tidyverse in your machine. If you lack the tidyverse packages in your machine, you can install a complete tidyverse from CRAN with code written as: install.packages(&quot;tidyverse&quot;) On your own computer, type that line of code in the console, and then press Enter to run it. R will download the packages from CRAN and install them onto your computer. Since tidyverse is an ecosystem, it will install all of the packages that are part of the tidyverse. Make sure your computer is connected to the internet when you want to install package, otherwise you will get an error. To use the functions in the tidyverse, you must first load it into your R session using the require() function require(tidyverse) ## Loading required package: tidyverse ## -- Attaching packages --------------------------------------- tidyverse 1.2.1 -- ## v tibble 2.1.1 v purrr 0.3.2 ## v tidyr 0.8.3 v stringr 1.4.0 ## v tibble 2.1.1 v forcats 0.4.0 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x lubridate::as.difftime() masks base::as.difftime() ## x lubridate::date() masks base::date() ## x magrittr::equals() masks testthat::equals() ## x tidyr::extract() masks magrittr::extract() ## x dplyr::filter() masks stats::filter() ## x kableExtra::group_rows() masks dplyr::group_rows() ## x lubridate::intersect() masks base::intersect() ## x magrittr::is_less_than() masks testthat::is_less_than() ## x purrr::is_null() masks testthat::is_null() ## x dplyr::lag() masks stats::lag() ## x dplyr::matches() masks testthat::matches() ## x magrittr::not() masks testthat::not() ## x purrr::set_names() masks magrittr::set_names() ## x lubridate::setdiff() masks base::setdiff() ## x lubridate::union() masks base::union() Once you run the code above, you will get the notification in the console of informing you that tidyverse had loaded the ggplot2, tibble, tidyr,readr, purrr, dplyr, stringr, and forcats packages. These packages contains functions that are widely used for importing, manipulate, transform, model data and visualize results of the nalysis in almost every R session. The message also indicate the functions filter() and lag() References "],
["readr.html", "Chapter 7 Importing data with readr 7.1 Comma-Separated (.csv) 7.2 Microsoft Excel(.xlsx) 7.3 Writing t a File 7.4 Basic Data Manipulation", " Chapter 7 Importing data with readr ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::filter over any other package ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::select over any other package You can lean R with the dataset it comes with when you install it in your machine. But sometimes you want to use the real data you or someone gathered already. One of critical steps for data processing is to import data with special format into R workspace.Data import refers to read data from the working directory into the workspace (???). In this chapter you will learn how to import common files into R. We will only focus on two common types of tabular data storage format—The comma-seprated .csv and excell spreadsheet (.xlsx). In later chapter we will explain how to read other types of data into R. 7.1 Comma-Separated (.csv) The most commonly format that R like is the comma-separated files. Although Base R provides various functions like read.table(), read.csv(), read.table() and read.csv2() to import data from the local directories into R workspace, for this book we use an read_csv() function from readr. Before we import the data, we need to load the packages that we will use their functions in this chapeter require(dplyr) require(readr) require(lubridate) require(readxl) require(haven) require(ggplot2) require(kableExtra) Consider a tabular data stored in my working directory in the .csv format in figure 7.1. Figure 7.1: A screenshot of the sample dataset We can import it with the read_csv() functions as: algoa.ctd = read_csv(&quot;algoa_ctd.csv&quot;) Parsed with column specification: cols( station = col_character(), time = col_datetime(format = &quot;&quot;), lon = col_double(), lat = col_double(), pressure = col_double(), temperature = col_double(), salinity = col_double(), oxygen = col_double(), fluorescence = col_double(), spar = col_double(), par = col_double(), density = col_double() ) When read_csv() has imported the data into R workspace, it prints out the name and type of of data for each variable. By simply glimpse the dataset, we see the format of the data is as expected. It has six variables(columns) and 177 observations (rows) similar to figure 7.1. Table 7.1 show sample of imported dataset. Table 7.1: CTD profiles Station Time Lon Lat Pressure Temperature Salinity Oxygen Fluorescence st1 2004-08-18 40.61 -10.54 5 25.17 33.92 3.93 0.56 st1 2004-08-18 40.61 -10.54 10 25.13 34.86 4.49 0.60 st1 2004-08-18 40.61 -10.54 15 25.11 34.86 4.50 0.65 st1 2004-08-18 40.61 -10.54 20 25.04 34.86 4.51 0.68 st1 2004-08-18 40.61 -10.54 25 24.95 34.86 4.51 0.76 st1 2004-08-18 40.61 -10.54 30 24.91 34.86 4.50 0.73 st1 2004-08-18 40.61 -10.54 35 24.88 34.87 4.49 0.74 st1 2004-08-18 40.61 -10.54 40 24.85 34.87 4.48 0.69 st1 2004-08-18 40.61 -10.54 45 24.80 34.88 4.46 0.70 st1 2004-08-18 40.61 -10.54 50 24.61 34.89 4.44 0.75 7.2 Microsoft Excel(.xlsx) Commonly our data is stored as a MS Excel file. we can import the file with read_xlsx() function of readxl package. The readxl package provides a function read_exel() that allows us to specify which sheet within the Excel file to read and what character specifies missing data (it assumes a blank cell is missing data if you don’t specifying anything). The function automatically convert the worksheet into a .csv file and read it. Let’s us import the the data in first sheet of the primary_productivity.xlsx. The dataset contain primary productivity value. We will use this file to illustrate how to import the excel file into R workspace with readxl package (Wickham and Bryan 2018). sheet1 = readxl::read_xlsx(&quot;./data/primary_productivity.xlsx&quot;, sheet = 1) sheet1 %&gt;% sample_n(5) ## # A tibble: 5 x 7 ## date year value month day site variable ## &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2010-09-15 00:00:00 2010 863.90467825443704 9 15 Pemba pp ## 2 2006-06-15 00:00:00 2006 869.29662433154999 6 15 Pemba pp ## 3 2003-12-15 00:00:00 2003 608.17409539473601 12 15 Pemba pp ## 4 2009-11-15 00:00:00 2009 577.39827558290096 11 15 Pemba pp ## 5 2011-05-15 00:00:00 2011 556.17832137978098 5 15 Pemba pp By printing the sheet1, we notice that the sheet contains monthly average value of primary productivity from the Pemba channel. sheet2 = readxl::read_xlsx(&quot;./data/primary_productivity.xlsx&quot;, sheet = 2) sheet2 %&gt;% sample_n(5) ## # A tibble: 5 x 7 ## date year value month day site variable ## &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2014-07-15 00:00:00 2014 1063.5653409090901 7 15 Zanzib~ pp ## 2 2006-08-15 00:00:00 2006 947.38417968750002 8 15 Zanzib~ pp ## 3 2009-06-15 00:00:00 2009 877.44613970588205 6 15 Zanzib~ pp ## 4 2003-04-15 00:00:00 2003 938.74180640243901 4 15 Zanzib~ pp ## 5 2015-04-15 00:00:00 2015 648.14237132352901 4 15 Zanzib~ pp sheet2 contains monthly average value of primary productivity from the Zanzibar channel. sheet3 = readxl::read_xlsx(&quot;./data/primary_productivity.xlsx&quot;, sheet = 3) sheet3 %&gt;% sample_n(5) ## # A tibble: 5 x 7 ## date year value month day site variable ## &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2018-08-15 00:00:00 2018 NA 8 15 Mafia pp ## 2 2011-02-15 00:00:00 2011 1095.05932800751 2 15 Mafia pp ## 3 2015-11-15 00:00:00 2015 1044.16794288079 11 15 Mafia pp ## 4 2008-09-15 00:00:00 2008 1210.32626748251 9 15 Mafia pp ## 5 2015-05-15 00:00:00 2015 1018.11701642335 5 15 Mafia pp sheet3 contains monthly average value of primary productivity from the Mafia channel. We look on the internal structure of the sheet3 file with the glimpse() function. You can interact with the table that show all variables and observations (Table ??) sheet3%&gt;%glimpse() ## Observations: 192 ## Variables: 7 ## $ date &lt;dttm&gt; 2003-01-15, 2003-02-15, 2003-03-15, 2003-04-15, 2003... ## $ year &lt;dbl&gt; 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003,... ## $ value &lt;chr&gt; &quot;1311.50104865771&quot;, &quot;1211.3158482142801&quot;, &quot;1302.45291... ## $ month &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5,... ## $ day &lt;dbl&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 1... ## $ site &lt;chr&gt; &quot;Mafia&quot;, &quot;Mafia&quot;, &quot;Mafia&quot;, &quot;Mafia&quot;, &quot;Mafia&quot;, &quot;Mafia&quot;,... ## $ variable &lt;chr&gt; &quot;pp&quot;, &quot;pp&quot;, &quot;pp&quot;, &quot;pp&quot;, &quot;pp&quot;, &quot;pp&quot;, &quot;pp&quot;, &quot;pp&quot;, &quot;pp&quot;,... sheet2 %&gt;% DT::datatable(rownames = FALSE, caption = &quot;An Interactive table of primary productivity in the Zanzibar channel&quot;) 7.3 Writing t a File Sometimes you work in the document and you want to export to a file. readr has write_csv() and write_tsv() functions that allows to export data frames from workspace to working directory write_csv(x = sheet1, path = &quot;./data/Primary_productivity_Pemba.csv&quot;) Wickham and Grolemund (2016) recomment the use of write_excel_csv() function when you want to export a data frame to Excel. readr has other tools that export files to other software like SAS, SPSS and more … write_excel_csv(x = sheet1, path = &quot;./data/Primary_productivity_Pemba.csv&quot;) 7.4 Basic Data Manipulation In this section, we brifely introduce some basic data handling and manipulation techniques, which are mostly associated with data frame. A data frame is a a tabular shaped contains columns and rows of equal length. In general a data frame structure with rows representing observations or measurements and with columns containing variables. 7.4.1 Explore the Data Frame We can visualize the table by simply run the name of the data flights octopus = read_csv(&quot;./data/octopus_data.csv&quot;) ## Parsed with column specification: ## cols( ## date = col_date(format = &quot;&quot;), ## village = col_character(), ## port = col_character(), ## ground = col_character(), ## sex = col_character(), ## dml = col_double(), ## tl = col_double(), ## weight = col_double(), ## lat = col_double(), ## lon = col_double() ## ) we can use class() to check if the data is data frame octopus %&gt;% class() ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; We can use names() to extract the variable names octopus %&gt;% names() ## [1] &quot;date&quot; &quot;village&quot; &quot;port&quot; &quot;ground&quot; &quot;sex&quot; &quot;dml&quot; &quot;tl&quot; ## [8] &quot;weight&quot; &quot;lat&quot; &quot;lon&quot; We can explore the internal structure of flights object with a dplyr()’s function glimpse() octopus %&gt;% glimpse() ## Observations: 1,079 ## Variables: 10 ## $ date &lt;date&gt; 2018-02-12, 2018-01-30, 2018-02-01, 2018-01-21, 2018-... ## $ village &lt;chr&gt; &quot;Somanga&quot;, &quot;Bwejuu&quot;, &quot;Somanga&quot;, &quot;Somanga&quot;, &quot;Somanga&quot;, ... ## $ port &lt;chr&gt; &quot;Mbuyuni&quot;, &quot;Kusini&quot;, &quot;Mbuyuni&quot;, &quot;Mbuyuni&quot;, &quot;Mbuyuni&quot;, ... ## $ ground &lt;chr&gt; &quot;CHAMBA CHA MACHANGE&quot;, &quot;NYAMALILE&quot;, &quot;BANIANI&quot;, &quot;CHAMBA... ## $ sex &lt;chr&gt; &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;,... ## $ dml &lt;dbl&gt; 14.0, 14.5, 17.0, 20.0, 12.0, 16.0, 15.0, 17.0, 12.0, ... ## $ tl &lt;dbl&gt; 110.0, 115.0, 115.0, 130.0, 68.0, 90.0, 96.0, 110.0, 7... ## $ weight &lt;dbl&gt; 1.385, 1.750, 1.000, 2.601, 0.670, 0.870, 1.020, 1.990... ## $ lat &lt;dbl&gt; -8.397838, -7.915809, -8.392644, -8.391614, -8.391146,... ## $ lon &lt;dbl&gt; 39.28079, 39.65424, 39.28153, 39.28089, 39.28251, 39.2... We can check how rows (observations/measurements) and columns (variables/fields) are in the data octopus %&gt;% dim() ## [1] 1079 10 The number of rows (observation) can be obtained using nrow() function octopus %&gt;% nrow() ## [1] 1079 The number of columns (variables) can be obtained using ncol() function octopus %&gt;% ncol() ## [1] 10 The length of the data frame is given by octopus %&gt;% length() ## [1] 10 Count the number of sample at each sex of octopus octopus %$% table(sex) ## sex ## F M ## 581 498 Count the number and compute the proportion of sample at each sex of octopus octopus %$% table(sex) %&gt;% prop.table() %&gt;% round(digits = 2) ## sex ## F M ## 0.54 0.46 7.4.2 simmple summary statistics The most helpful function for for summarizing rows and columns is summary(), which gives a collection of basim cummary statistics. The first method is to calculate some basic summary statistics (minimum, 25th, 50th, 75th percentiles, maximum and mean) of each column. If a column is categorical, the summary function will return the number of observations in each category. octopus %&gt;% summary() ## date village port ## Min. :2017-12-18 Length:1079 Length:1079 ## 1st Qu.:2018-01-14 Class :character Class :character ## Median :2018-01-20 Mode :character Mode :character ## Mean :2018-01-26 ## 3rd Qu.:2018-02-15 ## Max. :2018-03-12 ## ground sex dml tl ## Length:1079 Length:1079 Min. : 6.0 Min. : 11.00 ## Class :character Class :character 1st Qu.:10.0 1st Qu.: 68.00 ## Mode :character Mode :character Median :12.0 Median : 82.00 ## Mean :12.8 Mean : 86.01 ## 3rd Qu.:15.0 3rd Qu.:100.00 ## Max. :24.0 Max. :180.00 ## weight lat lon ## Min. :0.055 Min. :-8.904 Min. : 0.00 ## 1st Qu.:0.600 1st Qu.:-8.523 1st Qu.:39.28 ## Median :0.915 Median :-8.392 Median :39.50 ## Mean :1.232 Mean :-8.069 Mean :38.69 ## 3rd Qu.:1.577 3rd Qu.:-7.973 3rd Qu.:39.67 ## Max. :5.210 Max. : 0.000 Max. :39.75 You noticed that the summary() function provide the common metric for central tendency and measure of dispersion. We will look at them later. Now we turn to our favourite package dplyr References "],
["tidy.html", "Chapter 8 Reshaping data with tidyr 8.1 Gather—from wide to long format. 8.2 spread() —from long to wide format 8.3 separate()", " Chapter 8 Reshaping data with tidyr ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::filter over any other package ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::select over any other package One of the key task in data preparation is to organize thee dataset in a way that makes analysis and plottng easier. In practice, the data is often not stored like that and the data comes to us with repeated observations included on a single row. This is often done as a memory saving technique or because there is some structure in the data that makes the ‘wide’ format attractive. As a result, we need a way to convert data from wide4 to long5 and vice-versa (???). Structuring data frames to have the desired shape can be the most daunting part of statistical analysis, visualisation, and modeling. Several studies reported that 80% of data analysis is spent on the cleaning and preparing data. Tidy in this context means organize the data in a presentable and consistent format that facilitate data analysis and visualization. When you are doing data preparation in R for analysis or plottng, the first thing you do is a throughly mental thought on the desired structure of that data frame. You need to determine what each row and column will represent, so that you can consistently and clearly manipulate that data (e.g., you know what you will be selecting and what you will be filtering). There are basically three principles that we can follow to make a tidy dataset. First each variable must have its own a column, second each observation must have its own row, and finally, each cell must have its own value. The tidyr package is used to structure and work with data fames that follow three principles of tidy data. There are three advantages of using tidy data in R. First, having a consistent, uniform data structure is very important. Popular packages like dplyr (Wickham et al. 2019), ggplot2 (Wickham 2016), and all the other packages in the tidyverse (Wickham 2017) and their extensions like sf (Pebesma 2018), metR (Campitelli 2019), ggspatial (Dunnington 2018), ggrepel (Slowikowski 2018) etc are designed to work with tidy data (Wickham and Henry 2018). So consistent of tidy data ensure efficient processing, analysis and plotting of data. Third, placing variables into columns, facilities easy vectorization in R. Unfortunate, Many datasets that you receove are untidy and will require some work on your end. There are several reasons why a dataset messy. Often times the people who created the dataset aren’t familiar with the principles of tidy data. Another common reason that most datasets are messy is that data is often organized to facilitate something other than analysis. Data entry is perhaps the most common of the reasons that fall into this category. To make data entry as easy as possible, people will often arrange data in ways that aren’t tidy. So, many datasets require some sort of tidying before you can begin your analysis. As Wickham and Grolemund (2016) put it tidy data means that yo can plot or summarize the data efficiently. In othet words, it comes down to which data is represented as columns in a data frame and which is not.In principle, this means that there is column in the data frame that you can work with for the analysis you want to do. For example, if I want to look at the ctd dataset and see how the fluorescence varies among station in the upper water surface, we simply plot a boxplot of the station column against the fluorescence column shown in figure 8.1 Figure 8.1: Fluorescence variation against stations This works because I have a column for the x-axis and another for the y-axis. But what happens if I want to plot the different measurements of the irises to see how those are? Each measurement is a separate column. They are Petal.Length, Petal.Width, and so on.Now I have a bit of a problem because the different measurements are in different columns in my data frame. I cannot easily map them to an x-axis and a y-axis.The tidyr package addresses that. It contains functions for both mapping columns to values—, widely recognised as long format and for mapping back from values to columns—wide format. We are going to look for the function that are regularly used to tidy the data frames. These inlude: Gathering Spreading Separating Uniting 8.1 Gather—from wide to long format. Look at example of dataset. It has one common problem that the column names are not variables but rather values of a variable. In the table 8.1, the columns are actually values of the variable pressure. Each row in the existing table actually represents five observations from each station. Table 8.1: Wide format format Water Depth (meters) station 10 20 30 40 50 60 70 80 90 100 110 st1 0.599 0.678 0.729 0.693 0.752 1.098 0.857 0.481 0.313 0.221 0.138 st2 0.631 0.733 0.992 1.114 0.988 0.715 0.496 0.524 0.280 0.277 0.225 st3 0.980 0.934 1.149 1.200 1.187 1.035 0.854 0.530 0.437 0.347 0.324 st4 NA 0.623 0.603 0.742 0.724 0.799 0.914 0.819 0.801 0.692 0.444 st5 NA 0.350 0.415 0.421 0.566 0.592 0.591 0.634 0.751 0.774 0.575 The tidyr package can be used to gather these existing columns into a new variable. In this case, we need to create a new column called pressure and then gather the existing values in the these variable columns into the new pressure variable ctd.long = ctd.wide %&gt;% gather (key = &quot;depth&quot;, value = &quot;fluorescence &quot;, 2:12) ctd.long ## # A tibble: 55 x 3 ## station depth `fluorescence ` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 st1 10 0.599 ## 2 st2 10 0.631 ## 3 st3 10 0.98 ## 4 st4 10 NA ## 5 st5 10 NA ## 6 st1 20 0.678 ## 7 st2 20 0.733 ## 8 st3 20 0.934 ## 9 st4 20 0.623 ## 10 st5 20 0.350 ## # ... with 45 more rows As you can see from the chunk above, there are three arguments required in the gather() function. First is the key, which takes the variable names. Second, the value—the name of the variable whose values are spread over the cells. Finnaly, then you specify the set of columns that hold the values and not the variable names 8.2 spread() —from long to wide format A second tidy tool is spread(), which does the opposite of gather() function. It is used to convert a long format data frame to wide format. What this function does is to spread observation across multiple rows. ctd.long %&gt;% spread(key = &quot;depth&quot;, value = `fluorescence `) ## # A tibble: 5 x 12 ## station `10` `100` `110` `20` `30` `40` `50` `60` `70` `80` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st1 0.599 0.221 0.138 0.678 0.729 0.693 0.752 1.10 0.857 0.481 ## 2 st2 0.631 0.277 0.225 0.733 0.992 1.11 0.988 0.715 0.496 0.524 ## 3 st3 0.98 0.347 0.324 0.934 1.15 1.20 1.19 1.03 0.854 0.530 ## 4 st4 NA 0.692 0.444 0.623 0.603 0.742 0.724 0.799 0.914 0.819 ## 5 st5 NA 0.774 0.575 0.350 0.415 0.421 0.566 0.592 0.591 0.634 ## # ... with 1 more variable: `90` &lt;dbl&gt; The spread() function takes two arguments: the column that contains variable names, known as the key and a column that contains values from multiple variables – the value. 8.3 separate() Another common in tidyr package is a separate ()function, which split the variable into two or more variables. For example, the dataset below has a date column that actually contains the date and time variables separated by a space. ctd ## # A tibble: 115 x 12 ## station time lon lat pressure temperature salinity ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st1 2004-08-18 15:27:46 40.6 -10.5 5 25.2 33.9 ## 2 st1 2004-08-18 15:27:46 40.6 -10.5 10 25.1 34.9 ## 3 st1 2004-08-18 15:27:46 40.6 -10.5 15 25.1 34.9 ## 4 st1 2004-08-18 15:27:46 40.6 -10.5 20 25.0 34.9 ## 5 st1 2004-08-18 15:27:46 40.6 -10.5 25 24.9 34.9 ## 6 st1 2004-08-18 15:27:46 40.6 -10.5 30 24.9 34.9 ## 7 st1 2004-08-18 15:27:46 40.6 -10.5 35 24.9 34.9 ## 8 st1 2004-08-18 15:27:46 40.6 -10.5 40 24.9 34.9 ## 9 st1 2004-08-18 15:27:46 40.6 -10.5 45 24.8 34.9 ## 10 st1 2004-08-18 15:27:46 40.6 -10.5 50 24.6 34.9 ## # ... with 105 more rows, and 5 more variables: oxygen &lt;dbl&gt;, ## # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt; We use the separate() function splits the datetime column into two variables: date and time ctd %&gt;% separate(col = time, into = c(&quot;Date&quot;, &quot;Time&quot;), sep = &quot; &quot;) ## # A tibble: 115 x 13 ## station Date Time lon lat pressure temperature salinity oxygen ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st1 2004~ 15:2~ 40.6 -10.5 5 25.2 33.9 3.93 ## 2 st1 2004~ 15:2~ 40.6 -10.5 10 25.1 34.9 4.49 ## 3 st1 2004~ 15:2~ 40.6 -10.5 15 25.1 34.9 4.50 ## 4 st1 2004~ 15:2~ 40.6 -10.5 20 25.0 34.9 4.51 ## 5 st1 2004~ 15:2~ 40.6 -10.5 25 24.9 34.9 4.51 ## 6 st1 2004~ 15:2~ 40.6 -10.5 30 24.9 34.9 4.50 ## 7 st1 2004~ 15:2~ 40.6 -10.5 35 24.9 34.9 4.49 ## 8 st1 2004~ 15:2~ 40.6 -10.5 40 24.9 34.9 4.48 ## 9 st1 2004~ 15:2~ 40.6 -10.5 45 24.8 34.9 4.46 ## 10 st1 2004~ 15:2~ 40.6 -10.5 50 24.6 34.9 4.44 ## # ... with 105 more rows, and 4 more variables: fluorescence &lt;dbl&gt;, ## # spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt; The separate() function accepts arguments for the name of the variable to separate. You also need to specify the names of the variable to separate into, and an optional separator. ##unite () The unite()function is the exact opposite of separate() in that it combines multiple columns into a single column. While not used nearly as often as separate() , there may be times when you need the functionality provided by unite(). For example, we can combine the variable Date and Time to form siku.muda, and separate them with :-: symbol between the two variables. ctd.un %&gt;% unite(col = &quot;siku_muda&quot;, c(&quot;Date&quot;, &quot;Time&quot;), sep = &quot;:-:&quot;) ## # A tibble: 115 x 12 ## station siku_muda lon lat pressure temperature salinity oxygen ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st1 2004-08-~ 40.6 -10.5 5 25.2 33.9 3.93 ## 2 st1 2004-08-~ 40.6 -10.5 10 25.1 34.9 4.49 ## 3 st1 2004-08-~ 40.6 -10.5 15 25.1 34.9 4.50 ## 4 st1 2004-08-~ 40.6 -10.5 20 25.0 34.9 4.51 ## 5 st1 2004-08-~ 40.6 -10.5 25 24.9 34.9 4.51 ## 6 st1 2004-08-~ 40.6 -10.5 30 24.9 34.9 4.50 ## 7 st1 2004-08-~ 40.6 -10.5 35 24.9 34.9 4.49 ## 8 st1 2004-08-~ 40.6 -10.5 40 24.9 34.9 4.48 ## 9 st1 2004-08-~ 40.6 -10.5 45 24.8 34.9 4.46 ## 10 st1 2004-08-~ 40.6 -10.5 50 24.6 34.9 4.44 ## # ... with 105 more rows, and 4 more variables: fluorescence &lt;dbl&gt;, ## # spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt; References "],
["dplyr.html", "Chapter 9 Manipulating Data with dplyr 9.1 Why use dplyr? 9.2 Core dplyr Functions 9.3 Data 9.4 Choosing rows: Filtering observations 9.5 select 9.6 Summarizing and Grouping", " Chapter 9 Manipulating Data with dplyr ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::filter over any other package ## [conflicted] Removing existing preference ## [conflicted] Will prefer dplyr::select over any other package Before a dataset can be analysed in R, its often manipulated or transformed in various ways. For years manipulating data in R required more programming than actually analyzing data. That has improved dramatically with the dplyr package. It provides programmers with an intuitive vocabulary for executing data management and analysis tasks. Hadley Wickham (2019), the original creator of the dplyr package, refers to it as a Grammar of Data Manipulation. Because the package provides a set of functions (verbs) that let you modify data and perform common data preparation tasks. The key challenge in programming is mapping from questions about a data set to specific programming operations. With dplyr’s verbs, makes this process smoother, as it enables you to use the same vocabulary to both ask questions and write your code. In other words, the dplyr verbs lets you easily talk with data and transform a dataset in various ways with easy. 9.1 Why use dplyr? Using this package’s functions will allow you to code expressively—code that are easy to write and read, which make you effective and efficient data scientists. Great for data exploration and manipulation Intuitive to write and easy to read, especially when using the chaining syntax Fast on data frame—tabular dataset 9.2 Core dplyr Functions I will not go through all of the dplyr functions in this chapter. I will demonstrate the core functions that are used regularly for manipulating data. The five core functions also called verbs include: select() to select columns based on their names filter() to rows in data frame arrange() to re-order or arrange the rows in ascending or descending order mutate() to create new columns—add new variable summarise() to make a summary of variable(s) group_by() to group observation sample_n() and rename()to make random sample from the data set The group_by() function perform other common task which are related to the split-apply-combine concept. You can use these verbs when you describe the algorithm or process for interrogating data, and then use dplyr verbs to write code that will closely follow your “plain language” description because it uses functions and procedures that share the same language. For most of us who are familiar with the R base function, you will find that most dplyr functions on data frames can be expressed succinctly because you don’t need to repeat the name of the data frame. This becomes handy in operation, because dplyr package comes with the pipe operateor %&gt;% from the magrittr package (Bache and Wickham 2014), which allows to combine several functions in a chain to manipulate data. To use dplyr functions to manipulate your data, it must be already installed in your machine so that you can load with a require () function. Once the package is loaded, its functions are available for use. dplyr is a key package of the tidyverse ecosystem—a collection of R packages, which also includes other packages like, readr, purr,tibble, stringr, forcats, tidyr and ggplot2. require(tidyverse) 9.3 Data Data frames are ideal for representing data where each row is an observations and each column is a variable. Nearly all packages in a tidyverse work on data frames new version called tibble. A tibble provides stricter checking and better formatting than the traditional data frame. To demonstrate the usefulness of the dplyr package for manipulating data, we will use the CTD data of 22 stations casted along the coastal water of Tanzania. I have prepared the data, cleaned and align the profile into 5 meter standard depth for each cast and merged them into a single .csv file. You need to load the file into your R session. We can import the file with read_csv() function from the readr package (Wickham, Hester, and Francois 2017). The read_csv() function gives out a tibble. ctd = read_csv(&quot;algoa_ctd.csv&quot;) ## Parsed with column specification: ## cols( ## station = col_character(), ## time = col_datetime(format = &quot;&quot;), ## lon = col_double(), ## lat = col_double(), ## pressure = col_double(), ## temperature = col_double(), ## salinity = col_double(), ## oxygen = col_double(), ## fluorescence = col_double(), ## spar = col_double(), ## par = col_double(), ## density = col_double() ## ) 9.4 Choosing rows: Filtering observations The first dplyr verb we’ll explore is filter(). This function is primarily used to create a subset of observations that meet a specified conditions. The filter() function lets you pick out rows based on logical expressions. You give the function a predicate, specifying what a row should satisfy to be included. For instance, take a look at the chunk below: surface = ctd %&gt;% filter(pressure &lt; 150) surface ## # A tibble: 637 x 12 ## station time lon lat pressure temperature salinity ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st1 2004-08-18 15:27:46 40.6 -10.5 5 25.2 33.9 ## 2 st1 2004-08-18 15:27:46 40.6 -10.5 10 25.1 34.9 ## 3 st1 2004-08-18 15:27:46 40.6 -10.5 15 25.1 34.9 ## 4 st1 2004-08-18 15:27:46 40.6 -10.5 20 25.0 34.9 ## 5 st1 2004-08-18 15:27:46 40.6 -10.5 25 24.9 34.9 ## 6 st1 2004-08-18 15:27:46 40.6 -10.5 30 24.9 34.9 ## 7 st1 2004-08-18 15:27:46 40.6 -10.5 35 24.9 34.9 ## 8 st1 2004-08-18 15:27:46 40.6 -10.5 40 24.9 34.9 ## 9 st1 2004-08-18 15:27:46 40.6 -10.5 45 24.8 34.9 ## 10 st1 2004-08-18 15:27:46 40.6 -10.5 50 24.6 34.9 ## # ... with 627 more rows, and 5 more variables: oxygen &lt;dbl&gt;, ## # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt; The expression calls the ctd dataset and feed into the filter()and pick all observations with pressure below 150meters and create a new datase called surface. This is an expression where a single conditional statement is used. We can also limit the of the variable of interest by combining multiple conditional expressions as part of the filter(). Each expression (argument) is combined with an “AND” clause by default. This means that all expressions must be matched for a recorded to be returned. For instance we want to pick observations that were measured between 5 and 10 meters water only. We combine theses expressions with &amp; operator; ctd %&gt;% filter(pressure &gt;= 0 &amp; pressure &lt;= 10) ## # A tibble: 44 x 12 ## station time lon lat pressure temperature salinity ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st1 2004-08-18 15:27:46 40.6 -10.5 5 25.2 33.9 ## 2 st1 2004-08-18 15:27:46 40.6 -10.5 10 25.1 34.9 ## 3 st2 2004-08-18 17:00:01 40.8 -10.5 5 25.2 34.8 ## 4 st2 2004-08-18 17:00:01 40.8 -10.5 10 25.2 34.8 ## 5 st3 2004-08-18 20:32:54 41.0 -10.5 5 NA NA ## 6 st3 2004-08-18 20:32:54 41.0 -10.5 10 25.0 34.9 ## 7 st4 2004-08-18 22:44:56 41.1 -10.5 5 NA NA ## 8 st4 2004-08-18 22:44:56 41.1 -10.5 10 NA NA ## 9 st5 2004-08-19 00:59:59 41.3 -10.5 5 NA NA ## 10 st5 2004-08-19 00:59:59 41.3 -10.5 10 NA NA ## # ... with 34 more rows, and 5 more variables: oxygen &lt;dbl&gt;, ## # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt; We can also use the between() function, which is equivalent to pressure &gt;= 0 &amp; pressure &lt;= 10 in above chunk to achive the same result. ctd %&gt;% filter(between(pressure, 5,10)) ## # A tibble: 44 x 12 ## station time lon lat pressure temperature salinity ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st1 2004-08-18 15:27:46 40.6 -10.5 5 25.2 33.9 ## 2 st1 2004-08-18 15:27:46 40.6 -10.5 10 25.1 34.9 ## 3 st2 2004-08-18 17:00:01 40.8 -10.5 5 25.2 34.8 ## 4 st2 2004-08-18 17:00:01 40.8 -10.5 10 25.2 34.8 ## 5 st3 2004-08-18 20:32:54 41.0 -10.5 5 NA NA ## 6 st3 2004-08-18 20:32:54 41.0 -10.5 10 25.0 34.9 ## 7 st4 2004-08-18 22:44:56 41.1 -10.5 5 NA NA ## 8 st4 2004-08-18 22:44:56 41.1 -10.5 10 NA NA ## 9 st5 2004-08-19 00:59:59 41.3 -10.5 5 NA NA ## 10 st5 2004-08-19 00:59:59 41.3 -10.5 10 NA NA ## # ... with 34 more rows, and 5 more variables: oxygen &lt;dbl&gt;, ## # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt; In the next example, two conditional expressions are passed. The first is used to filter surface water below 200 m, and the second statement filter records that above latitude 6°S ctd %&gt;% filter(pressure &lt; 200 &amp; lat &gt; -6) ## # A tibble: 223 x 12 ## station time lon lat pressure temperature salinity ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st17 2004-08-23 19:42:30 40.1 -5.49 5 25.6 35.2 ## 2 st17 2004-08-23 19:42:30 40.1 -5.49 10 25.4 35.1 ## 3 st17 2004-08-23 19:42:30 40.1 -5.49 15 25.3 35.1 ## 4 st17 2004-08-23 19:42:30 40.1 -5.49 20 25.4 35.2 ## 5 st17 2004-08-23 19:42:30 40.1 -5.49 25 25.4 35.2 ## 6 st17 2004-08-23 19:42:30 40.1 -5.49 30 25.4 35.2 ## 7 st17 2004-08-23 19:42:30 40.1 -5.49 35 25.4 35.2 ## 8 st17 2004-08-23 19:42:30 40.1 -5.49 40 25.4 35.2 ## 9 st17 2004-08-23 19:42:30 40.1 -5.49 45 25.4 35.2 ## 10 st17 2004-08-23 19:42:30 40.1 -5.49 50 25.4 35.2 ## # ... with 213 more rows, and 5 more variables: oxygen &lt;dbl&gt;, ## # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt; In this case, the surface.transect dataset has records where both conditions are met—the pressure is blow 200 meter and latitude above -6. Note that when two or more conditions are paased, the &amp; operator is used. You may sometimes want to know stations and at what depth a particular variable has missing values. You can pick all variable in the data frame using is.na() function. ctd %&gt;% filter(is.na(fluorescence)) ## # A tibble: 7 x 12 ## station time lon lat pressure temperature salinity ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st3 2004-08-18 20:32:54 41.0 -10.5 5 NA NA ## 2 st4 2004-08-18 22:44:56 41.1 -10.5 5 NA NA ## 3 st4 2004-08-18 22:44:56 41.1 -10.5 10 NA NA ## 4 st5 2004-08-19 00:59:59 41.3 -10.5 5 NA NA ## 5 st5 2004-08-19 00:59:59 41.3 -10.5 10 NA NA ## 6 st10 2004-08-19 19:36:50 39.7 -8.83 5 NA NA ## 7 st10 2004-08-19 19:36:50 39.7 -8.83 10 NA NA ## # ... with 5 more variables: oxygen &lt;dbl&gt;, fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, ## # par &lt;dbl&gt;, density &lt;dbl&gt; You can also drop the observation with missing values in the data frame using the !is.na() operator ctd %&gt;% filter(!is.na(fluorescence)) ## # A tibble: 2,789 x 12 ## station time lon lat pressure temperature salinity ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st1 2004-08-18 15:27:46 40.6 -10.5 5 25.2 33.9 ## 2 st1 2004-08-18 15:27:46 40.6 -10.5 10 25.1 34.9 ## 3 st1 2004-08-18 15:27:46 40.6 -10.5 15 25.1 34.9 ## 4 st1 2004-08-18 15:27:46 40.6 -10.5 20 25.0 34.9 ## 5 st1 2004-08-18 15:27:46 40.6 -10.5 25 24.9 34.9 ## 6 st1 2004-08-18 15:27:46 40.6 -10.5 30 24.9 34.9 ## 7 st1 2004-08-18 15:27:46 40.6 -10.5 35 24.9 34.9 ## 8 st1 2004-08-18 15:27:46 40.6 -10.5 40 24.9 34.9 ## 9 st1 2004-08-18 15:27:46 40.6 -10.5 45 24.8 34.9 ## 10 st1 2004-08-18 15:27:46 40.6 -10.5 50 24.6 34.9 ## # ... with 2,779 more rows, and 5 more variables: oxygen &lt;dbl&gt;, ## # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt; When you have string variable in the data frame with character or factor format, you can filter the certain observation with %in% statement. For example, to obtain profiles from three stations: st1, st8, and st13, we can write the code as; ctd %&gt;% filter(station %in% c(&quot;st1&quot;, &quot;st8&quot;, &quot;st13&quot;)) ## # A tibble: 347 x 12 ## station time lon lat pressure temperature salinity ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st1 2004-08-18 15:27:46 40.6 -10.5 5 25.2 33.9 ## 2 st1 2004-08-18 15:27:46 40.6 -10.5 10 25.1 34.9 ## 3 st1 2004-08-18 15:27:46 40.6 -10.5 15 25.1 34.9 ## 4 st1 2004-08-18 15:27:46 40.6 -10.5 20 25.0 34.9 ## 5 st1 2004-08-18 15:27:46 40.6 -10.5 25 24.9 34.9 ## 6 st1 2004-08-18 15:27:46 40.6 -10.5 30 24.9 34.9 ## 7 st1 2004-08-18 15:27:46 40.6 -10.5 35 24.9 34.9 ## 8 st1 2004-08-18 15:27:46 40.6 -10.5 40 24.9 34.9 ## 9 st1 2004-08-18 15:27:46 40.6 -10.5 45 24.8 34.9 ## 10 st1 2004-08-18 15:27:46 40.6 -10.5 50 24.6 34.9 ## # ... with 337 more rows, and 5 more variables: oxygen &lt;dbl&gt;, ## # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt; 9.5 select The second verb we are going to demonstrate is the select() function. Often you work with large datasets with many columns but only a few are actually of interest to you. The select() function selects columns of the data frame. select() function allows you to choose variables that are of interest. You can use it to pick out a some columns from the dataset. For instance, fi we want pressure, temprature, salinity, fluorescence and ovygen variables from the data frame, we can simply write a code as; ctd %&gt;% select (pressure, temperature, salinity, fluorescence, oxygen) ## # A tibble: 2,796 x 5 ## pressure temperature salinity fluorescence oxygen ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 25.2 33.9 0.560 3.93 ## 2 10 25.1 34.9 0.599 4.49 ## 3 15 25.1 34.9 0.650 4.50 ## 4 20 25.0 34.9 0.678 4.51 ## 5 25 24.9 34.9 0.760 4.51 ## 6 30 24.9 34.9 0.729 4.50 ## 7 35 24.9 34.9 0.740 4.49 ## 8 40 24.9 34.9 0.693 4.48 ## 9 45 24.8 34.9 0.703 4.46 ## 10 50 24.6 34.9 0.752 4.44 ## # ... with 2,786 more rows Besides just selecting columns, you can use a minus sign to remove variables you do not need from the data frame. ctd %&gt;% select(-spar, -par, -density, -time) ## # A tibble: 2,796 x 8 ## station lon lat pressure temperature salinity oxygen fluorescence ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st1 40.6 -10.5 5 25.2 33.9 3.93 0.560 ## 2 st1 40.6 -10.5 10 25.1 34.9 4.49 0.599 ## 3 st1 40.6 -10.5 15 25.1 34.9 4.50 0.650 ## 4 st1 40.6 -10.5 20 25.0 34.9 4.51 0.678 ## 5 st1 40.6 -10.5 25 24.9 34.9 4.51 0.760 ## 6 st1 40.6 -10.5 30 24.9 34.9 4.50 0.729 ## 7 st1 40.6 -10.5 35 24.9 34.9 4.49 0.740 ## 8 st1 40.6 -10.5 40 24.9 34.9 4.48 0.693 ## 9 st1 40.6 -10.5 45 24.8 34.9 4.46 0.703 ## 10 st1 40.6 -10.5 50 24.6 34.9 4.44 0.752 ## # ... with 2,786 more rows ## or you can bind the variable you want to remove ctd %&gt;% select(-c(spar, par, density, time)) ## # A tibble: 2,796 x 8 ## station lon lat pressure temperature salinity oxygen fluorescence ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st1 40.6 -10.5 5 25.2 33.9 3.93 0.560 ## 2 st1 40.6 -10.5 10 25.1 34.9 4.49 0.599 ## 3 st1 40.6 -10.5 15 25.1 34.9 4.50 0.650 ## 4 st1 40.6 -10.5 20 25.0 34.9 4.51 0.678 ## 5 st1 40.6 -10.5 25 24.9 34.9 4.51 0.760 ## 6 st1 40.6 -10.5 30 24.9 34.9 4.50 0.729 ## 7 st1 40.6 -10.5 35 24.9 34.9 4.49 0.740 ## 8 st1 40.6 -10.5 40 24.9 34.9 4.48 0.693 ## 9 st1 40.6 -10.5 45 24.8 34.9 4.46 0.703 ## 10 st1 40.6 -10.5 50 24.6 34.9 4.44 0.752 ## # ... with 2,786 more rows You can drop a range of variables in the data frame with select() function. For instance, the code below drop all variables beween temperature to fluorescence. You can also select those variables in range by removing the negative sign # hide a range of columns ctd %&gt;% select(-(temperature:fluorescence)) ## # A tibble: 2,796 x 8 ## station time lon lat pressure spar par density ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st1 2004-08-18 15:27:46 40.6 -10.5 5 1177. 53.9 1022. ## 2 st1 2004-08-18 15:27:46 40.6 -10.5 10 1151. 40.3 1023. ## 3 st1 2004-08-18 15:27:46 40.6 -10.5 15 1135. 31.3 1023. ## 4 st1 2004-08-18 15:27:46 40.6 -10.5 20 1124. 25.6 1023. ## 5 st1 2004-08-18 15:27:46 40.6 -10.5 25 1111. 21.1 1023. ## 6 st1 2004-08-18 15:27:46 40.6 -10.5 30 1103. 17.2 1023. ## 7 st1 2004-08-18 15:27:46 40.6 -10.5 35 1097. 13.9 1023. ## 8 st1 2004-08-18 15:27:46 40.6 -10.5 40 1091. 11.2 1023. ## 9 st1 2004-08-18 15:27:46 40.6 -10.5 45 1087. 9.05 1024. ## 10 st1 2004-08-18 15:27:46 40.6 -10.5 50 1084. 7.30 1024. ## # ... with 2,786 more rows Just like you can pick columns with the matching name, you can also drop any column with a matching name ctd %&gt;% select(-contains(&quot;t&quot;)) ## # A tibble: 2,796 x 6 ## lon pressure oxygen fluorescence spar par ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 40.6 5 3.93 0.560 1177. 53.9 ## 2 40.6 10 4.49 0.599 1151. 40.3 ## 3 40.6 15 4.50 0.650 1135. 31.3 ## 4 40.6 20 4.51 0.678 1124. 25.6 ## 5 40.6 25 4.51 0.760 1111. 21.1 ## 6 40.6 30 4.50 0.729 1103. 17.2 ## 7 40.6 35 4.49 0.740 1097. 13.9 ## 8 40.6 40 4.48 0.693 1091. 11.2 ## 9 40.6 45 4.46 0.703 1087. 9.05 ## 10 40.6 50 4.44 0.752 1084. 7.30 ## # ... with 2,786 more rows Because of the naming conventions, many of the column names that you import dont make sense. You will often need to change the name of the variable. select() function allows you to accomplish that. For example, we want to select station, pressure and fluoresence, but we need also change the name of station to be Cast, pressure to Depth and fluorescence to Chlorophyll. You can achieve that with code written as; ctd %&gt;% select(Cast = station, Depth = pressure, Chlorophyll = fluorescence) ## # A tibble: 2,796 x 3 ## Cast Depth Chlorophyll ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st1 5 0.560 ## 2 st1 10 0.599 ## 3 st1 15 0.650 ## 4 st1 20 0.678 ## 5 st1 25 0.760 ## 6 st1 30 0.729 ## 7 st1 35 0.740 ## 8 st1 40 0.693 ## 9 st1 45 0.703 ## 10 st1 50 0.752 ## # ... with 2,786 more rows There are also a number of handy helper functions that you can use with the select() function to filter the returned columns. These include starts_with(), ends_with(), contains(), matches(), and num_range(). I wont illustrate them here, however, you can consult the help document for more information. 9.5.1 Adding new variables: mutate, transmute, add_rownames Besides selecting sets of existing columns, it’s often useful to add new columns that are functions of existing columns. This is the job of mutate(): Any new variable created with the mutate() function will be added to the end of the data frame. For example, raw fluorescence values are often skewed (Figure 9.1a) and we often transform them to have normal distribution (figure 9.1b). Figure 9.1: Histogram showing the distribution of a) raw fluorence and b) log-transformed fluorescence values At this situation, its handy to add a new column with transformed values in the data frame as shown in the code; ctd %&gt;% select(pressure, fluorescence) %&gt;% mutate(log.fluorescence = fluorescence %&gt;% log10()) ## # A tibble: 2,796 x 3 ## pressure fluorescence log.fluorescence ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 0.560 -0.251 ## 2 10 0.599 -0.223 ## 3 15 0.650 -0.187 ## 4 20 0.678 -0.169 ## 5 25 0.760 -0.119 ## 6 30 0.729 -0.138 ## 7 35 0.740 -0.131 ## 8 40 0.693 -0.159 ## 9 45 0.703 -0.153 ## 10 50 0.752 -0.124 ## # ... with 2,786 more rows The code tells important two steps: the first steps involved selecting the pressure and fluorescence variables, once these variables were selected fromt he ctd data frame were fed into a mutate() function, which computed the logarithmic of fluorescence and assign a new log.fluorescence variable into the data frame. In a similar way above, we can create a new variable of anomaly as the code below shows; ctd %&gt;% select(pressure, fluorescence) %&gt;% mutate(anomaly = fluorescence - mean(fluorescence, na.rm = TRUE)) ## # A tibble: 2,796 x 3 ## pressure fluorescence anomaly ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 0.560 0.425 ## 2 10 0.599 0.464 ## 3 15 0.650 0.515 ## 4 20 0.678 0.542 ## 5 25 0.760 0.624 ## 6 30 0.729 0.593 ## 7 35 0.740 0.604 ## 8 40 0.693 0.557 ## 9 45 0.703 0.568 ## 10 50 0.752 0.617 ## # ... with 2,786 more rows 9.5.2 Arranging rows The arrange() function in the dplyr package can be used to order the rows in a data frame. This function accepts a set of columns to order by with the default row ordering being in ascending order. ctd %&gt;% arrange(pressure) ## # A tibble: 2,796 x 12 ## station time lon lat pressure temperature salinity ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st1 2004-08-18 15:27:46 40.6 -10.5 5 25.2 33.9 ## 2 st2 2004-08-18 17:00:01 40.8 -10.5 5 25.2 34.8 ## 3 st3 2004-08-18 20:32:54 41.0 -10.5 5 NA NA ## 4 st4 2004-08-18 22:44:56 41.1 -10.5 5 NA NA ## 5 st5 2004-08-19 00:59:59 41.3 -10.5 5 NA NA ## 6 st6 2004-08-19 11:49:08 40.3 -8.83 5 25.2 34.9 ## 7 st7 2004-08-19 13:33:31 40.2 -8.83 5 25.3 34.9 ## 8 st8 2004-08-19 15:28:18 40.0 -8.83 5 25.0 34.9 ## 9 st9 2004-08-19 17:39:39 39.8 -8.83 5 25.1 34.9 ## 10 st10 2004-08-19 19:36:50 39.7 -8.83 5 NA NA ## # ... with 2,786 more rows, and 5 more variables: oxygen &lt;dbl&gt;, ## # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt; By default, it orders numerical values in increasing order, but you can ask for decreasing order using the desc() function: ctd %&gt;% arrange(pressure %&gt;% desc()) ## # A tibble: 2,796 x 12 ## station time lon lat pressure temperature salinity ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st3 2004-08-18 20:32:54 41.0 -10.5 1015 6.43 34.8 ## 2 st3 2004-08-18 20:32:54 41.0 -10.5 1010 6.45 34.8 ## 3 st3 2004-08-18 20:32:54 41.0 -10.5 1005 6.45 34.8 ## 4 st3 2004-08-18 20:32:54 41.0 -10.5 1000 6.45 34.8 ## 5 st3 2004-08-18 20:32:54 41.0 -10.5 995 6.46 34.8 ## 6 st3 2004-08-18 20:32:54 41.0 -10.5 990 6.48 34.8 ## 7 st3 2004-08-18 20:32:54 41.0 -10.5 985 6.55 34.8 ## 8 st3 2004-08-18 20:32:54 41.0 -10.5 980 6.60 34.8 ## 9 st3 2004-08-18 20:32:54 41.0 -10.5 975 6.60 34.8 ## 10 st3 2004-08-18 20:32:54 41.0 -10.5 970 6.62 34.8 ## # ... with 2,786 more rows, and 5 more variables: oxygen &lt;dbl&gt;, ## # fluorescence &lt;dbl&gt;, spar &lt;dbl&gt;, par &lt;dbl&gt;, density &lt;dbl&gt; 9.6 Summarizing and Grouping Summary statistics for a data frame can be produced with the summarise() function. The summarise() function produces a single row of data containing summary statistics from a data frame. For example, you can compute for the mean of fluorescence values: ctd %&gt;% summarise(fl.mean = mean(fluorescence, na.rm = TRUE)) ## # A tibble: 1 x 1 ## fl.mean ## &lt;dbl&gt; ## 1 0.118 By itself, it’s not that useful until chained with the group_by() verb to compute summary statistics. There you can split the data into different groups and compute the summaries for each group.For example, you can ask for the mean of and standard deviation values of fluorescence for each station in the data frame: ctd %&gt;% group_by(station) %&gt;% summarise(Mean = mean(fluorescence, na.rm = TRUE), STD = sd(fluorescence, na.rm = TRUE)) ## # A tibble: 5 x 3 ## station Mean STD ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st1 0.304 0.319 ## 2 st13 0.0897 0.179 ## 3 st18 0.101 0.287 ## 4 st4 0.0970 0.233 ## 5 st8 0.125 0.381 You can group by one or more variables; you just specify the columns you want to separate into different subsets to the function. It works best when grouping by factors or discrete numbers; there isn’t much fun in grouping by real numbers. ctd %&gt;% group_by(station, lon)%&gt;% summarise(Mean = mean(fluorescence, na.rm = TRUE), STD = sd(fluorescence, na.rm = TRUE)) ## # A tibble: 5 x 4 ## # Groups: station [5] ## station lon Mean STD ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 st1 40.6 0.304 0.319 ## 2 st13 40.1 0.0897 0.179 ## 3 st18 39.9 0.101 0.287 ## 4 st4 41.1 0.0970 0.233 ## 5 st8 40.0 0.125 0.381 summarise() can be used to count the number of rows in each group with nc()—which just counts how many observations you have in a subset of your data: You only need to parse the argument n() in the summarise()` function as; ctd %&gt;% group_by(station) %&gt;% summarise(frequency = n()) ## # A tibble: 5 x 2 ## station frequency ## &lt;chr&gt; &lt;int&gt; ## 1 st1 50 ## 2 st13 135 ## 3 st18 163 ## 4 st4 186 ## 5 st8 162 References "],
["working-with-dates-and-time.html", "Chapter 10 Working with Dates and Time 10.1 Getting the Date and Time with localtime 10.2 Converting strings to Dates 10.3 Converting date with lubridate package 10.4 Create Dates by Merging Data 10.5 ISOdate converts to a POSIXct object 10.6 Extract and Manipulate Parts of Dates 10.7 Creating Date Sequences 10.8 Calculations with Dates 10.9 Time difference of 15 days 10.10 example with time zones 10.11 Time Interval 10.12 ctd list 10.13 Arithmetic with date times 10.14 Dealing with Time Zones 10.15 Case study: Working with Date and Time of Argo float observations 10.16 Data 10.17 Data processing", " Chapter 10 Working with Dates and Time Most data we collect has a time stamp. The time stamp indicate the date and time the data was collected. Dealing with dates is a complicated task becuase of different formats and the time zones. R has built in functions that make your life working with dates and times a bit easier. In addition, Grolemund and Wickham (2011) developed a lubridate package that allows to work smoothly with dates and times. Therefore, this chapter deals specifically with dates and times. We deal with both gregorian calender that sound familiar to us as we see this structure in our wall calender. We will also deals with julian format, commonly used by oceanographers and meteorologist that counts days from specific reference. 10.1 Getting the Date and Time with localtime To get current date and time information that is pulled from your computer internal clock, simply use the Sys.Date() Sys.Date() [1] &quot;2019-06-10&quot; For for dates and times simply use Sys.time Sys.time() [1] &quot;2019-06-10 23:23:02 EAT&quot; use Sys.timezone() function to locate the timezone of your machine Sys.timezone() [1] &quot;Africa/Nairobi&quot; Note that the default date format in R is YYY-MM-DD. 10.2 Converting strings to Dates When date and times variables are imported into R’s worksapce, the functions that reads the file tend to convert the date into character. Hence, we ought to convert these strings back to date format. 10.2.1 convert strings to dates The function as.Date() is used to convert character into YYY-MM-DD date format. sampling.date = c(&quot;2019-02-19&quot;) as.Date(sampling.date) [1] &quot;2019-02-19&quot; There are times the date comes in a format that is weird and R can not figure out how to put in the right order. To obtain the list of all available dates and times conversion format, just write ?strftime() in console. A help window with the specific information will pop-up. sampling.date = c(&quot;05/06/2019&quot;) as.Date(sampling.date, format = &quot;%d/%m/%Y&quot;) [1] &quot;2019-06-05&quot; 10.3 Converting date with lubridate package The lubridate package has dozens of functions that convert dates and times from different characters format. The advantage of using the lubridate package is the fact that it automatically recognizes the common separators used when recording dates. Whether the date were separated with \"-\",\"/\",\".\",\"and\", or without separators, lubridate will recognize it. The only trick thing you have to bother is to specify the right order of date elements to determine the appropriate function applied. The table 10.1 indicates the date variables and the corresponding function data.frame(order = c(&quot;year,month,day&quot;, &quot;year,day,month&quot;, &quot;month,day,year&quot;,&quot;day,month,year&quot;, &quot;hour,minute&quot;, &quot;hour,minute,second&quot;, &quot;year,month,day,hour,minute,second&quot;), fun = c(&quot;ymd()&quot;, &quot;ydm()&quot;, &quot;mdy&quot;,&quot;dmy&quot;, &quot;hm&quot;, &quot;hms&quot;, &quot;ymd_hms&quot;)) %&gt;% kableExtra::kable(format = &quot;html&quot;, caption = &quot;Lubridate&#39;s function for dealing with dates and times&quot;, col.names = c(&quot;Date and Time variables&quot;, &quot;lubridate&#39;s function&quot;)) %&gt;% kableExtra::column_spec(column = 1, width = &quot;8cm&quot;) Table 10.1: Lubridate’s function for dealing with dates and times Date and Time variables lubridate’s function year,month,day ymd() year,day,month ydm() month,day,year mdy day,month,year dmy hour,minute hm hour,minute,second hms year,month,day,hour,minute,second ymd_hms require(lubridate) require(tidyverse) require(magrittr) require(oce) 10.4 Create Dates by Merging Data Sometimes your date data are collected in separate elements. To convert these separate data into one date object incorporate the ISOdate() function: yr &lt;- c (&quot;2012&quot;, &quot;2013&quot;, &quot;2014&quot;, &quot;2015&quot;) mo &lt;- c (&quot;1&quot;, &quot;5&quot;, &quot;7&quot;, &quot;2&quot;) day &lt;- c (&quot;02&quot;, &quot;22&quot;, &quot;15&quot;, &quot;28&quot;) 10.5 ISOdate converts to a POSIXct object ISOdate (year = yr, month = mo, day = day) [1] &quot;2012-01-02 12:00:00 GMT&quot; &quot;2013-05-22 12:00:00 GMT&quot; [3] &quot;2014-07-15 12:00:00 GMT&quot; &quot;2015-02-28 12:00:00 GMT&quot; as.Date ( ISOdate (year = yr, month = mo, day = day)) [1] &quot;2012-01-02&quot; &quot;2013-05-22&quot; &quot;2014-07-15&quot; &quot;2015-02-28&quot; Note that ISODate() also has arguments to accept data for hours, minutes, seconds, and time-zone if you need to merge all these separate components. 10.6 Extract and Manipulate Parts of Dates To extract and manipulate individual elements of a date I typically use the lubridate package due to its simplistic function syntax . The functions provided by lubridate to perform extraction and manipulation of dates are shown in table 10.2 data.frame(date = c(&quot;Year&quot;, &quot;Month&quot;, &quot;Week&quot;, &quot;Day of year&quot;,&quot;Day of month&quot;, &quot;Day of week&quot;, &quot;Hour&quot;, &quot;Minute&quot;, &quot;Second&quot;, &quot;Time zone&quot;), accessor = c(&quot;year()&quot;,&quot;month()&quot;,&quot;week()&quot;,&quot;yday()&quot;,&quot;mday()&quot;,&quot;wday()&quot;,&quot;hour()&quot;,&quot;minute()&quot;,&quot;second()&quot;,&quot;tz()&quot;)) %&gt;% kableExtra::kable(format = &quot;html&quot;, caption = &quot;Accessor functions for lubridate&quot;, col.names = c(&quot;Date component&quot;, &quot;Accessor&quot;)) %&gt;% kableExtra::column_spec(column = 1, width = &quot;5cm&quot;) Table 10.2: Accessor functions for lubridate Date component Accessor Year year() Month month() Week week() Day of year yday() Day of month mday() Day of week wday() Hour hour() Minute minute() Second second() Time zone tz() To extract an individual element of the date variable you simply use the accessor function desired. Note that the accessor variables have additional arguments that can be used to show the name of the date element in full or abbreviated form x &lt;- c (&quot;2015-07-01&quot;, &quot;2015-08-01&quot;, &quot;2015-09-01&quot;) lubridate::year (x) [1] 2015 2015 2015 lubridate::month(x) [1] 7 8 9 # show abbreviated name lubridate::month (x, label = TRUE) [1] Jul Aug Sep 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec # show unabbreviated name lubridate::month (x, label = TRUE, abbr = FALSE) [1] July August September 12 Levels: January &lt; February &lt; March &lt; April &lt; May &lt; June &lt; ... &lt; December lubridate::wday (x, label = TRUE, abbr = FALSE) [1] Wednesday Saturday Tuesday 7 Levels: Sunday &lt; Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; ... &lt; Saturday To manipulate or change the values of date elements we simply use the accessor function to extract the element of choice and then use the assignment function to assign a new value # convert to date format x = lubridate::ymd(x) x [1] &quot;2015-07-01&quot; &quot;2015-08-01&quot; &quot;2015-09-01&quot; # change the days for the dates lubridate::mday (x) [1] 1 1 1 10.7 Creating Date Sequences o create a sequence of dates we can leverage the seq () function. As with numeric vectors , you have to specify at least three of the four arguments ( from , to , by , and length.out ). ## by years seq(lubridate::ymd(&quot;2010-1-1&quot;), lubridate::ymd(&quot;2018-1-1&quot;), by = &quot;years&quot;) [1] &quot;2010-01-01&quot; &quot;2011-01-01&quot; &quot;2012-01-01&quot; &quot;2013-01-01&quot; &quot;2014-01-01&quot; [6] &quot;2015-01-01&quot; &quot;2016-01-01&quot; &quot;2017-01-01&quot; &quot;2018-01-01&quot; ## by quuarters seq(lubridate::ymd(&quot;2016-1-1&quot;), lubridate::ymd(&quot;2018-12-31&quot;), by = &quot;quarters&quot;) [1] &quot;2016-01-01&quot; &quot;2016-04-01&quot; &quot;2016-07-01&quot; &quot;2016-10-01&quot; &quot;2017-01-01&quot; [6] &quot;2017-04-01&quot; &quot;2017-07-01&quot; &quot;2017-10-01&quot; &quot;2018-01-01&quot; &quot;2018-04-01&quot; [11] &quot;2018-07-01&quot; &quot;2018-10-01&quot; ## by month seq(lubridate::ymd(&quot;2017-10-1&quot;), lubridate::ymd(&quot;2018-09-1&quot;), by = &quot;month&quot;) [1] &quot;2017-10-01&quot; &quot;2017-11-01&quot; &quot;2017-12-01&quot; &quot;2018-01-01&quot; &quot;2018-02-01&quot; [6] &quot;2018-03-01&quot; &quot;2018-04-01&quot; &quot;2018-05-01&quot; &quot;2018-06-01&quot; &quot;2018-07-01&quot; [11] &quot;2018-08-01&quot; &quot;2018-09-01&quot; ## by week seq(lubridate::ymd(&quot;2018-10-1&quot;), lubridate::ymd(&quot;2018-12-1&quot;), by = &quot;week&quot;) [1] &quot;2018-10-01&quot; &quot;2018-10-08&quot; &quot;2018-10-15&quot; &quot;2018-10-22&quot; &quot;2018-10-29&quot; [6] &quot;2018-11-05&quot; &quot;2018-11-12&quot; &quot;2018-11-19&quot; &quot;2018-11-26&quot; ## by days seq(lubridate::ymd(&quot;2018-1-1&quot;), lubridate::ymd(&quot;2018-1-31&quot;), by = &quot;3.5 days&quot;) [1] &quot;2018-01-01&quot; &quot;2018-01-04&quot; &quot;2018-01-07&quot; &quot;2018-01-10&quot; &quot;2018-01-13&quot; [6] &quot;2018-01-16&quot; &quot;2018-01-19&quot; &quot;2018-01-22&quot; &quot;2018-01-25&quot; &quot;2018-01-28&quot; [11] &quot;2018-01-31&quot; ## by days seq(lubridate::ymd(&quot;2018-1-1&quot;), lubridate::ymd(&quot;2018-1-10&quot;), by = &quot;day&quot;) [1] &quot;2018-01-01&quot; &quot;2018-01-02&quot; &quot;2018-01-03&quot; &quot;2018-01-04&quot; &quot;2018-01-05&quot; [6] &quot;2018-01-06&quot; &quot;2018-01-07&quot; &quot;2018-01-08&quot; &quot;2018-01-09&quot; &quot;2018-01-10&quot; 10.8 Calculations with Dates Since R stores date and time objects as numbers, this allows you to perform various calculations such as logical comparisons, addition, subtraction, and working with durations . x &lt;- Sys.Date() x [1] &quot;2019-06-10&quot; y = lubridate::ymd(&quot;2015-09-11&quot;) x &gt; y [1] TRUE x - y Time difference of 1368 days 10.9 Time difference of 15 days The nice thing about the date/time classes is that they keep track of leap years, leap seconds, daylight savings , and time zones. Use OlsonNames() for a full list of acceptable time zone specifications. ## create sequence and identify leap years seq(lubridate::dmy(&quot;290212&quot;), lubridate::ymd(&quot;170228&quot;), by = &quot;year&quot;) [1] &quot;2012-02-29&quot; &quot;2013-03-01&quot; &quot;2014-03-01&quot; &quot;2015-03-01&quot; &quot;2016-02-29&quot; # last leap year x &lt;- lubridate::ymd(&quot;2016-03-1&quot;) y &lt;- lubridate::ymd(&quot;2016-02-28&quot;) x - y Time difference of 2 days 10.10 example with time zones x &lt;- lubridate::now(tzone =&quot;US/Eastern&quot;) y &lt;- lubridate::now(tzone =&quot;Africa/Nairobi&quot;) x;y;y == x;y-x [1] &quot;2019-06-10 16:23:03 EDT&quot; [1] &quot;2019-06-10 23:23:03 EAT&quot; [1] FALSE Time difference of 0.0009961128 secs We can also deal with time spans by using the duration functions in lubridate. Durations simply measure the time span between start and end dates. Using base R date functions for duration calculations is tedious and often results in wrong measurements. lubridate provides simplistic syntax to calculate durations with the desired measurement (seconds, minutes, hours, etc.). # create new duration (represented in seconds) lubridate::duration (60) [1] &quot;60s (~1 minutes)&quot; # create durations for minutes, hours, years lubridate::dminutes (1) [1] &quot;60s (~1 minutes)&quot; lubridate::dhours(1) [1] &quot;3600s (~1 hours)&quot; lubridate::dyears(1) [1] &quot;31536000s (~52.14 weeks)&quot; # add/subtract durations from date/time object x &lt;- lubridate::ymd_hms (&quot;2015-09-22 12:00:00&quot;) x + lubridate::dhours (10) [1] &quot;2015-09-22 22:00:00 UTC&quot; x + lubridate::dhours (10) + lubridate::dminutes (33) + lubridate::dseconds (54) [1] &quot;2015-09-22 22:33:54 UTC&quot; 10.11 Time Interval You can save an interval of time an an interval object in R with lubridate. This is quite useful for example, you want to understand the interval between two or more successive CTD casts algoa = list.files(&quot;e:/Data Manipulation/ctd_algoa/&quot;, pattern = &quot;dst&quot;, full.names = TRUE) we notice that the files has an .cnv extenstion, which is oce–readable. We therefore load the oce package together the package in tidyverse. require(oce) require(tidyverse) 10.12 ctd list We create a list that accomodate and store the ctd file for each cast. To automate the reading of these twenty one files, the nestedfor() was used. The first loop imported the .cnv files from the working directory into the workspace. Once the file is the oce object format, the second for loop with latter j was used to convert the oce object to data frame. The data frame was sliced and only the the first rows that contain the cast date and, maximum depth together with longitude and latitude coordinates at each cast was retained. ctd.list = list() ctd.time = list() for (i in 1:length(algoa)){ ## First loop: read files from the working directory and convert them to oce object stored in the list ctd.list[[i]] = read.ctd(algoa[i]) %&gt;% ctdDecimate(p = 5) ## second loop: convert oce object into data frame and slice only the first observation picked for (j in 1:length(ctd.list)){ ctd.time[[j]] = ctd.list[[j]]@data%&gt;% as_tibble()%&gt;%select(pressure) %&gt;% mutate(date = ctd.list[[j]][[&quot;startTime&quot;]], lon = ctd.list[[j]][[&quot;longitude&quot;]], lat = ctd.list[[j]][[&quot;latitude&quot;]], max.depth = max(pressure)) %&gt;% slice(1) %&gt;% select(max.depth, lon,lat, date) } } ctd.time = ctd.time %&gt;% bind_rows() Once we have the data fram, we pulled the date variable as a vector and then used the interval() and as.duration() function from lubridate to obtain the duration. Durations simply measure the time span between start and end dates. As the results show, Algoa took 133.26 hours(~5.55 days) to complete its mission along coastal waters of Tanzania. ctd.time.vector = ctd.time%&gt;% pull(date) duration = lubridate::interval(start = ctd.time.vector[1], end = ctd.time.vector[21]) %&gt;% lubridate::as.duration() #duration %&gt;% as.numeric(&quot;hour&quot;) 10.13 Arithmetic with date times Often times CTD instruments records date times in UTC, which need to be standardized to the local time. For instance the ctd.time.vector we simply created is in UTC, to obtain the real local time, which is the East African time, which is three hours ahead, we need to add them up. lubridate package has Durations and Periods classes that help to handle the issues. The helper functions for creating periods are names after the units of time(plural) as highlighted in the chunk below lubridate::minutes(10); lubridate::hours(2);lubridate::seconds(10) [1] &quot;10M 0S&quot; [1] &quot;2H 0M 0S&quot; [1] &quot;10S&quot; Similar to helper functions for periods, the helper functions for creating durations follows the same format, but they begin with prefix d—for durations. lubridate::dminutes(10); lubridate::dhours(2);lubridate::dseconds(10) [1] &quot;600s (~10 minutes)&quot; [1] &quot;7200s (~2 hours)&quot; [1] &quot;10s&quot; Once we know the diffrence of the helper functions for periods and durations, we can then convert the UTC time to local time. We know that Tanzania is +3 hours of greenwhich, therefore , we create a local.time by add the three hours time on the date variable in the ctd.time. ctd.time.local = ctd.time %&gt;% mutate(local.time = date + lubridate::dhours(3)) ctd.time.local # A tibble: 22 x 5 max.depth lon lat date local.time &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dttm&gt; 1 250 40.6 -10.5 2004-08-18 15:27:46 2004-08-18 18:27:46 2 815 40.8 -10.5 2004-08-18 17:00:01 2004-08-18 20:00:01 3 1015 41.0 -10.5 2004-08-18 20:32:54 2004-08-18 23:32:54 4 930 41.1 -10.5 2004-08-18 22:44:56 2004-08-19 01:44:56 5 785 41.3 -10.5 2004-08-19 00:59:59 2004-08-19 03:59:59 6 945 40.3 -8.83 2004-08-19 11:49:08 2004-08-19 14:49:08 7 860 40.2 -8.83 2004-08-19 13:33:31 2004-08-19 16:33:31 8 810 40.0 -8.83 2004-08-19 15:28:18 2004-08-19 18:28:18 9 650 39.8 -8.83 2004-08-19 17:39:39 2004-08-19 20:39:39 10 630 39.7 -8.83 2004-08-19 19:36:50 2004-08-19 22:36:50 # ... with 12 more rows 10.14 Dealing with Time Zones lubridate package has a with_tz() functions, which change the time moment to the actual time at corresponding time zone. For example here we compute the now time for East Africa , which is three hours above the UTC then each time we define the respective time with the with_tz() function to obtain the local time. Printing the results, we see that the eac is real ahead by three hours to the utc. utc = now() %&gt;%with_tz(tzone = &quot;UTC&quot;) eac = now() %&gt;%with_tz(tzone = &quot;Africa/Nairobi&quot;) utc;eac [1] &quot;2019-06-10 20:23:06 UTC&quot; [1] &quot;2019-06-10 23:23:06 EAT&quot; 10.15 Case study: Working with Date and Time of Argo float observations In this chapter we will learn to work with date and time data in R. We will use the lubridate package developed by Garrett Grolemund and Hadley Wickham ~Grolemund and Wickham (2011). This package makes it easy to work with dates and time. Let’s us load the packages that we will use require(lubridate) require(tidyverse) require(magrittr) require(oce) 10.16 Data We will use the profiles data from Argo within the Indian Ocean. The data was downloaded from the Coriolis Global Data Assembly Center site (ftp://ftp.ifremer.fr/ifremer/argo/) as NetCDF files. 10.17 Data processing The argo profiles were converted from .nc format to data frame. The chunk below briefly describe each step. If you get stuck on the step, consult chapter @ref() that describe looping in details. There are 52 argo floats measured profiles of temperature and salinity as function of density between 2002-11-11 and 2002-11-11 and made a total of 8419 individual profiles. Say you want create a column that show the durationof each argo floats in the Indian Ocean. This information is important because it can help identify for on average how long does each float last or identify floats with the shortest or longest operation in the ocean. To accomplish this task and being able to answer those question, First, the argo floats were aggregated by id. Second, create two variable based on the Id, one variable contain the begin time of the float and the second variable is the end time of the variable. Third, compute the time interval and duration of the float based on the begin and end time. The table 10.3 show the sample of output resulted from the computation in the chunk below; floats.duration = argo.ctd.indian %&gt;% filter(pressure == 5) %&gt;% group_by(ID) %&gt;% summarise(start = first(time), end = last(time), period = interval(start, end) %&gt;% as.duration() %&gt;% as.numeric(&quot;years&quot;), count = n()) %&gt;% arrange(count %&gt;% desc()) floats.duration %&gt;% slice(1,seq(3,52, 6),52) %&gt;% kableExtra::kable(format = &quot;html&quot;, digits = 2, align = &quot;c&quot;, caption = &quot;The period and number of profiles made of randomly selected Argo floats&quot;, col.names = c(&quot;Float ID&quot;, &quot;Begin&quot;, &quot;End&quot;, &quot;Duration (years)&quot;, &quot;Profile&quot;)) %&gt;% kableExtra::column_spec(column = 1:3, width = &quot;3cm&quot;) %&gt;% kableExtra::column_spec(column = 4, width = &quot;4cm&quot;) %&gt;% kableExtra::add_header_above(c(&quot;&quot;, &quot;Time of Argo Float&quot; = 2, &quot;&quot;, &quot;&quot;)) Table 10.3: The period and number of profiles made of randomly selected Argo floats Time of Argo Float Float ID Begin End Duration (years) Profile 5900946 2005-05-26 2014-12-11 9.54 333 1900270 2004-11-26 2014-01-28 9.17 320 1901163 2011-05-03 2018-06-05 7.09 259 1900409 2006-10-16 2012-11-10 6.07 219 1900269 2004-11-26 2010-02-19 5.23 181 1901166 2011-05-08 2014-12-18 3.61 131 1901512 2010-10-20 2013-11-11 3.06 105 1901307 2013-06-26 2016-02-01 2.60 95 1900306 2004-01-07 2005-12-16 1.94 66 1900188 2003-09-25 2004-10-18 1.07 38 2901093 2007-12-05 2008-01-13 0.11 5 floats.duration %&gt;% filter(period &lt; 2) # A tibble: 8 x 5 ID start end period count &lt;dbl&gt; &lt;date&gt; &lt;date&gt; &lt;dbl&gt; &lt;int&gt; 1 1900814 2008-11-22 2010-11-12 1.97 68 2 1900306 2004-01-07 2005-12-16 1.94 66 3 1900170 2003-06-27 2005-03-08 1.70 60 4 1900162 2003-06-26 2005-01-13 1.55 54 5 1900186 2003-09-24 2005-03-13 1.47 52 6 2900564 2005-09-16 2006-11-20 1.18 44 7 1900188 2003-09-25 2004-10-18 1.07 38 8 2901093 2007-12-05 2008-01-13 0.107 5 argo2900564 = argo.ctd.indian %&gt;% filter(ID == 2900564) ggplot(data = argo2900564 %&gt;% filter(pressure == 5), aes(x = longitude, y = latitude, group = ID)) + geom_path()+geom_point() + metR::scale_y_latitude(ticks = 2, expand = c(.1,.1)) + metR::scale_x_longitude(ticks = 5, expand = c(.1,.1)) ggplot(data = argo2900564 %&gt;% filter(pressure &gt;= 5 &amp; pressure &lt; 400 &amp; longitude &gt; 52 &amp; longitude &lt; 58), aes(x = longitude, y = pressure, z = salinity)) + metR::geom_contour_fill(na.fill = TRUE, bins = 20) + scale_y_reverse() + coord_cartesian(expand = FALSE) + scale_fill_gradientn(colours = oce::oce.colors9A(120), name = &quot;Salinity&quot;) References "],
["control-flow.html", "Chapter 11 Control flow 11.1 for loop 11.2 if and else statement 11.3 Functions 11.4 packages", " Chapter 11 Control flow One of the prime purposes of using a computer is to automate a task that would be very tedious to perform by hand. The usual implication is that some task is to be performed over and over again in some systematic way. This chapter will be concerned with the programming concept of a control flow, a feature that is at the heart of nearly every computer algorithm. The two important control flows statements are* count-controlled* loops like for loops and conditional statements such as if-else construct. In this chapter, you’ll learn how to use if statements and for loop to automate programming processes. 11.1 for loop There are often operations where you know ahead of time exactly how many times an operation must be performed before stopping the loop. For example, if we have an array with 365 matrix representing a daily sea surface temperature and we want to convert it to data frame. Doing it one after the other is tedious. Fortunately, computers have some way to move from point 1 (convert the matrix to data frame) to point 2 (convert the second matrix to data frame the analysis) and so on. In R, and nearly all other programming languages, this type of operation is performed by a for() loop. The for loop, tell the computer to iterate—repeat a set of instructions times the defined number. In R, the basic style of for loop is in this form; for (i in 1:20){ DO THE DO } Every for loop in R has four parts. The first is a variable (in this case, i) that will be used to keep track of the number of times through the loop. The second is the index the variable i will take on as it repeats. In our example, i will start by taking the value 1, then 2, then 3 and so on until reaching 20. Third is to place a bound on what will be looped over, denoted in R by the braces {}. Fourth are the commands that will be executed (there can be as many as needed) within the {} lines. Note that this is a good example of using indentation to make clear where the loop starts and ends, as well as the commands to be executed. To better understand how loops work, let’s iterate the sentense R for data analysis five times. for (i in 1:5) print(&quot;R for data analysis&quot;) [1] &quot;R for data analysis&quot; [1] &quot;R for data analysis&quot; [1] &quot;R for data analysis&quot; [1] &quot;R for data analysis&quot; [1] &quot;R for data analysis&quot; We can create vector of monthly temperature numeric values and and month labels. We then display the temperature value for each months with its corresponding month name. THe code can be written as: temperature = c(28.6,28.8,29,28,27.8,27.2,26.8,25,26.2,27.2,27.5,27.8) months = c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;) for (j in 1:length(temperature)){ print(temperature[j]) print(months[j]) } [1] 28.6 [1] &quot;Jan&quot; [1] 28.8 [1] &quot;Feb&quot; [1] 29 [1] &quot;Mar&quot; [1] 28 [1] &quot;Apr&quot; [1] 27.8 [1] &quot;May&quot; [1] 27.2 [1] &quot;Jun&quot; [1] 26.8 [1] &quot;Jul&quot; [1] 25 [1] &quot;Aug&quot; [1] 26.2 [1] &quot;Sep&quot; [1] 27.2 [1] &quot;Oct&quot; [1] 27.5 [1] &quot;Nov&quot; [1] 27.8 [1] &quot;Dec&quot; In this code, the variable temperature and months will start at a value of 1. Then the loop will begin with i = 1. On the first pass through the loop, will be increased by 1 (e.g., x = 2). On the second pass through the loop, (i = 2) x will be increased again by 1, e.g., x = 3, and so on until i = 5. temperature.anomaly = NULL for (j in 1:length(temperature)){ temperature.anomaly[j] = temperature[j]-mean(temperature) } temperature.anomaly [1] 1.108333333 1.308333333 1.508333333 0.508333333 [5] 0.308333333 -0.291666667 -0.691666667 -2.491666667 [9] -1.291666667 -0.291666667 0.008333333 0.308333333 Note that like conditional statements, loops comprising two or more lines or statements requires braces. for (j in seq(1,12,2)){ print(temperature[j]) print(months[j]) } [1] 28.6 [1] &quot;Jan&quot; [1] 29 [1] &quot;Mar&quot; [1] 27.8 [1] &quot;May&quot; [1] 26.8 [1] &quot;Jul&quot; [1] 26.2 [1] &quot;Sep&quot; [1] 27.5 [1] &quot;Nov&quot; Often times you may encounter a situation where you need to allocate an empty object that will store the output of growing data. An empty object will provide space to store the output from each for loop. A more efficient practise is to initiate a list object that is empty then fill it with the output from each iteration. For example in the chunk below we create an empty list object called my.data and then initiate a for loop with and index for (i in 1:length(temperature), which count the month. Then within the for loop the body my.data[[i]]= data.frame(month = months[i], temperature = temperature[i],temperature.anomaly = temperature[i] - mean(temperature)). The body create a data frame of three columns—month, temperature and temperature anomaly for each month and store them in the list object my.data. Once the list is filled, we unlist dplyr::bind_rows(my.data)using the bind_rows() function from dplyr package (Wickham et al. 2019; Wickham 2017). my.data = list() for (i in 1:length(temperature)){ my.data[[i]]= data.frame(month = months[i], temperature = temperature[i], temperature.anomaly = temperature[i] - mean(temperature)) } dplyr::bind_rows(my.data) month temperature temperature.anomaly 1 Jan 28.6 1.108333333 2 Feb 28.8 1.308333333 3 Mar 29.0 1.508333333 4 Apr 28.0 0.508333333 5 May 27.8 0.308333333 6 Jun 27.2 -0.291666667 7 Jul 26.8 -0.691666667 8 Aug 25.0 -2.491666667 9 Sep 26.2 -1.291666667 10 Oct 27.2 -0.291666667 11 Nov 27.5 0.008333333 12 Dec 27.8 0.308333333 11.1.1 For loop on array Often times we receive array data with several matrix embbeded and you may be reguired to convert the array into other format—prefarably data frame so that you can have full control of data processing and visualization. Let’s load an array of sea surface temperature from the working directory with load() function. load(&quot;modis_pemba.RData&quot;) Exploring further the dataset, we notice that its an array with 35 longitude spacing and 7 latitude spacing and 54 matrix collected every month from 2014-11-16 to NA. Being an array, this dataset is not in the right format, since ggplot2 only accept the data frame format. sst$data %&gt;% class();sst$data %&gt;% dim() [1] &quot;array&quot; [1] 35 7 54 We can inspect the longitude of the array sst$longitude [1] 38.81250 38.85418 38.89583 38.93750 38.97918 39.02083 [7] 39.06250 39.10418 39.14583 39.18750 39.22918 39.27083 [13] 39.31250 39.35418 39.39583 39.43750 39.47918 39.52083 [19] 39.56250 39.60418 39.64583 39.68750 39.72918 39.77083 [25] 39.81250 39.85418 39.89583 39.93750 39.97918 40.02083 [31] 40.06250 40.10418 40.14583 40.18750 40.22918 We can also inspect the latitude of the array sst$latitude [1] -5.687505 -5.645833 -5.604169 -5.562505 -5.520833 -5.479169 [7] -5.437505 We can also check the first matrix of the array sst$data[,,1] [,1] [,2] [,3] [,4] [,5] [,6] [1,] NA NA NA NA NA NA [2,] NA NA NA NA NA NA [3,] 29.74117 NA 29.47223 NA NA NA [4,] 29.69169 29.83082 29.60132 29.65081 NA NA [5,] 29.45143 29.52171 29.34098 29.53175 29.59917 30.00438 [6,] 29.29006 29.23627 29.26496 29.22910 29.73974 29.58698 [7,] 29.61782 29.30799 29.30656 29.26711 29.35246 29.24416 [8,] 29.36106 29.38975 29.29365 29.18966 29.38975 29.10288 [9,] 28.90063 29.21548 29.23412 29.08997 29.36393 29.15165 [10,] 28.77656 29.08423 28.97379 29.11866 29.27070 29.09427 [11,] 28.66253 29.04263 28.85832 29.09858 29.05913 29.02614 [12,] 28.72134 28.92645 28.85832 29.08423 28.95083 29.09786 [13,] 28.70341 28.65822 28.87696 29.13443 29.44426 29.41557 [14,] 28.74715 28.77441 28.98239 29.42920 29.48586 29.33525 [15,] 29.18894 28.93577 29.35318 29.59343 29.50522 29.31301 [16,] 29.21404 29.31445 29.53678 29.31445 29.42131 28.91498 [17,] 29.01251 29.53175 29.32736 29.28146 29.37182 29.02614 [18,] 29.16599 29.16097 29.33453 29.24273 29.08280 28.80023 [19,] 29.20902 29.35174 29.34672 29.32592 29.03044 28.60228 [20,] 29.27572 29.49303 29.34098 29.07347 28.80166 28.63527 [21,] 29.60563 29.44497 29.22050 29.16814 28.60300 27.77393 [22,] 29.21261 28.95872 28.95370 28.98024 28.32330 28.05651 [23,] 29.24273 28.92717 28.97952 28.76365 28.55208 28.29676 [24,] 29.24775 29.09355 28.96948 28.86692 28.51048 28.45669 [25,] 29.01466 28.88987 28.75719 28.69552 28.60946 28.61376 [26,] 28.82891 28.78086 28.87912 28.69552 28.69910 28.72134 [27,] 28.85832 28.84613 28.92573 28.72492 28.86190 28.90709 [28,] 28.99387 28.88270 29.00893 28.66898 28.69193 28.70341 [29,] 29.09140 28.99387 28.99602 28.75002 28.76795 28.84971 [30,] 28.89274 28.87410 29.20902 29.03259 28.81385 29.13228 [31,] 29.02829 28.82461 29.03618 29.20185 29.04909 29.04192 [32,] 29.21763 29.01180 29.08280 28.99028 29.14089 28.97737 [33,] 29.17388 28.99889 29.10216 29.11005 28.98454 29.14161 [34,] 29.24560 29.10790 29.17101 29.23986 29.27142 29.24488 [35,] 29.38186 29.22480 29.14734 29.18033 29.33883 29.05913 [,7] [1,] NA [2,] NA [3,] NA [4,] NA [5,] 29.98000 [6,] 29.60491 [7,] 29.23269 [8,] 29.19253 [9,] 29.15667 [10,] 29.14519 [11,] 29.07778 [12,] 29.16814 [13,] 29.06702 [14,] 29.30871 [15,] 29.19970 [16,] 28.95442 [17,] 29.01610 [18,] 28.91426 [19,] NA [20,] NA [21,] NA [22,] NA [23,] 28.39860 [24,] 28.40936 [25,] 28.50905 [26,] 28.86190 [27,] 28.95944 [28,] 28.88844 [29,] 28.98885 [30,] 28.82676 [31,] 29.29078 [32,] 29.00247 [33,] 29.11794 [34,] 29.10431 [35,] 29.23125 Therefore, we need to tidy this dataset from the array into the data frame. We can iterate the conversion with a for loop in the chunk below. Note that the matrix_tb() function used in this loop to convert matrix along with longitude and latitude to data frame I created to help process the data. You can found this code for this function here ## preallocate object as a list() sst.tb = list() ## loop sst for (i in 1:length(sst$time)){ sst.tb[[i]] = matrix_tb(x = sst$longitude, y = sst$latitude, data = sst$data[,,i]) %&gt;% mutate(time = sst$time[i] %&gt;%as.Date()) %&gt;% dplyr::select(date = time,lon = x, lat = y, sst = value) } ## unlist the listed sst and chl files sst.tb = sst.tb %&gt;% bind_rows(sst.tb) sst.tb %&gt;% sample_n(size = 8) # A tibble: 8 x 4 date lon lat sst &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 2015-08-16 39.0 -5.65 26.4 2 2015-06-16 39.4 -5.44 27.5 3 2017-04-16 40.2 -5.56 29.8 4 2018-12-16 39.1 -5.65 29.1 5 2019-01-16 39.4 -5.65 29.0 6 2015-05-16 39.1 -5.52 28.0 7 2015-12-16 39.8 -5.48 29.0 8 2015-03-16 39.0 -5.56 28.8 Here a list object is created before the loop that will store data frame converted from matrix embedded in an array. Note that, in this case, the variable i is playing two roles. It is the loop variable, but it is also the index to the list object. This is one of the most powerful aspects of having a loop variable. NESTED LOOPS Loops are assigned a variable for two purposes.The first is so that the variable can be used to perform some useful function, e.g., as an index to an array or matrix. The second is because loops may exist within other loops, and we need variables to make it clear where in the iteration sequence we are. for (i in 1:N){ for (j in 1:M){ COMMANDS HERE } } The nested loop can be illustrated with the chunk below. In my working directory I have three Excel spreadsheet files containing the monthly average of chlorophyll-a, primary productivity and sea surface temperature. tafiri = dir(pattern = &quot;tafiri_&quot;) tafiri [1] &quot;tafiri_chl.xlsx&quot; &quot;tafiri_pp.xlsx&quot; &quot;tafiri_sst.xlsx&quot; Each file has four sheets containing values from four different sites: Pemba, Zanzibar and Mafia Channels and Exclusive Economic Zone. knitr::include_graphics(&quot;./images/tafiriData.png&quot;) Our task is to automate the process and instruct computer to do two task. First is to read each file from the working directory. The second task is to process the the data from each sheet in the file and tidy the variable in the right order. The later processs involves three steps. First is to assign the variables with appropriate names. The second is to mutate the new variables: month, day site, variables for each sheet and arrange them appropriate. The third process stitch the processed data frame below the former one. In simple language I can explain this code as. The var and sites were created to correspond the order of files in the directory and sites in each file sheet, respectively. Because once each sheet is procesed is stitched, I created and tafiri.data as an empty object to store the files. Once these three files are created, it is ready to iterate the process. The loop goes like this: the i is set to 1 (which is chl) to read the first file, then j set to sheet in each site, which is looped from 1 (Pemba channel) to 4 (EEZ). Then i is set to 2 (pp) and j is again looped from 1 (Pemba) to 4 (EEZ) and so on until i = 3 (SST). A sample of the dataset is shown in table 11.1 ## make a vector of variables. The order must be consistency with the files order in var = c(&quot;chl&quot;, &quot;pp&quot;, &quot;sst&quot;) ## make a vector of site. The order must be consistency with the sheets in files sites = c(&quot;Pemba&quot;, &quot;Zanzibar&quot;, &quot;Mafia&quot;, &quot;EEZ&quot;) ## preallocate an empty object tafiri.data = NULL for (i in 1:length(var)){ for (j in 1:length(sites)){ data = readxl::read_excel(path = tafiri[i], sheet = j)%&gt;% rename(date = 1, year = 2, value = 3) %&gt;% mutate(month = lubridate::month(date), day = 15, site = sites[j], variable = var[i], date = lubridate::make_date(year = year, month = month, day = day)) %&gt;% arrange(date) ## stitch processed data frame from each sheet tafiri.data = tafiri.data %&gt;% bind_rows(data) } } Note that when use i and j variable in a for loop, it’s better to create an empty object with NULL instead of list files with a`list() function Table 11.1: Sample of the tafiri dataset Site Date Day Month Year Variable Value EEZ 2007-05-15 15 5 2007 sst 28.74 Zanzibar 2009-02-15 15 2 2009 pp 707.78 Zanzibar 2014-01-15 15 1 2014 sst 28.80 Pemba 2005-09-15 15 9 2005 chl 0.40 Pemba 2004-02-15 15 2 2004 sst 29.23 Mafia 2007-12-15 15 12 2007 chl 0.95 Zanzibar 2006-07-15 15 7 2006 sst 25.89 Pemba 2011-08-15 15 8 2011 sst 26.07 Pemba 2017-08-15 15 8 2017 sst 26.15 Zanzibar 2006-09-15 15 9 2006 chl 0.39 11.2 if and else statement The second important staments to control the flow of script are if-else constructs, which evaluate an expression and then execute a group of instructions if the expression is true. Conditions can be more complicated than a single question, and if statements can also be combined with multiple questions and different responses based on the answer to each questions. As an example, we might ask, \"Is the surface temperature higher than 26 degreee celcius? and if the answer is yes, respond with that is ideal. An if-else statement like this might be writen in R like this: optimal = 25 measured = 21 if (measured &lt; optimal){ print(&quot;temperature is ideal&quot;) }else{ print(&quot;Temperature is not ideal&quot;) } [1] &quot;temperature is ideal&quot; An if-else stamement in the chunk above first evaluates whether the measured temperature is less than optimaland. If the evaluation is YES or TRUE, the temperature is ideal is displayed. If the answer is NO, the else command, which provides an alternative statement precede and display Temperatue is not ideal. We can also nest as many if-else statements in a one block of code optimal = 25 measured = 30.2 if (measured &lt; optimal){ print(&quot;temperature is ideal&quot;) }else if( measured &gt; 25 &amp; measured &lt;= 26.5){ print(&quot;Temperature is ideal but a bit higher&quot;) }else if(measured &gt;26.5 &amp; measured &lt; 30){ print(&quot;Temperature is not ideal&quot;) }else{ print(&quot;Avoid it at all cost&quot;) } [1] &quot;Avoid it at all cost&quot; Once we know the basic principal of for loop and if-else statment, we can combine them to answer a identify months with ideal temperature versus those with unsuitbale temperature.The combined statement can be written as: temperature = c(28.6,28.8,29,28,27.8,27.2,26.8,25,26.2,27.2,27.5,27.8) months = c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;) ideal = 27 for (i in 1:length(temperature)){ if (temperature[i] &lt; 27){ print(paste(&quot;Temperature for &quot;,months[i], &quot;is ideal&quot;)) } else{ print(paste(&quot;Temperature for &quot;,months[i], &quot;is not ideal&quot;)) } } [1] &quot;Temperature for Jan is not ideal&quot; [1] &quot;Temperature for Feb is not ideal&quot; [1] &quot;Temperature for Mar is not ideal&quot; [1] &quot;Temperature for Apr is not ideal&quot; [1] &quot;Temperature for May is not ideal&quot; [1] &quot;Temperature for Jun is not ideal&quot; [1] &quot;Temperature for Jul is ideal&quot; [1] &quot;Temperature for Aug is ideal&quot; [1] &quot;Temperature for Sep is ideal&quot; [1] &quot;Temperature for Oct is not ideal&quot; [1] &quot;Temperature for Nov is not ideal&quot; [1] &quot;Temperature for Dec is not ideal&quot; for (i in 1:length(temperature)){ if (temperature[i] &lt; ideal){ print(paste(&quot;Temperature for&quot;,months[i],&quot; is ideal&quot;)) }else if( temperature[i] &gt; 25 &amp; temperature[i] &lt;= 26.5){ print(paste(&quot;Temperature for&quot;,months[i],&quot;is ideal but a bit higher&quot;)) }else if(temperature[i] &gt;26.5 &amp; temperature[i] &lt; 30){ print(paste(&quot;Temperature for&quot;,months[i],&quot;is not ideal&quot;)) }else{ print(paste(&quot;Temperature for&quot;,months[i],&quot;avoid it at all cost&quot;)) } } [1] &quot;Temperature for Jan is not ideal&quot; [1] &quot;Temperature for Feb is not ideal&quot; [1] &quot;Temperature for Mar is not ideal&quot; [1] &quot;Temperature for Apr is not ideal&quot; [1] &quot;Temperature for May is not ideal&quot; [1] &quot;Temperature for Jun is not ideal&quot; [1] &quot;Temperature for Jul is ideal&quot; [1] &quot;Temperature for Aug is ideal&quot; [1] &quot;Temperature for Sep is ideal&quot; [1] &quot;Temperature for Oct is not ideal&quot; [1] &quot;Temperature for Nov is not ideal&quot; [1] &quot;Temperature for Dec is not ideal&quot; 11.3 Functions In R, functions are treaed as first–class objects similar to other data types like numeric, character or vector. This is the fundamental property of functional programming languages. 11.3.1 core functions R provides functions for common tasks like plotting, statistics and numerical analysis. In practice, most of the interactions we have with R is through functions—either provided by the base system or in packages. The syntax for calling a function in R is similar to most other programming languages. FOr instance, we use the function mean() to compute the average of a vector of numbers. This function is provided by the base package temperature = c(28.6,28.8,29,28,27.8,27.2,26.8,25,26.2,27.2,27.5,27.8) mean(temperature) [1] 27.49167 Sometimes you might need to extend some function that are neither offered by base nor by other packages. Then you need to define your own function to perform a particular task. For instance, we need to log–transform climatological monthly sea surface height anomaly. Let’s first create ana imaginary ssh anomaly ssh.anomaly = c(1.2,1.1,0.8,0.2,0,-0.1,-0.52,-0.4,0.12,0.58,0.89, 1.02) log(ssh.anomaly) [1] 0.18232156 0.09531018 -0.22314355 -1.60943791 -Inf [6] NaN NaN NaN -2.12026354 -0.54472718 [11] -0.11653382 0.01980263 We notice that log() function returns NaN for negative elements and Inf for zeros. only positive elements returned with numeric values. From the results above, it is clear that log() function never works with negative and zeros. Therefore, for this kind of data like the ssh anomaly where numeric values ranges from negative to positive with some instance of zeros, another function for transofrmation is needed. Unfortunately there is none at present from R. That’s when creating your own functions matters. 11.3.2 user defined functions R has a special syntax for defining functions. Like other programming languages, R offers a function() sysntax that allows to speficy the function name, parameters along with body of statements that executes and produce results and a return() funtion to output the result. The syntax for defining function is similar to that of creating objects, using the assignment operator either &lt;- or = depending on your preference. For instance, We create a function and assign it a name inverse_hyperbolic, which transform data close to logged but allows for zeros and negative, which log() function returns infinity and NaN, respectively. inverse_hyperbolic = function(x = &quot;a numeric, a data frame or matrix&quot;){ output = log(x + sqrt(x^2 + 1)) return(output) } It’s not necessary to specify the return value in function. The principal is that the last item evaluated in the function is automatically considered as the return value. However, we defined the return() for inverse_hyperbolic functionw to print out the result. Using the function we created is very simple, for instance we can tranform the ssh anaomaly with the function we just created as highlighted in the chunk below; ssh.anomaly.transofrm = inverse_hyperbolic(ssh.anomaly) ssh.anomaly.transofrm [1] 1.01597313 0.95034693 0.73266826 0.19869011 0.00000000 [6] -0.09983408 -0.49902844 -0.39003532 0.11971385 0.55159956 [11] 0.80141549 0.89544525 The literal name of our function inverse-hyperbolic, corresponds to the function object, while the ssh.anomaly.transofrm is the result of the call returned by the input ssh.anomaly after evaluated with inverse_hypperbolic() function. We can also create a function that repeate common task ## This function convert matrix into a data frame that is suibtable for tidyverse ecosystem. ## matrix_tb &lt;- function(x=&quot;supply the vector containing the the x value of the array&quot;, y = &quot; supply the vector containing the y value of the array&quot;, data = &quot;supply the the matrix from the array&quot;){ if(!is.matrix(data)){ stop(&quot;you supplied unsupported file, only matrix format file is acceptable&quot;) }else{ require(magrittr) require(tidyverse) dimension &lt;- data.frame(x, data) %&gt;% dim() output &lt;- data.frame(x, data) %&gt;% tidyr::gather(key = &quot;lati&quot;, value = &quot;value&quot;,2:dimension[2]) %&gt;% dplyr::mutate(y = rep(y, each = dimension[1])) %&gt;% dplyr::select(x,y, value) %&gt;% tibble::as_tibble() return(output) } } 11.4 packages In R packages refers to a collection of functions bundled together. In addition to functions, an R package can also contain datasets along with the dependincies. When you start R, the base packages is loaded also by default. The base packages contain basic functions for arithmetic, importing and exporting of dataset, plotting and numerous other simple tasks. In additon to base packages, there are thousands of packages avaialble in the CRAN covering. We can install these packages from R using the install.packages() function—which downloads the sources files for the packages from CRAN mirror websites and store the package in a local repository. We neeed to call install.packages() once to install the package(s). Once the package is installed, we use either library() or require() to load the functions into the workspace. We use several packages in this book and load them with the require() function. require(EnvStats) References "],
["functions-and-packages.html", "Chapter 12 Functions and packages 12.1 packages 12.2 function pipelines", " Chapter 12 Functions and packages In R, functions are treaed as first–class objects similar to other data types like numeric, character or vector. This is the fundamental property of functional programming languages. 12.0.1 core functions R provides functions for common tasks like plotting, statistics and numerical analysis. In practice, most of the interactions we have with R is through functions—either provided by the base system or in packages. The syntax for calling a function in R is similar to most other programming languages. FOr instance, we use the function mean() to compute the average of a vector of numbers. This function is provided by the base package temperature = c(28.6,28.8,29,28,27.8,27.2,26.8,25,26.2,27.2,27.5,27.8) mean(temperature) [1] 27.49167 Sometimes you might need to extend some function that are neither offered by base nor by other packages. Then you need to define your own function to perform a particular task. For instance, we need to log–transform climatological monthly sea surface height anomaly. Let’s first create ana imaginary ssh anomaly ssh.anomaly = c(1.2,1.1,0.8,0.2,0,-0.1,-0.52,-0.4,0.12,0.58,0.89, 1.02) log(ssh.anomaly) [1] 0.18232156 0.09531018 -0.22314355 -1.60943791 -Inf [6] NaN NaN NaN -2.12026354 -0.54472718 [11] -0.11653382 0.01980263 We notice that log() function returns NaN for negative elements and Inf for zeros. only positive elements returned with numeric values. From the results above, it is clear that log() function never works with negative and zeros. Therefore, for this kind of data like the ssh anomaly where numeric values ranges from negative to positive with some instance of zeros, another function for transofrmation is needed. Unfortunately there is none at present from R. That’s when creating your own functions matters. 12.0.2 user defined functions R has a special syntax for defining functions. Like other programming languages, R offers a function() sysntax that allows to speficy the function name, parameters along with body of statements that executes and produce results and a return() funtion to output the result. The syntax for defining function is similar to that of creating objects, using the assignment operator either &lt;- or = depending on your preference. For instance, We create a function and assign it a name inverse_hyperbolic, which transform data close to logged but allows for zeros and negative, which log() function returns infinity and NaN, respectively. inverse_hyperbolic = function(x = &quot;a numeric, a data frame or matrix&quot;){ output = log(x + sqrt(x^2 + 1)) return(output) } It’s not necessary to specify the return value in function. The principal is that the last item evaluated in the function is automatically considered as the return value. However, we defined the return() for inverse_hyperbolic functionw to print out the result. Using the function we created is very simple, for instance we can tranform the ssh anaomaly with the function we just created as highlighted in the chunk below; ssh.anomaly.transofrm = inverse_hyperbolic(ssh.anomaly) ssh.anomaly.transofrm [1] 1.01597313 0.95034693 0.73266826 0.19869011 0.00000000 [6] -0.09983408 -0.49902844 -0.39003532 0.11971385 0.55159956 [11] 0.80141549 0.89544525 The literal name of our function inverse-hyperbolic, corresponds to the function object, while the ssh.anomaly.transofrm is the result of the call returned by the input ssh.anomaly after evaluated with inverse_hypperbolic() function. We can also create a function that repeate common task ## This function convert matrix into a data frame that is suibtable for tidyverse ecosystem. ## matrix_tb &lt;- function(x=&quot;supply the vector containing the the x value of the array&quot;, y = &quot; supply the vector containing the y value of the array&quot;, data = &quot;supply the the matrix from the array&quot;){ if(!is.matrix(data)){ stop(&quot;you supplied unsupported file, only matrix format file is acceptable&quot;) }else{ require(magrittr) require(tidyverse) dimension &lt;- data.frame(x, data) %&gt;% dim() output &lt;- data.frame(x, data) %&gt;% tidyr::gather(key = &quot;lati&quot;, value = &quot;value&quot;,2:dimension[2]) %&gt;% dplyr::mutate(y = rep(y, each = dimension[1])) %&gt;% dplyr::select(x,y, value) %&gt;% tibble::as_tibble() return(output) } } 12.1 packages In R packages refers to a collection of functions bundled together. In addition to functions, an R package can also contain datasets along with the dependincies. When you start R, the base packages is loaded also by default. The base packages contain basic functions for arithmetic, importing and exporting of dataset, plotting and numerous other simple tasks. In additon to base packages, there are thousands of packages avaialble in the CRAN covering. We can install these packages from R using the install.packages() function—which downloads the sources files for the packages from CRAN mirror websites and store the package in a local repository. We neeed to call install.packages() once to install the package(s). Once the package is installed, we use either library() or require() to load the functions into the workspace. We use several packages in this book and load them with the require() function. 12.2 function pipelines In recent years, efforts have been made to implement an R analogy to Unix pipelines, with a notation for chaining function calls. For example, the mean of temperature from a vector may be computed conventionally in R with temperature = c(28.6,28.8,29,28,27.8,27.2,26.8,25,26.2,27.2,27.5,27.8) mean(temperature) [1] 27.49167 However, the modern R allows to pipe the sequence of operation with the magrittr package (Bache and Wickham 2014). The principal function provided by the magritrr package is the %&gt;% widely known as the pipe. The %&gt;% is a binary operator that takes the item on its left and supplies it as the first argument to the functio on its right. For example, the code for computing the mean of temperature can be written as: require(&quot;magrittr&quot;) temperature %&gt;% mean() [1] 27.49167 The power of %&gt;% is clearly obvious when you desire to perform multiple functions. For instance, if we are interested to use the oce package to read CTD file, remove the upcast and retain with the downcast measured values, align it th standard depth of 10 meter interval, select measured CTD values from the 5 meters and above and then plot the profile of temperature. The chunk below is piped to illustrate the expression above oce::read.ctd(file = &quot;e:/Data Manipulation/ctd_algoa/dstn047.cnv&quot;) %&gt;% ## read oce::ctdTrim(method = &quot;SBE&quot;) %&gt;% ## remove the upcast oce::ctdDecimate(p = 10) %&gt;% ## align to standard depth subset(pressure &gt;=5) %&gt;% ## pick only values from 5 meters and above oce::plotProfile(xtype = c(&quot;temperature&quot;), lty = 5, col = &quot;red&quot;) ## plot Integrating %&gt;% operator makes for more efficient and legible code in R. It’s an efficient in that it does’nt save unnecessary intermediate objects. Its legible in that you can read the code as we read normal words in the sentence because he %&gt;% is referred to then. The code above can be read as: first read the ctd file with oce::read.ctd(), then remove the upcast measurements with oce::ctdTrim(), then align the pressure to standard depth of 10 meter interval with oce::ctdDecimate(), then select only the value from 5 meters and above and finally plot the profile of temperature with oce::plotProfile(). Piping with R becomes ob and readable when is indented with line breaks, for instance ssh.anomaly = c(1.2,1.1,0.8,0.2,0,-0.1,-0.52,-0.4,0.12,0.58,0.89, 1.02) ssh.anomaly %&gt;% inverse_hyperbolic() %&gt;% lead() [1] 0.95034693 0.73266826 0.19869011 0.00000000 -0.09983408 [6] -0.49902844 -0.39003532 0.11971385 0.55159956 0.80141549 [11] 0.89544525 NA Some functions have arguments that need vector data. When you use the pipe %&gt;% operator for such functions in a data frame, you get an error stn.tb %&gt;% cor(x = pressure, y = temperature) You notice that, the cor() function only requires x and y arguments. When you pipe a dataset with variables in data frame with the cor(), you get an error because the cor() does not know how to handle the data frame as argument with the %&gt;% operator. However, with the %$% treat the data frame as individual vectors and hence accepted with the %$% operator. as: stn.tb %$% cor(x = pressure, y = temperature) [1] -0.9137736 you can also combine the %&gt;% and %$% operators in the chunk code to perform a particular task. For example, we are interested with the association of fluorescence with increasing pressure, but the pressure is restricted to the upper 200 meter deep. The code for this can be written as: stn.tb %&gt;% filter(pressure &lt;=200) %$% cor(x = pressure, y = fluorescence) [1] -0.8925602 References "],
["raster.html", "Chapter 13 Raster Data 13.1 Introduction to Raster data 13.2 Assign projection and Reproject Raster Data in R 13.3 Raster resolution 13.4 Raster Calculation 13.5 Multi-bands satellite imagery 13.6 Manipulate raster 13.7 Raster time series data 13.8 comparing NDVI from Two different sites", " Chapter 13 Raster Data Questions what is a raster dataset? What tools/functions are used to import raster in R? How to I work with and plot raster data in R How missing or bad data in R are handled with R Objectives Describe the fundamental attributes of a raster dataset Explore raster attributtes and metadata Import raster dataset into R workspace visualize raster object Distinguish single versus multi-bands rasters 13.1 Introduction to Raster data This this section introduce you to the fundamental principles, packages and metadata/raster attributes that are needed to work with raster data in R. The section discuss some of the core metadata elements that you need to understand to work with rasters in R, including CRS and resolution. Furthermore, missing and bad data values stored in raster will be explored and techniques to handles these elements will be illustrated. The book use several packages including the tidyverse ecosystem (Wickham 2017)—with popular packages like the ggplot2(Wickham 2016) and dplyr (Wickham et al. 2019). The widely used packages for handling raster and vector data like raster (Hijmans 2019), sp(Bivand, Pebesma, and Gomez-Rubio 2013), sf (Pebesma 2018) and rgdal (Bivand, Keitt, and Rowlingson 2019) make core tools in this book. R needs these packages imported into the environment to use their functions, which can easily done with the require() function. require(sf) require(sp) require(raster) require(tidyverse) require(metR) 13.1.1 GeoTiff A popular public domain raster data format is the GeoTIFF format. If maximum portability and platform independence is important, this file format may be a good choice. 13.1.2 Explore the raster attribute One of the common raster file is the *GeoTiff** that embed tags of metadata information bout the raster file. This metadata provide the information of the file and hence help us understand the internal structure of the file. This information can be accessed with the GDALinfo() function (Bivand, Keitt, and Rowlingson 2019). Looking at the metadata help us have a glimpse of the file before even the file is imported into the workspace. rgdal::GDALinfo(&quot;e:/GIS/Tanzania spatial data Bank/Lake_Tanganyika_Bathymetry/Lake_Tanganyika_Bathymetry/grid/tanganyika_dbm (2013_10_23 20_44_28 UTC).tif&quot;) rows 6589 columns 2575 bands 1 lower left origin.x 29.05769 lower left origin.y -8.811174 res.x 0.000833 res.y 0.000833 ysign -1 oblique.x 0 oblique.y 0 driver GTiff projection +proj=longlat +datum=WGS84 +no_defs file e:/GIS/Tanzania spatial data Bank/Lake_Tanganyika_Bathymetry/Lake_Tanganyika_Bathymetry/grid/tanganyika_dbm (2013_10_23 20_44_28 UTC).tif apparent band summary: GDType hasNoDataValue NoDataValue blockSize1 blockSize2 1 Float32 TRUE -3.402823e+38 128 128 apparent band statistics: Bmin Bmax Bmean Bsd 1 -1500 0 -607.0582 427.0307 Metadata: AREA_OR_POINT=Area 13.1.3 Read a GeoTIFF raster data Once you have a glimpse of the information of the raster—for example the above information show that the tiff contain elevation values and provide the summary statistics of the elevation with minimum value of 0 and maximum value of 1500 with average of 607. It also show the geographical extent with minimum longitude o 29.05769 and maximul latitude of -8.811174. Furthermore, the metadata tell us that the file is projected with World Geodetic System (WGS84) and the cell has a horizontal resolution of 0.000833 degree. Once we know this information, we can read the file with the raster function of raster package (Hijmans 2019) For this example, we use the bathmetry information of Lake Tanganyika found in Africa. It the world’s longest freshwater lake, the second largest largest by volume, and the second deepest lake in the world after lake Baikal in Siberia (“Lake Tanyanyika,” n.d.). lt.bath = raster(&quot;e:/GIS/Tanzania spatial data Bank/Lake_Tanganyika_Bathymetry/Lake_Tanganyika_Bathymetry/grid/tanganyika_dbm (2013_10_23 20_44_28 UTC).tif&quot;) We can summary function to look at the statistics of the bathmetry of this lake. Looking at the descriptive statistics, we notice that the lake has the depth range from 0 to 1500 m and there is no cell without a value. lt.bath %&gt;% summary() tanganyika_dbm_.2013_10_23_20_44_28_UTC. Min. -1500.0000 1st Qu. -958.3341 Median -529.9788 3rd Qu. -250.8468 Max. 0.0000 NA&#39;s 0.0000 There are times when a raster file does not show the summary statistics. When this occurs you can manually calculate the cell values using the setMinMax() function. lt.bath %&gt;% setMinMax() class : RasterLayer dimensions : 6589, 2575, 16966675 (nrow, ncol, ncell) resolution : 0.000833, 0.000833 (x, y) extent : 29.05769, 31.20266, -8.811174, -3.322537 (xmin, xmax, ymin, ymax) crs : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 source : e:/GIS/Tanzania spatial data Bank/Lake_Tanganyika_Bathymetry/Lake_Tanganyika_Bathymetry/grid/tanganyika_dbm (2013_10_23 20_44_28 UTC).tif names : tanganyika_dbm_.2013_10_23_20_44_28_UTC. values : -1500, 0 (min, max) 13.1.4 View Raster Coordinate Reference System A spatial reference system (SRS) or coordinate referene system (CRS) is a coordinate-based local, regional or global system used to locate geographical entities (“Coordinate Reference System,” n.d.). A spatial reference system defines a specific map projection and transofrmation betweeen diffferent spatial reference systems. We can look the embedded CRS in the raster file with teh crs() function from raster package. lt.bath %&gt;% crs() CRS arguments: +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 We notice that the raster file is projected in Word Geodetic System of 1984 (WGS84). In summary the projection +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 tell us as follows: proj=longlat: the projectionis in longitude and latitude decimal degrees datum=WGS84: the datum is WGS84 and it referes to the 0,0 refereence for the coordinate system used in the projection ellps=WGS84: the ellipsoidd—how the earth’s roundness is calculated for the data is WGS84 13.1.5 Dealing with missing data in raster Raster data often has NoData value to represent the absence of data. This is a value assigned to pixels where data is missing or absent. The raster comes with a ract1 contain both the elevation and bathmetry information. But if you want to plot area contour from the coastline offshore, you will need to remove elevation information and this is when you assign the elevation pixel with the NoData value. The values that is conventionally used to represent missing data varies by ther raster data type. For example, for floating points rasters, the figure -3.4e+38 is commonly used while for integers a figure -9999 is common. However, when raster are imported, R assigns these missing cell with NA. tz.bath = raster(&quot;e:/GIS/ROADMAP/Etopo 1/Tanzania_etopo1/tanz1_-3432.asc&quot;) tz.bath %&gt;% summary() tanz1_.3432 Min. -4892 1st Qu. -3519 Median -271 3rd Qu. 615 Max. 5435 NA&#39;s 0 13.2 Assign projection and Reproject Raster Data in R Sometimes we encounter raster datasets that do not “line up” when plotted or analyzed. Rasters that don’t line up are most often in different Coordinate Reference Systems (CRS). This section explains how to deal with rasters in different, known CRSs. It will walk though reprojecting rasters in R using the projectRaster() function in the raster package. We can assess the projection of the two raster data we loaded earlier with the crs() function from raster package. Let’s begin with the bathmetry raster of Lake Tanganyika crs(lt.bath) CRS arguments: +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 We notice that the bathmetry raster of Lake Tanganyika has defined coordinate reference system—WGS84. let us also check the bathmetry data from the coastal water of Tanzania using the same crs() function. crs(tz.bath) CRS arguments: NA Unfortunately, the bathmetry raster of coastal water of Tanzania lack the coordinate reference systm—this idicate that the projection is not defined yet. fortunate, raster package has projectRaster() function that allows to reproject raster without defined CRS or reproject a raster from one CRS into another. Since the lt.bath has the projection, we can use its projection to define the missing coordinate system in tz.bath raster file. Because we need to define a projection of the missing raster, we simply use the crs() function to copy the projection of lt.bath into the tz.bath as the code block illustrate crs(tz.bath) = crs(lt.bath) We can check the coordinate of the two files if they are correct crs(tz.bath); crs(lt.bath) CRS arguments: +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 CRS arguments: +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 Since we know that the coastal water of Tanzania lies at zone 37 south, we can simply assign the appropriate projection and then transform the bathmetry from WGS84 to UTM zone 37 south. Since we know the text of the zone, let us define it tzutm = &quot;+proj=utm +zone=37 +south +datum=WGS84 +units=m +no_defs&quot; We then use the projectExtent() function to transform the CRS from WGS84 to UTM Zone 37 south tz.bath.utm = projectExtent(tz.bath, tzutm) Then check the files projections. Instead of using the crs() to assess the type of projection, we use the projection() function instead. tz.bath %&gt;% projection(asText = F); tz.bath.utm %&gt;% projection(asText = F) CRS arguments: +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 CRS arguments: +proj=utm +zone=37 +south +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 13.3 Raster resolution Let’s next have a look at the resolution of reprojected tz.bath.utm and the tz.bath files. tz.bath.utm %&gt;% res();tz.bath %&gt;% res() [1] 1859.258 1860.036 [1] 0.01666667 0.01666667 We notice that the horizontal resolution of projected utm tz.bath.utm file is given in meters of 1859.258 by 1860.036. But the the wgs84 tz.bath is given in degree of 0.01666667 by 0.01666667. Therefore, depending on how you intend to use the raster in analysis and mapping, you will find yourself resonate between geographical coordianate system (WGS) and Universal Transeverse Mercator (UTM). The former is in degree while the later is in meters. 13.4 Raster Calculation Often times we want to perform calculations on two or more rasters to create a new output raster. For example, if we are interested in mapping the heights of trees across an entire field site, we might want to calculate the difference between the Digital Surface Model (DSM, tops of trees) and the Digital Terrain Model (DTM, ground level). The resulting dataset is referred to as a Canopy Height Model (CHM) and represents the actual height of trees, buildings, etc. with the influence of ground elevation removed. 13.5 Multi-bands satellite imagery High resolution digital globe multispectral image. Let us explore the image first rgdal::GDALinfo(&quot;e:/GIS/trevor/053575070010_01/053575070010_01_P001_MUL/14AUG19082006-M2AS-053575070010_01_P001.TIF&quot;) rows 5013 columns 2340 bands 8 lower left origin.x 431076 lower left origin.y 7020086 res.x 2 res.y 2 ysign -1 oblique.x 0 oblique.y 0 driver GTiff projection +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs file e:/GIS/trevor/053575070010_01/053575070010_01_P001_MUL/14AUG19082006-M2AS-053575070010_01_P001.TIF apparent band summary: GDType hasNoDataValue NoDataValue blockSize1 blockSize2 1 UInt16 FALSE 0 224 2340 2 UInt16 FALSE 0 224 2340 3 UInt16 FALSE 0 224 2340 4 UInt16 FALSE 0 224 2340 5 UInt16 FALSE 0 224 2340 6 UInt16 FALSE 0 224 2340 7 UInt16 FALSE 0 224 2340 8 UInt16 FALSE 0 224 2340 apparent band statistics: Bmin Bmax Bmean Bsd 1 214 1012 241.0628 6.579988 2 172 1235 203.5383 11.027946 3 170 2047 240.6267 24.320807 4 101 2047 212.6599 37.599717 5 49 1528 128.3042 32.559744 6 100 2047 306.6574 67.605365 7 56 1729 353.3486 106.036615 8 32 1723 333.0844 98.604913 Metadata: AREA_OR_POINT=Area TIFFTAG_COPYRIGHT=(C) COPYRIGHT 2014 DigitalGlobe, Inc., Longmont CO USA 80503 TIFFTAG_DATETIME=2014:08:19 12:17:12 TIFFTAG_IMAGEDESCRIPTION={ bandList = [ 6; 2; 3; 7; 4; 8; 5; 9; ] } TIFFTAG_MAXSAMPLEVALUE=2047 TIFFTAG_MINSAMPLEVALUE=0 dg = raster(&quot;e:/GIS/trevor/053575070010_01/053575070010_01_P001_MUL/14AUG19082006-M2AS-053575070010_01_P001.TIF&quot;) When we explore the raster file, it tell that the class is raster layer and it has eight bands with spatial resolution of 2 meters. dg class : RasterLayer band : 1 (of 8 bands) dimensions : 5013, 2340, 11730420 (nrow, ncol, ncell) resolution : 2, 2 (x, y) extent : 431076, 435756, 7020086, 7030112 (xmin, xmax, ymin, ymax) crs : +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 source : e:/GIS/trevor/053575070010_01/053575070010_01_P001_MUL/14AUG19082006-M2AS-053575070010_01_P001.TIF names : X14AUG19082006.M2AS.053575070010_01_P001 values : 214, 1012 (min, max) Check for geographical coordinate system dg %&gt;% projection() [1] &quot;+proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; check for the resolution res(dg) [1] 2 2 13.5.1 Rater Stacks The multi-bands raster in R are handled easily with the stack() function, which bling all bands of multi-band raster dg.bands = stack(&quot;e:/GIS/trevor/053575070010_01/053575070010_01_P001_MUL/14AUG19082006-M2AS-053575070010_01_P001.TIF&quot;) We then preview the attributes embedded in the stacked raster object dg.bands class : RasterStack dimensions : 5013, 2340, 11730420, 8 (nrow, ncol, ncell, nlayers) resolution : 2, 2 (x, y) extent : 431076, 435756, 7020086, 7030112 (xmin, xmax, ymin, ymax) crs : +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 names : X14AUG190//_01_P001.1, X14AUG190//_01_P001.2, X14AUG190//_01_P001.3, X14AUG190//_01_P001.4, X14AUG190//_01_P001.5, X14AUG190//_01_P001.6, X14AUG190//_01_P001.7, X14AUG190//_01_P001.8 min values : 214, 172, 170, 101, 49, 100, 56, 32 max values : 1012, 1235, 2047, 2047, 1528, 2047, 1729, 1723 We can view the attributes of each band in the stack with a single output using the $ operator and call the layer dg.bands@layers [[1]] class : RasterLayer band : 1 (of 8 bands) dimensions : 5013, 2340, 11730420 (nrow, ncol, ncell) resolution : 2, 2 (x, y) extent : 431076, 435756, 7020086, 7030112 (xmin, xmax, ymin, ymax) crs : +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 source : e:/GIS/trevor/053575070010_01/053575070010_01_P001_MUL/14AUG19082006-M2AS-053575070010_01_P001.TIF names : X14AUG19082006.M2AS.053575070010_01_P001.1 values : 214, 1012 (min, max) [[2]] class : RasterLayer band : 2 (of 8 bands) dimensions : 5013, 2340, 11730420 (nrow, ncol, ncell) resolution : 2, 2 (x, y) extent : 431076, 435756, 7020086, 7030112 (xmin, xmax, ymin, ymax) crs : +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 source : e:/GIS/trevor/053575070010_01/053575070010_01_P001_MUL/14AUG19082006-M2AS-053575070010_01_P001.TIF names : X14AUG19082006.M2AS.053575070010_01_P001.2 values : 172, 1235 (min, max) [[3]] class : RasterLayer band : 3 (of 8 bands) dimensions : 5013, 2340, 11730420 (nrow, ncol, ncell) resolution : 2, 2 (x, y) extent : 431076, 435756, 7020086, 7030112 (xmin, xmax, ymin, ymax) crs : +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 source : e:/GIS/trevor/053575070010_01/053575070010_01_P001_MUL/14AUG19082006-M2AS-053575070010_01_P001.TIF names : X14AUG19082006.M2AS.053575070010_01_P001.3 values : 170, 2047 (min, max) [[4]] class : RasterLayer band : 4 (of 8 bands) dimensions : 5013, 2340, 11730420 (nrow, ncol, ncell) resolution : 2, 2 (x, y) extent : 431076, 435756, 7020086, 7030112 (xmin, xmax, ymin, ymax) crs : +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 source : e:/GIS/trevor/053575070010_01/053575070010_01_P001_MUL/14AUG19082006-M2AS-053575070010_01_P001.TIF names : X14AUG19082006.M2AS.053575070010_01_P001.4 values : 101, 2047 (min, max) [[5]] class : RasterLayer band : 5 (of 8 bands) dimensions : 5013, 2340, 11730420 (nrow, ncol, ncell) resolution : 2, 2 (x, y) extent : 431076, 435756, 7020086, 7030112 (xmin, xmax, ymin, ymax) crs : +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 source : e:/GIS/trevor/053575070010_01/053575070010_01_P001_MUL/14AUG19082006-M2AS-053575070010_01_P001.TIF names : X14AUG19082006.M2AS.053575070010_01_P001.5 values : 49, 1528 (min, max) [[6]] class : RasterLayer band : 6 (of 8 bands) dimensions : 5013, 2340, 11730420 (nrow, ncol, ncell) resolution : 2, 2 (x, y) extent : 431076, 435756, 7020086, 7030112 (xmin, xmax, ymin, ymax) crs : +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 source : e:/GIS/trevor/053575070010_01/053575070010_01_P001_MUL/14AUG19082006-M2AS-053575070010_01_P001.TIF names : X14AUG19082006.M2AS.053575070010_01_P001.6 values : 100, 2047 (min, max) [[7]] class : RasterLayer band : 7 (of 8 bands) dimensions : 5013, 2340, 11730420 (nrow, ncol, ncell) resolution : 2, 2 (x, y) extent : 431076, 435756, 7020086, 7030112 (xmin, xmax, ymin, ymax) crs : +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 source : e:/GIS/trevor/053575070010_01/053575070010_01_P001_MUL/14AUG19082006-M2AS-053575070010_01_P001.TIF names : X14AUG19082006.M2AS.053575070010_01_P001.7 values : 56, 1729 (min, max) [[8]] class : RasterLayer band : 8 (of 8 bands) dimensions : 5013, 2340, 11730420 (nrow, ncol, ncell) resolution : 2, 2 (x, y) extent : 431076, 435756, 7020086, 7030112 (xmin, xmax, ymin, ymax) crs : +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 source : e:/GIS/trevor/053575070010_01/053575070010_01_P001_MUL/14AUG19082006-M2AS-053575070010_01_P001.TIF names : X14AUG19082006.M2AS.053575070010_01_P001.8 values : 32, 1723 (min, max) plotRGB(dg.bands, r = 3, g = 2, b = 1, scale = 800, stretch = &quot;hist&quot;) b1 = raster(&quot;e:/bookdown/spatil_r/landsat_clip_2014/clip_LC81660652014164LGN00_B1.tif&quot;) b2 = raster(&quot;e:/bookdown/spatil_r/landsat_clip_2014/clip_LC81660652014164LGN00_B2.tif&quot;) b3 = raster(&quot;e:/bookdown/spatil_r/landsat_clip_2014/clip_LC81660652014164LGN00_B3.tif&quot;) b4 = raster(&quot;e:/bookdown/spatil_r/landsat_clip_2014/clip_LC81660652014164LGN00_B4.tif&quot;) b5 = raster(&quot;e:/bookdown/spatil_r/landsat_clip_2014/clip_LC81660652014164LGN00_B5.tif&quot;) b6 = raster(&quot;e:/bookdown/spatil_r/landsat_clip_2014/clip_LC81660652014164LGN00_B6.tif&quot;) b7 = raster(&quot;e:/bookdown/spatil_r/landsat_clip_2014/clip_LC81660652014164LGN00_B7.tif&quot;) stack(b1, b2, b3) %&gt;% plotRGB(r = 3, g = 2, b = 1, scale = TRUE) 13.6 Manipulate raster 13.7 Raster time series data 13.8 comparing NDVI from Two different sites References "],
["plotting-in-r-with-ggplot2.html", "Chapter 14 Plotting in R with ggplot2 14.1 A dataset 14.2 Components of ggplot objects {components} 14.3 Building a plot 14.4 Customize glidline and axis labels 14.5 Modify the position of the legend 14.6 Basic plots 14.7 Linegraph 14.8 Advanced plots 14.9 Colour 14.10 size 14.11 scaling 14.12 Guides 14.13 Add-on packages 14.14 ggridges 14.15 Varying fill colors along the x axis 14.16 metR", " Chapter 14 Plotting in R with ggplot2 The ability to create visualizations—graphical representations of data is an important step to convery your results—information and findings to others. In this chapter, we illustrate how you can use visualize your data and create elegant graphics. One of things that makes R such a great tools is it’s data visualization capabilities. R has many systems for visualization and creating plots, some of which are—base R graphics, lattice and ggplot2, but we focus on the use of ggplot2. The ggplot2 package is a phenomenal tool for creating graphics in R. It provide a unifying framework—a grammar of graphics for describing and building graphs. Just as the grammar of language helps you construct meaningful sentences out of words, the Grammar of Graphics helps you construct graphicss out of different visual elements. This grammar provides a way to talk about parts of a visual plot: all the circles, lines, arrows, and text that are combined into a diagram for visualizing data. Originally developed by Leland Wilkinson, the Grammar of Graphics was adapted by Hadley Wickham (2016) to describe the components of a plot The ggplot2 package provides a powerful and flexible approach to data visualization, and its is suitable both for rapdi exploration of different visualization approaches and for producing carefully crafted publication–quality figures. However, getting ggplot2 to make figures that look exactly the way you want them to, can sometimes be challenging and beginners and expert alike can get consufed by themes, scales, coords, guides or facets. ggplot2 further organizes these components into layers, where each layer displays a single type of (highly configurable) geometric object. Even the most experienced R users need to refer to ggplot2 Cheat Sheet while creating elegant graphics, we will demonstrate step–by–step how to get the most out of ggplot2 package, including how to choose and customize scales, how to theme plots, and when to use add-in packages that extend the ggplot2 capabilities. Some of the extension of gggplot2 that we will introuduce to your include ggspatial, metR, ggrepel, cowplot, etc. Rather than loading these extension packages with require() function, we’ll call their functions using the :: notation. This will help make it clear which funcions are built into ggplot2, and which comes from extensions. require(tidyverse) require(magrittr) load addition functions source(&quot;e:/Data Manipulation/semba_functions.R&quot;) 14.1 A dataset We’re using a CTD profile data along the coastal water of Tanzania collected in August 2004 with Algoa. I have processed and cleaned the profile data from a CTD instrument, and created a very basic and simple dataset for us to start with. I have tidy the data into a data frame and it contains 10 variables collected at 22 stations (Table 14.1). These variables include the time, longitude and latitude coordinates and the name at each station. It also contains the measured profile of temperature, salinity, oxygen, and fluorescence, spar, par and density as function of pressure. Table 14.1: A sample of CTD profile dataset. For visibility of all the 22 stations, only variables measured at 5 meter standard depth are shown Cast Time Cast Location Measured Variabes Station Date Hour Lon Lat Pressure Temperature Salinity Oxygen Fluorescence st1 2004-08-18 15 40.61 -10.54 5 25.17 33.92 3.93 0.56 st2 2004-08-18 17 40.77 -10.54 5 25.16 34.85 4.47 0.62 st3 2004-08-18 20 40.95 -10.54 5 NA NA NA NA st4 2004-08-18 22 41.12 -10.54 5 NA NA NA NA st5 2004-08-19 0 41.28 -10.54 5 NA NA NA NA st6 2004-08-19 11 40.34 -8.83 5 25.21 34.86 4.48 0.24 st7 2004-08-19 13 40.18 -8.83 5 25.25 34.87 4.52 0.44 st8 2004-08-19 15 40.00 -8.83 5 25.02 34.86 4.59 1.14 st9 2004-08-19 17 39.82 -8.83 5 25.11 34.86 4.64 1.53 st10 2004-08-19 19 39.67 -8.83 5 NA NA NA NA st11 2004-08-22 16 39.60 -9.03 5 25.44 34.91 4.98 1.71 st12 2004-08-23 3 40.26 -7.04 5 25.12 34.87 4.50 0.99 st13 2004-08-23 6 40.10 -7.05 5 25.18 34.87 4.49 0.45 st14 2004-08-23 7 39.93 -7.05 5 25.28 34.88 4.60 0.84 st15 2004-08-23 9 39.76 -7.04 5 25.26 34.89 4.66 1.06 st16 2004-08-23 11 39.59 -7.04 5 25.92 34.88 4.31 0.62 st17 2004-08-23 19 40.07 -5.49 5 25.64 35.19 4.40 0.82 st18 2004-08-23 22 39.90 -5.49 5 25.28 34.90 4.52 0.85 st19 2004-08-24 1 39.56 -5.49 5 25.27 34.90 4.53 0.98 st20 2004-08-24 3 39.40 -5.47 5 25.23 34.89 4.81 1.11 st21 2004-08-24 4 39.24 -5.48 5 25.82 34.93 4.34 0.21 st22 2004-08-24 5 39.10 -5.48 5 26.05 34.95 4.32 0.42 ## make a variable with labelled latitude ctd = ctd %&gt;% filter(lat &gt; -6) %&gt;% mutate(transect = &quot;transect 1&quot;, Lat.label = median(lat)) %&gt;% bind_rows(ctd %&gt;% filter(lat &gt; -8 &amp; lat &lt; -6) %&gt;% mutate(transect = &quot;transect 2&quot;, Lat.label = median(lat)), ctd %&gt;% filter(lat &gt; -10 &amp; lat &lt; -8) %&gt;% mutate(transect = &quot;transect 3&quot;, Lat.label = median(lat)), ctd %&gt;% filter(lat &lt; -10) %&gt;% mutate(transect = &quot;transect 4&quot;, Lat.label = median(lat))) %&gt;% mutate(Lat.label = metR::LatLabel(Lat.label%&gt;% round(2)) %&gt;%as.factor()) 14.2 Components of ggplot objects {components} I have created a simple plot of this data that show scatterplot of temperature versus fluorescence at the four different transects (Fig @fig:fig1299). The plot show the concentration of fluorescence against temperature for the twenty one stations along the coastal water of Tanzania. Figure 14.1: Association of fluorescence concentration per temperature collected in August 2004 Let’s explore in details the key elements used to make figure 14.1: data: The data like the one illustrated in table 14.1. It must be a data frame for ggplot2 read and understand. aesthetics: is used to map the x and y axis for 2–dimensional plot and add the z value for 3–dimensionla plots. It is also used to define visual properties like color, size, shapes or height etc, and. For instance in the figure 14.1, the position along the y-axis is mapped to the concentration of fluorescence and the x - axis is mapped to temperature values. For the points, the color is mapped to the geogrphical location along the transects. Other aesthetics—like size, shape, and transparency have been left at their default settings. geometry; a layer which define the type of plot you want to make, whether is histogram, boxplot, barplot, scatterplot, lineplot etc. coordinate system: used to set a limit for the plot. The cartesian coordinate is the most familiar and common system that is widely used to zoom the plot and does not change the underlying data. scales: scales allows to customize the plot. For instance in figure 14.1 both x and y-axis used continuous data and hence the scale_x_continuous() and scale_y_continuous() were used to modiy the values of the axis. For color, I simply stick on scale_colour_discrete() and customize the legend name. labels: The plot is well labelled and easy to understand. It has title, subtitle, axes and caption for the courtesy of the data. theme: the plot stick on the default theme_gray theme, which has a gray background color and white gridlines, a sans serif font family, and a base font size of 11. We can customize all the propoerties in the theme to suit our standard. 14.3 Building a plot Since you now have a clue of the different layers added to create a plot, its time to work around to create a plot with the ggplot2 package. We use the same profile dataset that used to make figure 14.1. First, you neeed to import the data into your R session. 14.3.1 Plotting layers To create a data visualization using ggplot2 package, we will add layers for each of the plot elements described in section ??. I will take you through step by step of the key ines needed to make such a plot. First make sure the ggplot2 or tidyverse packages are loaded in your R’s session. You can load the package with this code; require(tidyverse) The ggplot2 create a ggplot object, and you initialize the object with the ggplot() function ggplot() The plot above is black with grey background. This is because we have not specified the data and aesthetic arguments inside the ggplot() function. Let’s specify the data, which in our case is the ctd dataset and also specify the x-axis with temperature and y-axis with fluorescence. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence)) Now the plot has gridlines and axis with values and labels—x-axis show the value of temperature and the y-axis show the value of fluorescence concentrations. However, there is no graphics. This is because we have not added any geom yet. Therefore, since we have already specified the data and the aesthetic values, now we can add the geom where we map the aesthetics to columns in the dataset. Let’s add the geom_point() and specify the size to 3 ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence)) + geom_point(size = 3) 14.3.2 Customize legend The plot now show points distributed in the panel plot. But our interest is to color the point at each transect. Next, we add an argument col in the aesthetic that map points that fall along a certain transect with similar color. Since we have a variable called transect in the dataset, we specify this as the code below ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) The plot above show the points are color–coded to refrect the transect in the legend. Note that the colors scheme used in this plot is the default one. Sometimes you will want to customize the aesthetic for all the points in the plot. To change a color scale, you can use the scale_color_viridis_d(). The _d here represent the discrete variable. If you were using the continuous data, you would use scale_color_viridis_c() scheme instead. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d() To change the title of the legend you must specify the name argument in your scale_* function. For instance, we specified scale_colour_viridis_d(name = \"Transects\") to change the legend title for this plot ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects\\n of CTD Casts&quot;) If you want to remove the legend title, we can add a theme layer and specify theme(legend.title = element_blank()) ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;) + theme(legend.title = element_blank()) We can also change the legend title by specifying theme(legend.title = element_blank()) ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects\\nof CTD Casts&quot;) + theme(legend.title = element_text(size = 12, colour = &quot;chocolate&quot;, face = &quot;bold&quot;)) 14.3.3 Working with titles and labels Often times you need to customize the axis labels. By default, the scale label for each scale is the name of the variable in the dataset. We can change the labels with labs() function. The other elements we would like to add are the plot title and subtitle. If you want to label SI unit in the ggplot, use the expression() function. Note that I used the expression() function to express mathematical symbols in the x and y–axis of the plot. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;) 14.4 Customize glidline and axis labels The other layer that we would like to add to customize our plot are the scale_x_continous(), which change the gridline of the x-axis and the scale_y_continuous(), which change the gridlines of the y-axis. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2)) 14.4.1 Remove the gray box of points on legend The default ggplot always put a gray background of the scatterplot. You can remove it by adding a theme layer and specify the argument legend.key = element_blank() to get rid of them ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank()) 14.5 Modify the position of the legend By default, ggplot2 place the legend on the right position. You can decided to place either left, right, top or bottom. For example, if we want to place the legend at the bottom, we simply specifying the argument legend.position = \"bottom\" in the theme layer. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), legend.position = &quot;bottom&quot;) We can also position the legend insite the plot and specify the x and y coordinates. The coordinates range from 0 to 1, from left to right or bottom to top of the plot. For instance, we can place the legend at the top right corner of the plot by specifying legend.position = c(.9,.75) in the theme layer. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), legend.position = c(.9,.75)) 14.5.1 Change the legend background and stroke color By default, ggplot legend has a white fill for background and without color for stroke. We can customize the look of the legend in the theme layer. For example, we want our legend to have an ivory fill and black stroke of 0.25 size. This can be achieved by adding and argumentlegend.background = element_rect(fill = \"ivory\", colour = \"black\", size = .25) in the theme layer. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), legend.position = c(.9,.75), legend.background = element_rect(fill = &quot;ivory&quot;, colour = &quot;black&quot;, size = .25)) 14.5.2 Modify background colors You can remove the background fill and stroke color with the panel.background = element_blank() ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), legend.position = c(.9,.75), legend.background = element_rect(fill = &quot;ivory&quot;, colour = &quot;black&quot;, size = .25), panel.background = element_blank()) You can customize the the background fill and stroke colors to your what fit you best with the panel.background = element_rect() and specify the stroke with color and background with the fill arguments. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), legend.position = c(.9,.75), legend.background = element_rect(fill = &quot;ivory&quot;, colour = &quot;black&quot;, size = .25), panel.background = element_rect(fill = &quot;lightblue&quot;, colour = &quot;black&quot;)) 14.5.3 change the gridlines You can also customize the color and shape of the gridlines. You can remove the gridline with panel.grid = element_blank() argument in the theme layer. But if you want to customize the gridlines, then you use the panel.grid = element_line() and specify the color and linetype etc. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), legend.position = c(.9,.75), legend.background = element_rect(fill = &quot;ivory&quot;, colour = &quot;black&quot;, size = .25), panel.background = element_blank(), panel.grid = element_line(colour = &quot;grey60&quot;, linetype = &quot;dotted&quot;, size = .25)) 14.5.4 Change the font size for axis, labels and titles The fonts for axis ticks, axis-title text have a default font size of 11, if we want to increase the font size to 12 then then we can do that inside the theme(). We can also change the font size for titles, subtitles and legend text and title as the chunk below highlight. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), axis.text = element_text(size = 11), axis.title = element_text(size = 13), plot.subtitle = element_text(size = 12), plot.title = element_text( size = 20), legend.text = element_text(size = 11), legend.title = element_text(size = 12), plot.caption = element_text(size = 11)) 14.5.5 Limit axis to a range We use the coord_cartesian() function to adjust the visible area of the plot. You should specify the range of the ylim=c() and xlim = c() and expand = FALSE to show only the area of the plot you want to visualize. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), axis.text = element_text(size = 11), axis.title = element_text(size = 13), plot.subtitle = element_text(size = 12), plot.title = element_text( size = 20), legend.text = element_text(size = 11), legend.title = element_text(size = 12), plot.caption = element_text(size = 11)) + coord_cartesian(xlim = c(24.9,26.05), ylim = c(0.1,2), expand = FALSE) 14.5.6 Adding labels ggplot2 provide geom_tex() and geom_label function for adding label on the plot. geom_text() adds text directly to the plot. geom_label() draws a rectangle behind the text, making it easier to read. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + geom_text(aes(label = station), nudge_x = 0.05, nudge_y = .04)+ scale_colour_viridis_d(name = &quot;Transects&quot;) I find it difficult to position label with geom_text(), especially when you have a lot of points to label. I often use the ggrepel package, which provides text and label geoms for ggplot2 that help to avoid overlapping text labels. These function makes labels repel away from each other and away from the data points. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + ggrepel::geom_text_repel(aes(label = station))+ scale_colour_viridis_d(name = &quot;Transects&quot;) 14.5.7 Add text annotation There are times you may need to add a text on top of the plot. In ggplot2 you can place a label using the real values on the plot instead of the hard-core coorinates to specify the location based on scaled coordinates where 0 is low and 1 is high. This is useful for adding small annotations (such as text labels) or if you have your data in vectors, and for some reason don’t want to put them in a data frame. ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = temperature, y = fluorescence, col = transect)) + geom_point(size = 3) + scale_colour_viridis_d(name = &quot;Transects&quot;)+ labs(x = expression(Temperature~(degree*C)), y = expression(Fluorescence~(mgm^{-3})), title = &quot;Association of Temperature and Profile&quot;, subtitle = &quot;The plot indicat a remarkable sign of transect dependency&quot;, caption = &quot;Courtesy of IIOE-2&quot;)+ scale_x_continuous(breaks = seq(25,26,0.25))+ scale_y_continuous(breaks = seq(0.2,1.8,.2))+ theme(legend.key = element_blank(), axis.text = element_text(size = 11), axis.title = element_text(size = 13), plot.subtitle = element_text(size = 12), plot.title = element_text( size = 20), legend.text = element_text(size = 11), legend.title = element_text(size = 12), plot.caption = element_text(size = 11)) + coord_cartesian(xlim = c(24.9,26.05), ylim = c(0.1,2), expand = FALSE) + annotate(geom = &quot;text&quot;, x = 25.95 , y = 1.82, label = &quot;My label&quot;) 14.5.8 Make the x and y axis the same The coord_cartesian() is the coordinate system that you will use most of the time because most of the plot are created from two or more variables that have different scales. However, there are situations where you create plots that use the same scale of x-and y-coordinates. For example, ploting temperature profiles from two stations, then an appropriate coordinae system in that case is the coord_equal() ## tidy the temperature from long to wide format ctd.temprature.wide = ctd %&gt;% select(station, temperature, pressure) %&gt;% spread(key = &quot;station&quot;, value = &quot;temperature&quot;) ## make aplot ggplot(data = ctd.temprature.wide, aes(x = st3, y = st8)) + geom_point() + coord_equal() 14.5.9 Faceting —Creating multi–panel plots Facets are ways of arranging plots into multiple different pieces (subplots). This allows you to view a separate plot for each unique value in a categorical variable. The ggplot2 package has a nice function for creating multi-panel plots. The facet_wrap creates essentially a ribbon of plots based on a single variable. For example the plot below, I first filtered only profile below 201 meters and classify them into bins of below 50 meters, 50 to 100 and above 100 meters. This computed depth class was used to make multiple plot of boxplot of fluorescence at the four transects. in the facet_wrap() layer, I specified the ~depth.class to use this variable for faceting and nrow = 1, for the canvas to have one row with multiple columns depending on the groups, four our case we get three columns from the depth classes. ## filter the pressure and break them into class of 50,100,200 ctd.class.depth = ctd %&gt;% filter(pressure &lt; 201) %&gt;% mutate(depth.class = cut(pressure, breaks = c(0,50,100,200), labels = c(&quot;Below 50&quot;, &quot;50-100&quot;, &quot;Above 100&quot;))) ggplot(data = ctd.class.depth, aes(x = transect,y = fluorescence, fill = transect)) + geom_boxplot() + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) + scale_fill_discrete(limits = c(&quot;transect 1&quot;, &quot;transect 2&quot;, &quot;transect 3&quot;, &quot;transect 4&quot;), labels = c(&quot;Pemba&quot;, &quot;Kimbiji&quot;, &quot;Lindi&quot;, &quot;Mtwara&quot;)) + labs(x = NULL, y = expression(Fluorescence~(mgm^{-3})))+ facet_wrap(~depth.class, nrow = 1) 14.5.10 Allow scales to roam free (scales) The default for multi-panel plots in ggplot2 is to use equivalent scales in each panel. But sometimes you want to allow a panel’s own data to determine the scale. This may mislead to the audience since it give creaes plots with different scales. You can specify the scales=\"free\" in the facet_wrap() layer written as; ## filter the pressure and break them into class of 50,100,200 ctd.class.depth = ctd %&gt;% filter(pressure &lt; 201) %&gt;% mutate(depth.class = cut(pressure, breaks = c(0,50,100,200), labels = c(&quot;Below 50&quot;, &quot;50-100&quot;, &quot;Above 100&quot;))) ggplot(data = ctd.class.depth, aes(x = transect,y = fluorescence, fill = transect)) + geom_boxplot() + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) + scale_fill_discrete(limits = c(&quot;transect 1&quot;, &quot;transect 2&quot;, &quot;transect 3&quot;, &quot;transect 4&quot;), labels = c(&quot;Pemba&quot;, &quot;Kimbiji&quot;, &quot;Lindi&quot;, &quot;Mtwara&quot;)) + labs(x = NULL, y = expression(Fluorescence~(mgm^{-3})))+ facet_wrap(~depth.class, nrow = 1, scales = &quot;free_y&quot;) 14.6 Basic plots The ggplot2 package provides a set of functions that mirror the Grammar of Graphics, enabling you to efficaciously specify what you want a plot to look like. To have a glimpse of ggplot2, we start with five basic types of plots that are familiar to most people. These include: scatterplot linegraphs boxplots histograms barplots Note that the four graphs works with quantitative data and barplots are appropriate for categorical data. Thus, understanding the type of data you want to plot is a fundamental principle before you throw the variable into ggplot2 to make plot for you. 14.6.1 scatterplot Scatterplots are also called bivariate, allows you to visualize the association between two numerical variables. They are among the widely used plot because they can provide an immediate way to see the pattern of association between two numerical variables. Figure 14.2: scatterplot from base R Most of us are familiar with scatterplot shown in figure and made several of them using base R, but probably you have not made one based on the fundamental theorem of grammar of graphics. We will visualize the relationship between temperature and fluorescence. Because the ctd is profile data frame with variable as function of pressure, we want to check for the association of all the 22 station but at fixed depth of 10 meters. What this means is that we will take the ctd data and filter all variable at all station measured at water depth of 10 meters from the surface and save this in a new data frame called ctd10d. This can be written as: ctd10d = ctd %&gt;% filter(pressure == 10) Let’s now dive into the code of using the *grammar of graphics to create the scatterplot. We use the ggplot() function from ggplot2** package. The code to create figure ?? is written as; ggplot(data = ctd10d, aes(x = temperature, y = fluorescence)) + geom_point() Let’s explore the block above piece-by-piece The plotting in ggplot2 begin with ggplot() function, where the two components of grammar of graphics are required. in the data component we specify the data frame of variables measured at 10 meter water below the surface by setting data = ctd10d. Then the second argument aesthetic that map the plot with coordinate was set by aes(x = temperature, y = fluorescence)). In a nutshell, the aes() define the variable–axis specifications. For the code above we set the variables temperature into the x coordinate and the variable fluorescence into the y-axis. We then add a layer to the ggplot() function calll using the + sign. The added layer specify the third part of the *grammar—the geometric component. Becasue we want to plot scatterplot, the appropriate geom for this case is the geom_point(). Figure 14.3: Scatterplot showing the association between temperature and fluorescence at 10 meter water from the surface Once the code is run, it produce two outputs: a warning message and figure ??. The warning message notify us that out of the 22 stations, there are three stations with missing data at 10 meter water. We can check station with missing values with code written below; ctd10d %&gt;% filter(is.na(temperature)) %&gt;% select(station, pressure, temperature, fluorescence) # A tibble: 3 x 4 station pressure temperature fluorescence &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 st10 10 NA NA 2 st4 10 NA NA 3 st5 10 NA NA 14.6.2 adding regression line you can simply add the regression line by addign a geom_smooth() layer and specify the method = \"lm\" and we dont need to show the confidence errors, hence we specify se = FALSE ggplot(data = ctd10d, aes(x = temperature, y = fluorescence)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) While the geom_point() and geom_smooth() layers in this code both use the same aesthetic mappings, there’s no reason you couldn’t assign different aesthetic mappings to each geometry. Note that if the layers do share some aesthetic mappings, you can specify those as an argument to the ggplot() function Looking on the generated scatterplot (Figure ??), we notice a negative relation exist between temperature and fluorescence—stations with high temperature have relatively low concentration of fluorescence and viceversa. We also notice that 16 stations are clustered within \\(25-25.25^{\\circ}\\) C and only three are found at temperature range between \\(25.75-26^{\\circ}\\) C. We can identify these pattern by adding a col argument in aes by setting aes(x = temperature, y = fluorescence, col = Lat.label)). The code for creating figure ?? is written as; ggplot(data = ctd10d, aes(x = temperature, y = fluorescence, col = Lat.label)) + geom_point() Looking on the color of thelegend, it clearly indicate the two stations with temperatur above \\(25.75^{\\circ}\\) C are found along the transect lies along latitude 5.49°S in Pemba Channel and and one at latitde7.04°S off the Kimbiji. We may also be interested to knwo whether dissolved oxygen has influrence on the the association of temperature and fluoresce. We can achieve that by simply parsing the the shape = oxygen, arguments into the aesthetic component. Figure ?? clearly shows the stations with relatively high chlorophyll value have bigger shape (high DO value) and found in relatively less warmer waters in contrast to stations with low chl-value have smaller shape size, indicating low DO and found in relatively warmer water. The chunk for making figure ?? is written as: ggplot(data = ctd10d, aes(x = temperature, y = fluorescence, size = oxygen)) + geom_point() Figure 14.4: Scatterplot showing the association between temperature and fluorescence at 10 meter water from the surface and the influence of dissolved oxygen 14.7 Linegraph # I donwloaded the data using the code below and save it into the project directory. This chunk will never run but rather the proceeding step will use the data that was downloade and processed with code in this chunk. result = xtractomatic::searchData(&quot;datasetname:mday&quot;) ## sst, Aqua MODIS, NPP, L3SMI, Global,0.041, Science Quality (Monthly Composite) xtractomatic::getInfo(dtype = &quot;mhsstdmday&quot;) ## Chlorophyll-a, Aqua MODIS, NPP, L3SMI, Global,0.041, Science Quality (Monthly Composite) xtractomatic::getInfo(dtype = &quot;mhchlamday&quot;) sst = xtractomatic::xtracto_3D(dtype = &quot;mhsstdmday&quot;, xpos = c(38.79272,40.24292), ypos = c(-5.692232,-5.451759), tpos = c(&quot;2014-11-02&quot;, &quot;2019-04-16&quot;)) chl = xtractomatic::xtracto_3D(dtype = &quot;mhchlamday&quot;, xpos = c(38.79272, 40.24292), ypos = c(-5.692232, -5.451759), tpos = c(&quot;2003-01-17&quot;, &quot;2019-04-16&quot;)) The next basic graph of ggplot2 is the linegraph. Linegraphs is similar to drawing points, except that it conncets the points with line. often times you dont show the points. Linegraphs are commonly used to show time series data. Its inappropriate to use the linegraphs for data that has no clear sequential ordering . Let’s illustrate how to create linegraphs using another dataset ofchlorophyll concentration in the Pemba channel. This is a monthly average data from MODIS satellite. I processed the data and stored them in .RData format. We can load this dataset from the working directory with the load() function as shown in the chunk below; load(&quot;modis_pemba.RData&quot;) Exploring further the dataset, we notice that its an array with 35 longitude spacing and 7 latitude spacing and 196 matrix collected every month from 2003-01-16 to 2019-04-16. Being an array, this dataset is not in the right format, since ggplot2 only accept the data frame format. chl$data %&gt;% class();chl$data %&gt;% dim() [1] &quot;array&quot; [1] 35 7 196 Therefore, we need to tidy this dataset from the array into the data frame and find average chlorophyll concentration for each month for 196 months. Because there are 196 matrix, that is a lot of work to do it one after the other. The easiest solution is to loop the process with a for loop. If you are not familiar with how loop works in R, I suggest you check section__ of chapter__, which describe in details. The chunk below highlight the for loop used to convert an array to matrix ## preallocate object sst.tb = list() chl.tb = list() ## loop sst for (i in 1:length(sst$time)){ sst.tb[[i]] = matrix_tb(x = sst$longitude, y = sst$latitude, data = sst$data[,,i]) %&gt;% mutate(time = sst$time[i] %&gt;%as.Date()) %&gt;% select(date = time,lon = x, lat = y, sst = value) } ## loop chl for (j in 1:length(chl$time)){ chl.tb[[j]] = matrix_tb(x = chl$longitude, y = chl$latitude, data = chl$data[,,j]) %&gt;% mutate(time = chl$time[j] %&gt;%as.Date()) %&gt;% select(date = time,lon = x, lat = y, chl = value) } ## unlist the listed sst and chl files sst.tb = sst.tb %&gt;% bind_rows(sst.tb) chl.tb = chl.tb %&gt;% bind_rows(chl.tb) We then used the function in lubridate::year() and lubridate::month() to create the year and month variables from the date variables chl.decompose = chl.tb %&gt;% mutate(month = lubridate::month(date, label = TRUE, abb = TRUE), year = lubridate::year(date) %&gt;% as.integer()) To make a plot of annual average of chlorophyll concentration in the Pemba Channel, we need to monthly mean. We exclude data for year 2003 and 2019 because there only seven months in 2003 and four months in 2019, which can affect the analysis. computing annual average can be written as; chl.yearly = chl.decompose %&gt;% filter( year &gt;= 2003 &amp; year &lt;= 2018)%&gt;% group_by(year) %&gt;% summarise(chl = mean(chl, na.rm = TRUE)) Let’s create linegraph of annual average chlorophyll-a concentration in the Pemba Channel. Like the scatterplot we made earlier, where supply the data frame in data argument and specified the aesthetic mapping with x and y cooridnates, but instead of using geom_point(), we use the geom_line(). The code to make the line graphy of annual average chlorophyll-a conceration shown in figure ?? are written as; ggplot(data = chl.yearly, aes(x = year, y = chl))+ geom_line() Since the selected area is affected with monsoon season, we can further decode the months into their respective southeast (SE) and northeast (NE) and inter-monsoon (IN) seasons using the decode() function from dplyr package as the chunk below show; chl.season = chl.decompose %&gt;% mutate(season = month %&gt;% as.character(), season = recode(.x = season, Jan = &quot;NE&quot;), season = recode(.x = season, Feb = &quot;NE&quot;), season = recode(.x = season, Mar = &quot;NE&quot;), season = recode(.x = season, Apr = &quot;IN&quot;), season = recode(.x = season, May = &quot;SE&quot;), season = recode(.x = season, Jun = &quot;SE&quot;), season = recode(.x = season, Jul = &quot;SE&quot;), season = recode(.x = season, Aug = &quot;SE&quot;), season = recode(.x = season, Sep = &quot;SE&quot;), season = recode(.x = season, Oct = &quot;IN&quot;), season = recode(.x = season, Nov = &quot;NE&quot;), season = recode(.x = season, Dec = &quot;NE&quot;)) We then compute the year average of chlorophyll-a at each season as written in the chunk below chl.season = chl.season %&gt;% filter( year &gt;= 2003 &amp; year &lt; 2019)%&gt;% group_by(year,season) %&gt;% summarise(chl = mean(chl, na.rm = TRUE)) To make a linegraphy that show annual average of chlorophyll-a concetration for each season as in figure ??, we simply parse the argument col == season in the aes() component as the chunk below hightlight ggplot(data = chl.season, aes(x = year, y = chl, col = season))+ geom_line()+ scale_x_continuous(breaks = seq(2002,2019,2)) 14.7.1 histogram A histogram is a plot that can be used to examine the shape and spread of continuous data. It looks very similar to a bar graph and organized in intervals or classes. . It divides the range of the data into bin equal intervals (also called bins or classes), count the number of observations n in each bin, and display the frequency distribution of observations as a bar plot. Such histogram plots provide valuable information on the characteristics of the data, such as the central tendency, the dispersion and the general shape of the distribution. Let’s consider the chl variable in the chl.tb data frame. Since histogram works for single variable that contains quantitative values, you can not bother looking for relationship as we have seen in previous plots, but histogram offers an opportunity to answer question like What are the smallest and largest values of chl-a? What is the center value? 3 How does these values spread out? We can make a histogram shown in figure ?? by simply setting aes(x = chl) and add geom_histogram(). Within the geom_histogram(), we simply specify the number of bins bins = 28 and also the color seprating each column of the histogram with col == \"white\". The code to create figure ??, which displays a histogram with twenty eight classes, is written as; ggplot(data = chl.decompose %&gt;% filter(chl &lt; .5), aes(x = chl))+ geom_histogram(bins = 28, col = &quot;white&quot;) Figure 14.5: Histogram showing the distribution of chlorophyll concentration 14.7.2 boxplot The box plot is a standardized way of displaying the distribution of data based on the five number summary: minimum, first quartile, median, third quartile, and maximum. Box plots are useful for detecting outliers and for comparing distributions. These five number summary also called the 25th percentile, median, and 75th percentile of the quantitative data. The whisker (vertical lines) capture reoungly 99% of a distribution, and observaion outside this range are plottted as points representing outliers as shown in figure 14.6. knitr::include_graphics(&quot;./images/boxplot-01.png&quot;) Figure 14.6: Boxplot five numbers Box plots can be created by using the geom_boxplot() function and specify the data variable. ggplot(data = chl.decompose %&gt;% filter( chl &lt; .25), aes(y = chl))+ geom_boxplot() The firstinput must be a categorical variable and the second must be a continuous variable. ggplot(data = chl.season %&gt;% filter( chl &lt; .25), aes(x = season, y = chl))+ geom_boxplot() the geom_boxplot() has outlier_ arguments that allows to highlight and modify the color, shape, size, alpha … etc of outliers —extreme observation. For instance, you can highlight the outlier with; ggplot(data = chl.season %&gt;% filter( chl &lt; .25), aes(x = season, y = chl))+ geom_boxplot( outlier.colour = &quot;red&quot;, outlier.shape = 8, outlier.size = 4) We can also map the fill and color to variable in to distinguish boxplot. for example, we can specify the fill = season argument in the aes() to fill the boxplot based on season. ggplot(data = sst.season , aes(x = month, y = sst, fill = season))+ geom_boxplot( outlier.colour = &quot;black&quot;, outlier.shape = 8, outlier.size =1.2)+ scale_fill_manual(values = c(4,2,3)) We can add the points on top of the boxplot with the geom_jitter(). It also allows for specifying other arguments like colors and width of the points. ggplot(data = chl.season %&gt;% filter( chl &lt; .25), aes(x = season, y = chl))+ geom_boxplot()+ geom_jitter(width = .05, height = .05, col = &quot;blue&quot;) 14.7.3 barplot Bar graphs are perhaps the widely used plot. They are typically used to display numeric values on the y-axis for different groups on the x-axis. There is an important distiction you should be aware of when making bar graphs. The heigh of a bar in barplot may represent either the counts or values of elements in the dataset. Let’s begin with the former—count. We use the drifter observations, which passed at the confluence—where the South Equatorial Current splits to form the northward flowing called the East Africa Coastal Current and the southward flowing the Mozambique Current. We first load the daset as the code highlight; drifter_confluence = read_csv(&quot;drifter_confluence.csv&quot;) 14.7.3.1 barplot for count To make the bar graph that show the number of drifters crossed the ares per month over the entire period you you simply specify the the variable month in the x coordinates in the aesthetic and add the geom_bar() ggplot(data = drifter_confluence, aes(x = month)) + geom_bar() Sometimes you may wish to stack bars. For instance drifter have drogued—an anchor, which reduce speed of the drifter wind effect. When the drogue is lost, drifter slip and tend to overestimate the current speed. In our dataset, the drogue are coded with 0 == Absent, and 1 = Present. Because zero and ones make no sense, we create a new variables of that replace zero with LOST and ones with PRESENT. we use the if_else() function from dplyr to convert these values and assign the object as drifter.drogue. The conversion can be attained with code written as; drifter.drouge = drifter_confluence %&gt;% mutate(drogue.status = if_else(condition = drogue==1, true = &quot;PRESENT&quot;, false = &quot;LOST&quot;)) Then to stack the bar based on the drogue status, we simpy add the fill = drogue.status in aes() part ggplot(data = drifter.drouge, aes(x = month, fill = drogue.status))+ geom_bar() You can flip the order of bar with position = position_stack(reverse = TRUE) ggplot(data = drifter.drouge, aes(x = month, fill = drogue.status))+ geom_bar(position = position_stack(reverse = TRUE)) Instead of stacking, you can dodge the bar with position = position_dodge() argument ggplot(data = drifter.drouge, aes(x = month, fill = drogue.status))+ geom_bar(position = position_dodge()) To add a black stroke color of the bar, add the argument col = \"black\" inside the geom_bar() ggplot(data = drifter.drouge, aes(x = month, fill = drogue.status))+ geom_bar(position = position_dodge(), col = &quot;black&quot;) And to specify the width of the bar you specify a value in width argument in geom_bar() ggplot(data = drifter.drouge, aes(x = month, fill = drogue.status))+ geom_bar(position = position_dodge(), col = &quot;black&quot;, width = .75) Sometimes you want to map bar with different colors for negative and positive values. For this case we will use the sea surface temperature from drifter observation and create a new variable called anomaly—indicate the temperature value above (positive value) and below (negative) the climatological mean. The code for computing anomaly is; sst.anomaly = drifter_confluence %&gt;% group_by(year) %&gt;% summarise(average = mean(sst, na.rm = TRUE)) %&gt;% mutate(anomaly = average - mean(average), anomaly.status = if_else(anomaly &gt; 0 , &quot;Above&quot;, &quot;Below&quot;)) %&gt;% filter(year &gt; 1996) once we have computed the anomaly of sea surface temperature for each year, we can plot the withe geom_col() and specify x = year, and y = anomaly and fill = anomaly.status. We also need to specify the position == \"identity\" to prevent notification message of poor defined stacking for bar with negative values. ggplot(data =sst.anomaly, aes(x = year, y = anomaly, fill = anomaly.status)) + geom_col(position = &quot;identity&quot;, width = .8) Although the plot looks good but sometimes we may wish to reverse the filled color.We can reorder the color with the scale layer. Because the variable used to fill the bar is categorical, the appropriate scale is scale_fill_discrete() and you specify the limits with limits = c(\"Below\", \"Above\"). The legend is this plot is not important and we can remove from th plot with guides(fill = FALSE) ggplot(data =sst.anomaly, aes(x = year, y = anomaly, fill = anomaly.status)) + geom_col(position = &quot;identity&quot;, width = .8)+ scale_fill_discrete(limits = c(&quot;Below&quot;, &quot;Above&quot;))+ guides(fill = FALSE) The red 14.7.4 barplot for values We have seen how to make barplot that show the count with geom_bar(). You can also use the barplot to show the values with the geom_col() function and specify what variables you want on the x and y axis. For instance, we want to show how surface current varies over twelve months. Because the geom_col() requires summarized statistics, we need to compute the average current speed for each month. The chunk below highlight how to compute the mean, maximum, minimum and standard deviation of surface current velocity for each month. current.speed.monthly = drifter.drouge %&gt;% filter() %&gt;% mutate(speed = sqrt(u^2 + v^2)) %&gt;% group_by(month, drogue.status) %&gt;% summarise(speed_min = min(speed, na.rm = TRUE), speed_mean = mean(speed, na.rm = TRUE), speed_max = max(speed, na.rm = TRUE), speed_sd = sd(speed, na.rm = TRUE)) %&gt;% ungroup() Once we have computed the statistics, we can use them to make barplot. Note that unlike the geom_bar() that need only the x variable, geom_col() requires x and y variables specified. For illustration, we specified the x = month, and y = speed_mean in the aes() to make a barplot that show the monthly monthly mean surface current. ggplot(data = current.speed.monthly, aes(x = month,y = speed_mean))+ geom_col() Although ggplot2 allows to stack the barplot that present values, although they seems appealing, you must avoid making such kind of plot, because they tend to mislead and difficult to distinguish betwen the groups. ggplot(data = current.speed.monthly, aes(x = month,y = speed_max, fill = drogue.status))+ geom_col() The appropriate way if you want to compare two or more groups with geom_col(), you better doge the bar instead of stacking them. This makes easier to compare. For instance, in the case here, its cleary to see months of which drifter with lost drogues move relatively faster than those with droguue. ggplot(data = current.speed.monthly, aes(x = month,y = speed_max, fill = drogue.status))+ geom_col(position = position_dodge()) Note: the basic barplot that present count or frequency has one categorical variable on the x axis and you can make with geom_bar()And the barplot that present the values for instance the average of the variable has one categorical variable on the x axis and continuos variable on the y axis and you create them with geom_col() function. You can make a grouped bar plot by mapping that variable to fill, which represent the fill color of the bars. Like the variables mapped to the x axis, variables that are specified to the fill color of bars must be categorical instead of continuous. drifter.interest = drifter_confluence %&gt;% group_by(id) %&gt;% summarise(count = n(), begin = dplyr::first(date), end = dplyr::last(date), period = lubridate::interval(start = begin, end = end, tzone = &quot;&quot;) %&gt;% lubridate::as.duration() %&gt;% as.numeric(&quot;hours&quot;)) %&gt;% filter(period &gt;1000) drifter.interest.point = drifter.interest %&gt;% left_join(drifter_confluence, by = &quot;id&quot;) %&gt;% mutate(id.drifter = paste(&quot;ids&quot;, id, sep = &quot;&quot;)) ggplot(data = drifter.interest.point , aes(x = lon, y = lat, col = id.drifter))+geom_path()+geom_point()+ metR::scale_x_longitude(ticks = .3)+ metR::scale_y_latitude(ticks = .4) 14.8 Advanced plots ggplot2 with its add-in packages provides extensive capabilities for visualizing your data. You can produce 2D plots, 3D plots, and animations; you can view images; and you can create histograms, contour and surfaces plots, and other graphical representations of your data. its’s fun to explore pattern over time with linegraphy. Time is so embedded in our day–to–day life tha so many aspects of visualizing temporal data are fairly intuitive. You understand thing changing and evolving—the hard part is figuring out by how much is changing and look for that change in the graph. It’s easy to glance over some lines on a plot and say something is increasing, which is good as that is the core function of visualization to help you see the patterns. But, because of averaging, we sometimes miss some subtle changes that hidden when we lumped data together. chl.tb %&gt;% mutate(month = lubridate::month(date), year = lubridate::year(date)) %&gt;% group_by(year, month) %&gt;% summarise(chl = mean(chl, na.rm = TRUE))%&gt;% filter(year &lt; 2019) %$% interpolate2(x = year , y = month, z = chl, n = 16)%&gt;% rename(year = x, month = y, chl = z) %&gt;% filter(chl &lt; .4)%&gt;% ggplot(aes(x =year, y = month, z = chl))+ metR::geom_contour_fill(na.fill = TRUE, bins = 20)+ scale_fill_gradientn(colours = oce::oce.colorsJet(n = 120))+ scale_y_reverse(breaks = 1:12, label = seq(lubridate::dmy(010119), lubridate::dmy(311219), by = &quot;month&quot;) %&gt;% lubridate::month(abbr = TRUE, label = TRUE))+ scale_x_continuous(breaks = seq(2002,2017,2))+ coord_cartesian(expand = FALSE)+ labs(x = NULL, y = NULL)+ theme_bw()%+% theme(axis.text = element_text(size = 11, colour = &quot;black&quot;))+ guides(fill = guide_colorbar(raster = FALSE, barheight = 15, barwidth = 1.1, title.theme = element_text(angle = 90, size = 13), title.position = &quot;right&quot;, title.hjust = .5, label.theme = element_text(size = 10), title = expression(Chlorophyll~concetration~(mgm^{-3})))) I used oce::oce.colorsJet(120) and specify 120 color gradient to obtain color scheme similar to the Matlab Jet scheme. It widely used for maps but works great for general visualization like heatmaps. 14.8.1 Facets Sometimes you may wish to split your plot into facets—subplots that each display one subset of the data. Faceting is used when you wish make subplots from categorical variables. It creates multiple copies of the same type of plot with matching x and y axes. There is a scale(), which allows you to adjust the x and y axes according to groups in the categorical variable. ## list files from the working directory tafiri = list.files(pattern = &quot;tafiri_&quot;) ## make a vector of variables. The order must be consistency with the files order in var = c(&quot;chl&quot;, &quot;pp&quot;, &quot;sst&quot;) ## make a vector of site. The order must be consistency with the sheets in files sites = c(&quot;Pemba&quot;, &quot;Zanzibar&quot;, &quot;Mafia&quot;, &quot;EEZ&quot;) ## preallocate an empty object tafiri.data = NULL for (i in 1:length(var)){ for (j in 1:length(sites)){ data = readxl::read_excel(path = tafiri[i], sheet = j)%&gt;% rename(date = 1, year = 2, value = 3) %&gt;% mutate(month = lubridate::month(date), day = 15, site = sites[j], variable = var[i], date = lubridate::make_date(year = year, month = month, day = day)) %&gt;% arrange(date) ## stitch processed data frame from each sheet tafiri.data = tafiri.data %&gt;% bind_rows(data) } } For instance, suppose we’re interested in looking at how the hovmoller of monthly primary productivity varies over time across the four stations, We split this heatmaps by the four stations. We achieve this by simply adding facet_wrap(~site, nrow = 2) layer; ggplot()+ metR::geom_contour_fill(data = tafiri.data %&gt;% filter(variable == &quot;pp&quot;), aes(x = year, y = month, z = value), na.fill = TRUE, bins = 20)+ coord_cartesian(expand = FALSE)+ scale_y_reverse(breaks = seq(2,11,2), label = c(&quot;February&quot;, &quot;April&quot;, &quot;June&quot;, &quot;August&quot;, &quot;October&quot;))+ scale_x_continuous(breaks = seq(2004,2017,4))+ scale_fill_gradientn(colors = oce::oce.colors9A(120), breaks = seq(400,1600,200))+ theme_bw() %+replace% theme(axis.text = element_text(size = 12, colour = 1))+ guides(fill = guide_colorbar(title = expression(mgm^{-3}), title.position = &quot;top&quot;, title.hjust = 0.5, direction = &quot;vertical&quot;, reverse = FALSE, barwidth = unit(.4, &quot;cm&quot;), barheight = unit(4, &quot;cm&quot;)))+ labs(x = NULL, y = NULL)+ guides(fill = guide_colorbar(raster = FALSE, barheight = 15, barwidth = 1.1, title.theme = element_text(angle = 90, size = 13), title.position = &quot;right&quot;, title.hjust = .5, label.theme = element_text(size = 10), title = expression(Primary~Productivity~(Cm^{-3}~yr^{-1}))))+ facet_wrap(~site, nrow = 2) Note the use of the tilde ~ before the site in facet_wrap(). The tilde is required when you want specify the variable that will be used to split the plots into subplots. We can add other arguments in the facet_wrap() function. Let say we want our plot to be in one rows and four columns by simply adding the argument nrow = 1 14.8.2 Sea Surface Temperature ggplot()+ metR::geom_contour_fill(data = tafiri.data %&gt;% filter(variable == &quot;sst&quot;), aes(x = year, y = month, z = value), na.fill = TRUE, bins = 20)+ coord_cartesian(expand = FALSE)+ scale_y_reverse(breaks = seq(2,11,2), label = c(&quot;February&quot;, &quot;April&quot;, &quot;June&quot;, &quot;August&quot;, &quot;October&quot;))+ scale_x_continuous(breaks = seq(2004,2017,4))+ scale_fill_gradientn(colors = oce::oce.colors9A(120))+ theme_bw() %+replace% theme(axis.text = element_text(size = 11, colour = 1))+ guides(fill = guide_colorbar(title = expression(mgm^{-3}), title.position = &quot;top&quot;, title.hjust = 0.5, direction = &quot;vertical&quot;, reverse = FALSE, barwidth = unit(.4, &quot;cm&quot;), barheight = unit(4, &quot;cm&quot;)))+ labs(x = NULL, y = NULL)+ guides(fill = guide_colorbar(raster = FALSE, barheight = 15, barwidth = 1.1, title.theme = element_text(angle = 90, size = 13), title.position = &quot;right&quot;, title.hjust = .5, label.theme = element_text(size = 10), title = expression(Sea~Surface~Temperature~(degree*C))))+ facet_wrap(~site, nrow = 1) 14.8.3 Subplot There are occasions when it is convenient to display several plots side-by-side. In these instances, you will want to use the cowplot package. we first create ggplot2 object of Primary productivity for the channels we are interested: for Mafia Channel we assign the name mafia.pp and for Zanzibar Channel zanzibar.pp. The chunk below show how to create the these object. mafia.pp = ggplot()+ metR::geom_contour_fill(data = tafiri.data %&gt;% filter(variable == &quot;pp&quot; &amp; site == &quot;Mafia&quot;), aes(x = year, y = month, z = value), na.fill = TRUE, bins = 20)+ coord_cartesian(expand = FALSE)+ scale_y_reverse(breaks = 1:12, label = seq(lubridate::dmy(010119), lubridate::dmy(311219), by = &quot;month&quot;) %&gt;% lubridate::month(abbr = TRUE, label = TRUE))+ scale_x_continuous(breaks = seq(2004,2017,4))+ scale_fill_gradientn(colors = oce::oce.colors9A(120),limits = c(300,1700), breaks = seq(400,1600,200))+ theme_bw() %+replace% theme(axis.text = element_text(size = 12, colour = 1), legend.position = &quot;none&quot;)+ guides(fill = guide_colorbar(title = expression(mgm^{-3}), title.position = &quot;top&quot;, title.hjust = 0.5, direction = &quot;vertical&quot;, reverse = FALSE, barwidth = unit(.4, &quot;cm&quot;), barheight = unit(4, &quot;cm&quot;)))+ labs(x = NULL, y = NULL)+ guides(fill = guide_colorbar(raster = FALSE, barheight = 15, barwidth = 1.1, title.theme = element_text(angle = 90, size = 13), title.position = &quot;right&quot;, title.hjust = .5, label.theme = element_text(size = 10), title = expression(Primary~Productivity~(Cm^{-3}~yr^{-1})))) zanzibar.pp = ggplot()+ metR::geom_contour_fill(data = tafiri.data %&gt;% filter(variable == &quot;pp&quot; &amp; site == &quot;Zanzibar&quot;), aes(x = year, y = month, z = value), na.fill = TRUE, bins = 20)+ coord_cartesian(expand = FALSE)+ scale_y_reverse(breaks = 1:12, label = seq(lubridate::dmy(010119), lubridate::dmy(311219), by = &quot;month&quot;) %&gt;% lubridate::month(abbr = TRUE, label = TRUE))+ scale_x_continuous(breaks = seq(2004,2017,4))+ scale_fill_gradientn(colors = oce::oce.colors9A(120), limits = c(300,1700), breaks = seq(400,1600,200))+ theme_bw() %+replace% theme(axis.text = element_text(size = 12, colour = 1), axis.text.y = element_blank())+ guides(fill = guide_colorbar(title = expression(mgm^{-3}), title.position = &quot;top&quot;, title.hjust = 0.5, direction = &quot;vertical&quot;, reverse = FALSE, barwidth = unit(.4, &quot;cm&quot;), barheight = unit(4, &quot;cm&quot;)))+ labs(x = NULL, y = NULL)+ guides(fill = guide_colorbar(raster = FALSE, barheight = 15, barwidth = 1.1, title.theme = element_text(angle = 90, size = 13), title.position = &quot;right&quot;, title.hjust = .5, label.theme = element_text(size = 10), title = expression(Primary~Productivity~(Cm^{-3}~yr^{-1})))) Once we have the ggplot2 objects, we can now combine them side by side with the cowplot::plot_grid() function. cowplot::plot_grid(mafia.pp, zanzibar.pp, nrow = 1, rel_widths = c(.8,1), labels = c(&quot;Mafia&quot;, &quot;Zanzibar&quot;), label_x = c(.2,.001), label_y = .98, label_fontface = &quot;plain&quot;, label_size = 12) profile = ctd %&gt;% filter(station == &quot;st2&quot;) ggplot(data = profile, aes(x = temperature, y = pressure)) + geom_point() This produce a scatterplot shown in figure ??, which shows a strong correlation as the water depth increase the temperature decrease. Using the *grammar of graphics of the ggplot2**, the structure of the code used to make figure ?? is defined by; Data: profile, Aesthetic mapping: temperature values mapped to x position, pressure values mapped to y position Geometry: points In ggplot2, a plot is created with the ggplot() function, where data and aesthetic mappings are supplied as arguments, then layers are added on with +. 14.9 Colour The aesthetic component in ggplot2 allows to add additional variables to a graph. For instance, We can map the colors of the points to the temperature variable to reveal the gradient of dissolved oxygen as a function of depth ggplot(data = profile, aes(x = temperature, y = pressure, col = oxygen)) + geom_point() 14.10 size The aesthetic component in ggplot2 also allows to distinguish the plot with size of the variableh. For instance, We can map the size of the points to the temperature variable to reveal the gradient of dissolved oxygen as a function of depth ggplot(data = profile %&gt;% sample_frac(.25), aes(x = temperature, y = pressure, col = oxygen, size = oxygen)) + geom_point() Although we can map the temperature profile as points, it is appropriate to plot this profile as line. We can replace the geom_point() with geom_path() for that purpose ggplot(data = profile, aes(x = temperature, y = pressure, col = oxygen)) + geom_path() 14.11 scaling A scale controls the mapping from data to aesthetic attributes. Its recommended to manipulate the scale for every aesthetic used on a plot. For example we want the x-axis labels to be position at the top and reverse the y-axis. The code can be writeen as; ggplot(data = profile, aes(x = temperature, y = pressure, col = oxygen)) + geom_path()+ scale_y_reverse(name = &quot;Pressure [m]&quot;, breaks = seq(0,810,100))+ scale_x_continuous(position = &quot;top&quot;, name = expression(Temperature~(degree*C)), breaks = seq(6,29,3)) We might want to change the default color provided with ggplot2 on the legend. Since we commanded the plot to display gradient colors from dissolved oxygen, We use the scale_color_gradientn() function. The code can be writeen as; ggplot(data = profile, aes(x = temperature, y = pressure, col = oxygen)) + geom_path()+ scale_y_reverse(name = &quot;Pressure [m]&quot;, breaks = seq(0,810,100))+ scale_x_continuous(position = &quot;top&quot;, name = expression(Temperature~(degree*C)), breaks = seq(6,29,3))+ scale_color_gradientn(colours = rainbow(11) %&gt;% rev()) 14.12 Guides Context is provided by guides. A guide help a human reader to understand the meaning of the visual cues. For example ggplot(data = profile, aes(x = temperature, y = pressure, col = oxygen)) + geom_path()+ scale_y_reverse(name = &quot;Pressure [m]&quot;, breaks = seq(0,810,100))+ scale_x_continuous(position = &quot;top&quot;, name = expression(Temperature~(degree*C)), breaks = seq(6,29,3))+ scale_color_gradientn(colours = rainbow(11) %&gt;% rev())+ # theme(legend.position = &quot;right&quot;) + guides(color = guide_colorbar(title = &quot;Dissolved oxygen (ml/L)&quot;, title.position = &quot;right&quot;, title.theme = element_text(angle = 90), barheight = 15, barwidth = .95, title.hjust = .5, ticks.colour = &quot;black&quot;)) ggplot(data = ctd %&gt;% filter(pressure == 10), aes(x = lon, y = lat))+ geom_point()+ ggrepel::geom_text_repel(aes(label = station)) 14.13 Add-on packages The R community has developed packages that extend the capability of ggplot2. Some of the packages include: metR: Provide addition tools for plotting filled contour, and label contour lines ggrepel: Contains tools for automatically position non-overlapping text labels ggspatial: Spatial Data Framework for ggplot2 RcolorBrewer: Contains color palettes for continuous and discrete plots cowplot: Contains addition themes and tools to combine ggplot2 plots in one panel egg: Provide tools for plot aligning and symmetrised ggplot2 plots oce: Provide color pallete for visualization of Oceanographic Data ggsn: Provide tools for mapping North symbols and scale bars on maps created with ggplot2 gganimate: convert static ggplot2 plots to animations ggformula: adds some additional plot options to ggplot2 sf : Add capabilities of ggplot2 to map spatial data such as simple features ggthemes: contains extra themes, scales, and geoms, and functions for and related to ggplot2 ggridges: extend the geom_density function by plotiing closed polygons insted of ridgelines 14.14 ggridges Althoug ehe ggridges package provides geom_ridgeline and geom_density_ridges, we focus on the latter because it has ability to estimates data densities and then draws those using ridgelines.The geom geom_density_ridges calculates density estimates from the provided data and then plots those, using the ridgeline visualization. ctd = ctd %&gt;% mutate(strata = cut(x = pressure, breaks = c(0,80,120,200), labels = c(&quot;Upper&quot;, &quot;Middle&quot;, &quot;Lower&quot;))) ctd.strata = ctd %&gt;% group_by(strata, pressure, lon,lat) %&gt;% summarise(temperature = mean(temperature, na.rm = TRUE), salinity = mean(salinity, na.rm = TRUE), oxygen = mean(oxygen, na.rm = TRUE), fluorescence = mean(fluorescence, na.rm = TRUE)) %&gt;% ungroup() %&gt;% filter(!is.na(strata)) ggplot(data = ctd.strata, aes(x = temperature, y = strata))+ ggridges::geom_density_ridges2()+ scale_y_discrete(limits = c(&quot;Lower&quot;, &quot;Middle&quot;, &quot;Upper&quot;)) Trailing tails can be cut off using the rel_min_height() aesthetic. This aesthetic sets a percent cutoff relative to the highest point of any of the density curves. A value of 0.01 usually works well, but you may have to modify this parameter for different datasets. ggplot(data = ctd.strata, aes(x = temperature, y = strata))+ ggridges::geom_density_ridges2(rel_min_height = 0.01)+ scale_y_discrete(limits = c(&quot;Lower&quot;, &quot;Middle&quot;, &quot;Upper&quot;)) ggplot(data = ctd.strata, aes(x = temperature, y = strata))+ ggridges::geom_density_ridges2(scale = 8,rel_min_height = 0.01)+# scale =80, substantial overlap scale_y_discrete(limits = c(&quot;Lower&quot;, &quot;Middle&quot;, &quot;Upper&quot;)) 14.15 Varying fill colors along the x axis Sometimes we would like to have the area under a ridgeline not filled with a single solid color but rather with colors that vary in some form along the x axis. This effect can be achieved with the geoms geom_ridgeline_gradient and geom_density_ridges_gradient. Both geoms work just like geom_ridgeline and geom_density_ridges, except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both. Here is a simple example of changing fill colors with geom_ridgeline_gradient: ggplot(data = ctd.strata, aes(x = temperature, y = strata, fill = strata))+ ggridges::geom_density_ridges_gradient(scale = 5,rel_min_height = 0.01)+# scale =5, substantial overlap scale_y_discrete(limits = c(&quot;Lower&quot;, &quot;Middle&quot;, &quot;Upper&quot;)) ggplot(data = ctd.strata, aes(x = temperature, y = strata, fill = ..x..))+ ggridges::geom_density_ridges_gradient(scale = 5,rel_min_height = 0.01)+# scale =5, substantial overlap scale_y_discrete(limits = c(&quot;Lower&quot;, &quot;Middle&quot;, &quot;Upper&quot;))+ scale_fill_gradientn(colours = oce::oce.colorsTemperature(120), breaks = seq(14,26,2))+ guides(fill = guide_colorbar(title =expression(Temperature~~(degree*C)), title.position = &quot;right&quot;, title.hjust = .5, raster = TRUE, title.theme = element_text(angle = 90, size = 12), label.theme = element_text(size = 11), barheight = 15, barwidth = .95)) 14.16 metR transect2 = ctd %&gt;% filter( lat &gt;= -8 &amp;lat &lt; -6 &amp; pressure &lt; 205) ggplot(data = transect2, aes(x = lon, y = pressure, z = fluorescence))+ metR::geom_contour_fill(na.fill = TRUE, bins = 20)+ # metR::geom_contour2()+ scale_y_reverse(breaks = seq(0, 205,30))+ scale_x_continuous(breaks = transect2 %&gt;% distinct(lon) %&gt;% pull(), labels = metR::LonLabel(transect2 %&gt;% distinct(lon) %&gt;% pull()%&gt;%round(digits = 2)))+ scale_fill_gradientn(colours = oce::oceColors9A(120), breaks = seq(0,2,.2))+ coord_cartesian(expand = FALSE)+ guides(fill = guide_colorbar(title =expression(Chlorophyll~concentration~(mgm^{-3})), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 12), label.theme = element_text(size = 11), barheight = 15, barwidth = .95))+ labs(x = NULL, y = &quot;Water depth [m]&quot;)+ geom_vline(xintercept = transect2 %&gt;% distinct(lon) %&gt;% pull(), linetype = &quot;dashed&quot;, col = &quot;ivory&quot;)+ theme_bw()+ theme(axis.text = element_text(size = 11), axis.title = element_text(size = 12)) depth = c(10,50,100,200) algoa = NULL for (i in seq_along(depth)){ strata = ctd %&gt;% filter(lon &gt; 39.3 &amp; lon &lt; 40.5 &amp; lat &gt;= -10 &amp; pressure == depth[i]) %&gt;% select(3:9) Lon = strata %&gt;% pull(lon) Lat = strata %&gt;% pull(lat) data = strata %&gt;% select(4:7) for (j in seq_along(data)){ algoa.interp = strata %$% oce::interpBarnes(x = Lon, y = Lat, z = data[j]%&gt;% pull()) algoa.tb = algoa.interp %$% matrix_tb(x = xg, y = yg, data = zg) %&gt;% rename(lon = x, lat = y) %&gt;% mutate(variable = colnames(data[j]), pressure = depth[i]) algoa = algoa %&gt;% bind_rows(algoa.tb) } } # algoa %&gt;% group_by(pressure, variable) %&gt;% summarise(average = mean(value, na.rm = TRUE)) ggplot()+ metR::geom_contour_fill(data = algoa %&gt;% filter(variable == &quot;fluorescence&quot;), aes(x = lon, y = lat, z = value), na.fill = TRUE, bins = 20)+ scale_fill_gradientn(colours = oce::oceColors9A(120))+ scale_y_continuous(breaks = seq(-8.5,-6,2.5), labels = metR::LatLabel(seq(-8.5,-6,2.5)))+ scale_x_continuous(breaks = seq(39.55,40.25,length.out = 3) %&gt;% round(2), labels = metR::LonLabel(seq(39.55,40.25,length.out = 3)))+ facet_wrap(~pressure)+ labs(x = NULL, y = NULL)+ theme_bw()+ theme(axis.text = element_text(size = 11))+ coord_cartesian(expand = FALSE)+ guides(fill = guide_colorbar(title =expression(Chlorophyll~concentration~(mgm^{-3})), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 12), label.theme = element_text(size = 11), barheight = 15, barwidth = .95)) ggplot()+ metR::geom_contour_fill(data = algoa %&gt;% filter(variable == &quot;temperature&quot;), aes(x = lon, y = lat, z = value), na.fill = TRUE, bins = 20)+ scale_fill_gradientn(colours = oce::oceColors9A(120))+ scale_y_continuous(breaks = seq(-8.5,-6,2.5), labels = metR::LatLabel(seq(-8.5,-6,2.5)))+ scale_x_continuous(breaks = seq(39.55,40.25,length.out = 3) %&gt;% round(2), labels = metR::LonLabel(seq(39.55,40.25,length.out = 3)))+ facet_wrap(~pressure)+ labs(x = NULL, y = NULL)+ theme_bw()+ theme(axis.text = element_text(size = 11))+ coord_cartesian(expand = FALSE)+ guides(fill = guide_colorbar(title =expression(Temperature~(degree*C)), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 12), label.theme = element_text(size = 11), barheight = 15, barwidth = .95)) ggplot()+ metR::geom_contour_fill(data = algoa %&gt;% filter(variable == &quot;oxygen&quot;), aes(x = lon, y = lat, z = value), na.fill = TRUE, bins = 20)+ scale_fill_gradientn(colours = oce::oceColors9A(120))+ scale_y_continuous(breaks = seq(-8.5,-6,2.5), labels = metR::LatLabel(seq(-8.5,-6,2.5)))+ scale_x_continuous(breaks = seq(39.55,40.25,length.out = 3) %&gt;% round(2), labels = metR::LonLabel(seq(39.55,40.25,length.out = 3)))+ facet_wrap(~pressure)+ labs(x = NULL, y = NULL)+ theme_bw()+ theme(axis.text = element_text(size = 11))+ coord_cartesian(expand = FALSE)+ guides(fill = guide_colorbar(title =expression(Dissolved~oxygen~(mgm^{-3})), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 12), label.theme = element_text(size = 11), barheight = 15, barwidth = .95)) ggplot()+ metR::geom_contour_fill(data = algoa %&gt;% filter(variable == &quot;salinity&quot;), aes(x = lon, y = lat, z = value), na.fill = TRUE, bins = 20)+ scale_fill_gradientn(colours = oce::oceColors9A(120))+ scale_y_continuous(breaks = seq(-8.5,-6,2.5), labels = metR::LatLabel(seq(-8.5,-6,2.5)))+ scale_x_continuous(breaks = seq(39.55,40.25,length.out = 3) %&gt;% round(2), labels = metR::LonLabel(seq(39.55,40.25,length.out = 3)))+ facet_wrap(~pressure)+ labs(x = NULL, y = NULL)+ theme_bw()+ theme(axis.text = element_text(size = 11, colour = &quot;black&quot;))+ coord_cartesian(expand = FALSE)+ guides(fill = guide_colorbar(title =expression(Salinity), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 12), label.theme = element_text(size = 11), barheight = 15, barwidth = .95)) References "],
["a-grammar-for-graphics.html", "Chapter 15 A grammar for graphics 15.1 geoms 15.2 aes() 15.3 Axis and labels 15.4 Text annotaion 15.5 Scales 15.6 Guides 15.7 Themes", " Chapter 15 A grammar for graphics 15.1 geoms In this section, we will create some of the most routinely used plots to explore data using the geom_ functions. We will use the following libraries in this post: readr ggplot2 tibble dplyr Which are part of the tidyverse. By loading the tidyverse, we also load all the packages mentioned above require(tidyverse) All the data sets used in this post can be found here and code can be downloaded from here. octopus = read_csv(&quot;./data/octopus_data.csv&quot;) The variables representing the X and Y axis can be specified either in ggplot() or in geom_point(). We will learn to modify the appearance of the points in a different post. ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point() 15.1.1 Regression Line You can fit the regression on the scatterplot with geom_smooth() lm = ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;lm&quot;, se = TRUE)+ labs(x = NULL, y = NULL, title = &quot;linear model&quot;) gam = ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;gam&quot;, se = TRUE)+ labs(x = NULL, y = NULL, title = &quot;GAM&quot;) loess = ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;loess&quot;, se = FALSE)+ labs(x = NULL, y = NULL, title = &quot;LOESS&quot;) egg::ggarrange(lm,gam, loess, nrow = 1) 15.1.2 Horizontal/ vertical lines A segment of horizontal or vertical line can be added on the plot using egg::ggarrange 15.1.2.1 Vertical Line For the vertical line, the x axis intercept must be specified in geom_vline() ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_vline(xintercept = 100, linetype = 1, size = .5, col = &quot;red&quot;) 15.1.2.2 Vertical Line In similar manner, for the horizontal line, the y axis intercept must be specified in geom_hline() ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_hline(yintercept = 2, linetype = 1, size = .5, col = &quot;red&quot;) 15.2 aes() In this section, we focus on the aesthetics i.e. color, shape, size, alpha, line type, line width etc. We can map these to variables or specify values for them. If we want to map the above to variables, we have to specify them within the aes() function. We will look at both methods in the following sections. Explore aesthetics such as color shape size fill alpha width 15.2.1 Color In ggplot2, when we mention color or colour, it usually refers to the color of the geoms. The fill argument is used to specify the color of the shapes in certain cases. In this section, we will see how we can specify the color for the different geoms we learnt in the previous post. For points, the color argument specifies the color of the point for certain shapes and border for others. The fill argument is used to specify the background for some shapes and will not work with other shapes. Let us look at an example: ggplot(data = octopus, aes(x = tl, y = weight, col = sex)) + geom_point() If you do not want to map a variable to color, you can specify it separately using the color argument but in this case it should be outside the aes() function. ggplot(data = octopus, aes(x = tl, y = weight, col = sex)) + geom_point(col = &quot;blue&quot;) 15.2.2 shape ggplot(data = octopus, aes(x = tl, y = weight, shape = sex)) + geom_point() Let us map size of points to a variable. It is advised to map size only to continuous variables and not categorical variables. ggplot(data = octopus, aes(x = tl, y = weight, col = sex, size = dml)) + geom_point() 15.3 Axis and labels In this section, we learn about about aesthetic and focus on add title and subtitle to the plot modify axis labels modify axis range remove axis format axis Let us start with a simple scatter plot. We will continue to use the octopus data set and examine the relationship between total length and body weight using geom_point(). oct = ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;gam&quot;, se = TRUE) We add the axis labels, title and subtitle for the plot using the labs() ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;gam&quot;, se = TRUE) + labs(x = &quot;Total length (cm)&quot;, y = &quot;Weight (g)&quot;, title = &quot;Octopus&quot;, subtitle = &quot;The total length and body weight of octopus&quot;) ## Axis Range Often times, you may want to modify the range of the axis value. In ggplot2, we can achieve this using scale_function ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;gam&quot;, se = TRUE) + labs(x = &quot;Total length (cm)&quot;, y = &quot;Weight (g)&quot;, title = &quot;Octopus&quot;, subtitle = &quot;The total length and body weight of octopus&quot;)+ scale_x_continuous(breaks = seq(30,180,30))+ scale_y_continuous(breaks = seq(0,5,1)) Sometimes the axis label become a reduntat ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;gam&quot;, se = TRUE) + theme(axis.title = element_blank()) 15.4 Text annotaion Annotation help to add custom text to the plot. ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;gam&quot;, se = TRUE) + annotate(geom = &quot;text&quot;, x = 20, y = 2.2, label = &quot;outlier&quot;, color = &quot;red&quot;) 15.5 Scales Whenever you specify an aesthetic mapping, ggplot2 uses a particular scale to determine the range of values that the data encoding should be mapped to. However, there times you need to customize the scale. ggplot2 has scales_*() function that allows to modify titles, labels, limits, breaks and position of the axis. Each scale can be represented by a function named in the following format: scale_, followed by the name of the aesthetic property (e.g., x or color), followed by an _ and the type of the scale (e.g., continuous or discrete). A continuous scale will handle values such as numeric data (where there is a continuous set of numbers), whereas a discrete scale will handle values such as colors (since there is a small discrete list of distinct colors). In simple language, the x and y-axis of a continuous data is modified with the scale_x_continuous() and scale_y_continuous() functions. ggplot(data = octopus, aes(x = tl, y = weight)) + geom_point()+ geom_smooth(method = &quot;gam&quot;, se = TRUE)+ scale_x_continuous(limits = c(50, 150), breaks = seq(50,150,20))+ scale_y_continuous(limits = c(0,3), breaks = seq(.5, 3, .5)) The x and y-axis of a continuous data is modified with the scale_x_discrete() and scale_y_continuous() functions. ggplot(data = mafia.chl.season , aes(x = season, y = chl))+ geom_boxplot( outlier.colour = &quot;red&quot;, outlier.shape = 8, outlier.size = 4)+ scale_x_discrete(limits = c(&quot;NE&quot;, &quot;IN&quot;, &quot;SE&quot;))+ scale_y_continuous(breaks = seq(0.5,2.5,.4)) when the data has been transformed, for instance because of the low value, chlorophyll-a are often stretched with the log-tranformation for visual appeal. But the log-transformed values make no sense about concentration and hence the real values must replace them. We can change the tick labels using the labels argument. When adding labels, tick breaks and labels must have the same length. ggplot(data = mafia.chl.season , aes(x = season, y = chl %&gt;% log()))+ geom_boxplot( outlier.colour = &quot;red&quot;, outlier.shape = 8, outlier.size = 4)+ scale_x_discrete(limits = c(&quot;NE&quot;, &quot;IN&quot;, &quot;SE&quot;), labels = c(&quot;Inter&quot;, &quot;Northeast&quot;, &quot;Southeast&quot;))+ scale_y_continuous(breaks = seq(-0.5,1,length.out = 5), labels = seq(0.5,2.6,length.out = 5)) The position of the axes can be changed using the position argument. For instance, to move the the x-axis to the top of the plot you only need to specify position = top as written in code below; ggplot(data = mafia.chl.season , aes(x = season, y = chl %&gt;% log()))+ geom_boxplot( outlier.colour = &quot;red&quot;, outlier.shape = 8, outlier.size = 4)+ scale_x_discrete(position = &quot;top&quot;, limits = c(&quot;NE&quot;, &quot;IN&quot;, &quot;SE&quot;), labels = c(&quot;Inter&quot;, &quot;Northeast&quot;, &quot;Southeast&quot;))+ scale_y_continuous(breaks = seq(-0.5,1,length.out = 5), labels = seq(0.5,2.6,length.out = 5)) Fill the boxplot with season to specify the colors and arrange the colors manual with scale_fill_manual() function as written below. ggplot(data = mafia.chl.season , aes(x = season, y = chl %&gt;% log(), fill = season))+ geom_boxplot( outlier.colour = &quot;red&quot;, outlier.shape = 8, outlier.size = 4)+ scale_x_discrete(position = &quot;top&quot;, limits = c(&quot;NE&quot;, &quot;IN&quot;, &quot;SE&quot;), labels = c(&quot;Inter&quot;, &quot;Northeast&quot;, &quot;Southeast&quot;))+ scale_y_continuous(breaks = seq(-0.5,1,length.out = 5), labels = seq(0.5,2.6,length.out = 5)) Note the order of layers matter here: you scale_fill_manual() function must start before scale_x_discrete() function. Otherwise the colours you specify mismatch with legend colors as shown ggplot(data = mafia.chl.season , aes(x = season, y = chl %&gt;% log(), fill = season))+ geom_boxplot( outlier.colour = &quot;red&quot;, outlier.shape = 8, outlier.size = 4)+ scale_x_discrete(position = &quot;top&quot;, limits = c(&quot;NE&quot;, &quot;IN&quot;, &quot;SE&quot;), labels = c(&quot;Inter&quot;, &quot;Northeast&quot;, &quot;Southeast&quot;))+ scale_fill_manual(values = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;))+ scale_y_continuous(breaks = seq(-0.5,1,length.out = 5), labels = seq(0.5,2.6,length.out = 5)) The scale_*_reverse() allows to reverse the order of the axis. For instance, when plotting profiles, we reverse y-xis and position the label of x-axis at the top ggplot(data = algoa.average, aes(x = value, y = pressure))+ geom_path() + scale_y_reverse(limits = c(800,0))+ scale_x_continuous(position = &quot;top&quot;) + facet_wrap(~variable, scales = &quot;free_x&quot;, nrow = 1) 15.6 Guides guides() helps to set, modify and remove legend for a specific aesthetic. It has two functions—guide_legend() or guide_colorbar(). Let make a section plot of the fluorescence with the default options for the legend section = ggplot(data = algoa %&gt;% filter(lat &lt; -10), aes(x = lon, y = pressure, z = fluorescence)) + metR::geom_contour_fill(na.fill = TRUE) + metR::scale_x_longitude(ticks = .15) + scale_y_reverse( limits = c(200,0))+ scale_fill_gradientn(colours = oce::oce.colors9A(120), breaks = seq(0.15,1.25, length.out = 10) %&gt;%round(2))+ labs(subtitle = paste(&quot;Section of oxygen along latitude&quot;, metR::LatLabel(-10.54))) section We can add the contour labels and remove the legend in a graph section + metR::geom_contour2() + metR::geom_text_contour() + guides(fill = FALSE) The guide_colorbar() modify the look and appearance of the legend to smooth colorbar. for instane the code of lines below highlight the key arguments that one has to specify to modify the legend of colorbar. section + guides(fill = guide_colorbar(title = expression(Chlorophyll~concentration~(mgm^{-3})), title.position = &quot;right&quot;, title.theme = element_text(angle = 90, size = 13), title.hjust = .5, label.theme = element_text(angle = 0, size = 11), label.position = &quot;right&quot;, raster = FALSE, nbin = 12, reverse = FALSE, barwidth = 1.1, barheight = 15)) If you want the legend to appear as individual key, use guide_colorbar() as written in the code below section + theme(legend.position = &quot;bottom&quot;) + guides(fill = guide_legend(title = expression(Chlorophyll~concentration~(mgm^{-3})), title.position = &quot;top&quot;, title.theme = element_text(angle = 0, size = 13), title.hjust = .5,nrow = 1, reverse = FALSE, keywidth = 3., keyheight = .8, direction = &quot;horizontal&quot;, label.theme = element_text(angle = 0, size = 11), label.position = &quot;bottom&quot;)) 15.7 Themes Themes in ggplot modify the appearance of all non data compoments in the plot like: axis, legend, panel, plot area, background, margin, facets etc. Let’s create a profile plot of temperature with default theme settings profile = ggplot(data = algoa %&gt;% filter(lat &lt; -10), aes(x = temperature, y = pressure, color = station))+ geom_path() + scale_y_reverse(limits = c(800,0))+ scale_x_continuous(position = &quot;top&quot;) + labs(y = &quot;Pressure [m]&quot;,x = expression(Temperature~(degree*C))) profile We can modify the size and color of axis label with the axis.text() and axis title with axis.title() functions. You can use axis.title.y to modify the Y axis title and to modify the title of both the axis together, use axis.title. profile + theme(axis.text = element_text(size = 11, colour = &quot;black&quot;), axis.title = element_text(size = 14, colour = &quot;black&quot;)) To modify the appearance of the axis ticks, use the axis.ticks_* argument. You can change the color, size, linetype and length of the ticks using the element_line() function as shown below. profile + theme(axis.text = element_text(size = 11, colour = &quot;black&quot;), axis.title = element_text(size = 14, colour = &quot;black&quot;), axis.ticks.length = unit(.3, &quot;cm&quot;)) The panel_grid argument is used to modify the appearance of the gridlines. You can change the color, size and linetype of the line using the element_line() function. profile + theme(axis.text = element_text(size = 11, colour = &quot;black&quot;), axis.title = element_text(size = 14, colour = &quot;black&quot;), axis.ticks.length = unit(.3, &quot;cm&quot;), panel.grid = element_line(colour = &quot;grey60&quot;, linetype = 3)) The background of the legend can be modified using the legend.background argument. You can change the background color, the border color and line type using element_rect(). profile + theme(axis.text = element_text(size = 11, colour = &quot;black&quot;), axis.title = element_text(size = 14, colour = &quot;black&quot;), axis.ticks.length = unit(.3, &quot;cm&quot;), panel.grid = element_line(colour = &quot;grey60&quot;, linetype = 3), panel.background = element_rect(fill = &quot;white&quot;, colour = &quot;black&quot;)) Now, let us look at modifying the non-data components of a legend. profile + theme(axis.text = element_text(size = 11, colour = &quot;black&quot;), axis.title = element_text(size = 14, colour = &quot;black&quot;), axis.ticks.length = unit(.3, &quot;cm&quot;), panel.grid = element_line(colour = &quot;grey60&quot;, linetype = 3), panel.background = element_rect(fill = &quot;white&quot;, colour = &quot;black&quot;), legend.key = element_blank(), legend.position = c(.9,.3), legend.background = element_rect(colour = &quot;black&quot;, fill = &quot;white&quot;)) The appearance of the text can be modified using the legend.text argument. You can change the color, size and font using the element_text() function. The position and direction of the legend can be changed using legend.position() function. profile + scale_color_discrete(name = &quot;Stations&quot;)+ theme(axis.text = element_text(size = 11, colour = &quot;black&quot;), axis.title = element_text(size = 14, colour = &quot;black&quot;), axis.ticks.length = unit(.3, &quot;cm&quot;), panel.grid = element_line(colour = &quot;grey60&quot;, linetype = 3), panel.background = element_rect(fill = &quot;white&quot;, colour = &quot;black&quot;), legend.key = element_blank(), legend.position = c(.9,.3), legend.background = element_rect(colour = &quot;black&quot;, fill = &quot;white&quot;), legend.text = element_text(size = 11, colour = &quot;black&quot;), legend.title = element_text(size = 13, colour = &quot;black&quot;)) "],
["graphics.html", "Chapter 16 Making Maps 16.1 Geographical data in a tidy format 16.2 Introduction 16.3 Static Maps 16.4 Basemap 16.5 Creating contour map 16.6 Inset maps 16.7 Animated maps 16.8 Interactive maps", " Chapter 16 Making Maps Along with all the geographical data makingits way into the public domain, a variety of tools to map that data have also been devoped. For a long time R base provides tools to map geographical data—data with latitude and longitude coordinates attached to it. However, mapping in the early days of R was not easy, it was not elegant either. Recently, some packages have been deveoped for mapping geographical data that align with the ggplot framework. So they allow us to map spatial data in similar ways as other plots, because mapping also is based on the grammar of graphic principle. For example Edzer Pebesma (2018) developed an awesome sf package for mapping vector data in R. The power of this function lies in the fact that it structure the spatial data in tidy format, allowing for easy manipulation with the tidyverse function and also for plotting with the ggplot2 flavor. The sf package allows you to read in and work with geogrphica data in a tidy format. 16.1 Geographical data in a tidy format The sf package allows you to create a simple feature—a data frame with additional columns that hold spatial component called the geometry. This column contains the geometrical nature of the data needed to draw the data. Often the sf object has two classes—the simple feature and the data.frame classes. The data.frame holds attribute information of the dataset and the geometry contains the geographical coordinates. For example, the simple feature displayed below is a dataset of sampling stations, where each row gives the data for each station. The data.frame here holds the first four columns— the id, type, depth and sst, whereas the geometry column include the geometry type, for this case the point and the embedded latitude and longitude geographical coordinates. There different ways to create simple feature in R using the sf package. We will create a few of them later that we will use for mapping examples. FALSE Reading layer `simple_feature&#39; from data source `E:\\bookdown\\spatil_r\\data\\simple_feature.shp&#39; using driver `ESRI Shapefile&#39; FALSE Simple feature collection with 11 features and 4 fields FALSE geometry type: POINT FALSE dimension: XY FALSE bbox: xmin: 39.50958 ymin: -8.425115 xmax: 42.00623 ymax: -6.414011 FALSE epsg (SRID): 4326 FALSE proj4string: +proj=longlat +datum=WGS84 +no_defs FALSE Simple feature collection with 11 features and 4 fields FALSE geometry type: POINT FALSE dimension: XY FALSE bbox: xmin: 39.50958 ymin: -8.425115 xmax: 42.00623 ymax: -6.414011 FALSE epsg (SRID): 4326 FALSE proj4string: +proj=longlat +datum=WGS84 +no_defs FALSE First 10 features: FALSE id type depth sst geometry FALSE 1 294 marker 29 27.87999 POINT (39.50958 -6.438159) FALSE 2 300 marker -604 27.97999 POINT (39.6318 -6.621774) FALSE 3 306 marker -569 27.97999 POINT (39.65447 -6.746649) FALSE 4 312 marker -485 28.03999 POINT (39.62563 -6.805321) FALSE 5 318 marker -325 28.03999 POINT (39.58374 -6.833973) FALSE 6 326 marker -461 28.03999 POINT (39.66476 -6.837384) FALSE 7 414 marker -505 28.02999 POINT (39.95728 -7.843535) FALSE 8 428 marker -132 28.23999 POINT (39.67712 -8.136846) FALSE 9 434 marker -976 28.16999 POINT (39.74853 -8.425115) FALSE 10 456 marker -3311 28.33999 POINT (42.00623 -7.025368) 16.1.1 Create simple feature from data.frame If you have a regular dataframe, you can convert it into a simple feature object with tools in the sf package. For instance, in the working directory we have a dataset of eleven stations named points.csv. We can simply import this dataset into R session with the read_csv() function. If we print the file, it give about the variables and rows presented in the datasete. There six variables—id, type, depth and sst along with the latitude and longitude coordinates. These stations contains measured variable of sea surface temperature and their maximum depth. stations = read_csv(&quot;./data/points.csv&quot;) stations # A tibble: 11 x 6 lon lat id type depth sst &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 39.5 -6.44 294 marker 29 27.9 2 39.6 -6.62 300 marker -604 28.0 3 39.7 -6.75 306 marker -569 28.0 4 39.6 -6.81 312 marker -485 28.0 5 39.6 -6.83 318 marker -325 28.0 6 39.7 -6.84 326 marker -461 28.0 7 40.0 -7.84 414 marker -505 28.0 8 39.7 -8.14 428 marker -132 28.2 9 39.7 -8.43 434 marker -976 28.2 10 42.0 -7.03 456 marker -3311 28.3 11 41.8 -6.41 462 marker -3248 28.6 Athough this dataset contains geographical coordinates in it (latitude and longitude), it’s just a regular data frame. We can use the geographical coordinates in the dataframe to convert it to simple feature with the st_as_sf() function, and specify the columns with the geographical information using the coords parameter. simple_feature = stations %&gt;% sf::st_as_sf(coords = c(&quot;lon&quot;, &quot;lat&quot;)) Once we have the simple feature object, we can set the geographica coordinate system to World Geodetic System of 1984 (WGS84). I prefer using its code, which is easy to punch in instead of the whole text. If we print out the simple feature we just created, it gives the extra information at the top of the print-out, which include the number of features and columls(fields), the geometry type as point, the geographical extent (the bounding box) of and the projection both in epsg code and string. simple_feature = simple_feature %&gt;% sf::st_set_crs(4326) ggplot()+ geom_sf(data = simple_feature, aes(col = sst, size = inverse_hyperbolic((sst))))+ scale_colour_gradientn(colors = oce::oce.colors9A(120)) 16.1.2 Importing shapefile When you create maps, you will often want to import shapefile—a widely used format for storing geographical data in GIS. sf package offers tools to read and load shapefiles into R. Let’s import africa’s country boundary shapefile from the working directory. We use the st_read() function from sf package to read the shapefile boundary layer. Like the simple features we created, the shapefile also display extra information at the top confirming that it’s no longer a shapefile but rather a simple feature. ## read africa = sf::st_read(&quot;./data/africa.shp&quot;) Reading layer `africa&#39; from data source `E:\\bookdown\\spatil_r\\data\\africa.shp&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 59 features and 7 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -25.35875 ymin: -34.83983 xmax: 57.80085 ymax: 37.34962 epsg (SRID): 4326 proj4string: +proj=longlat +datum=WGS84 +no_defs africa Simple feature collection with 59 features and 7 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -25.35875 ymin: -34.83983 xmax: 57.80085 ymax: 37.34962 epsg (SRID): 4326 proj4string: +proj=longlat +datum=WGS84 +no_defs First 10 features: COUNT CNTRY_NAME FIPS_CNTRY LAND_AREA_ REGIONA 1 34 Angola AO 124670 &lt;NA&gt; 2 114 Burundi BY 2783 &lt;NA&gt; 3 77 Benin BN 11262 &lt;NA&gt; 4 301 Burkina Faso UV 27400 &lt;NA&gt; 5 25 Botswana BC 58173 &lt;NA&gt; 6 51 Central African Republic CT 62298 &lt;NA&gt; 7 51 Cameroon CM 47544 &lt;NA&gt; 8 186 Ivory Coast IV 32246 &lt;NA&gt; 9 46 Congo CF 34200 &lt;NA&gt; 10 15 Cape Verde CV 403 &lt;NA&gt; EMPTY EMPTY2 geometry 1 0 0 MULTIPOLYGON (((12.84118 -6... 2 0 0 MULTIPOLYGON (((29.05021 -2... 3 0 0 MULTIPOLYGON (((3.849006 10... 4 0 0 MULTIPOLYGON (((-5.272945 1... 5 0 0 MULTIPOLYGON (((23.14635 -1... 6 0 0 MULTIPOLYGON (((22.03557 4.... 7 0 0 MULTIPOLYGON (((9.640797 3.... 8 0 0 MULTIPOLYGON (((-6.091862 4... 9 0 0 MULTIPOLYGON (((16.45276 2.... 10 0 0 MULTIPOLYGON (((-24.64849 1... Since the layer is for the whole Africa, to reduce the processing time, we must reduce the geographical extent to the area of interest. We use the st_crop() function to chop the area that we want to map and discard the rest. kimbiji = africa %&gt;% sf::st_crop(xmin = 38.0, xmax = 40.5, ymin = -8, ymax = -5.5) 16.1.3 Reading other format Sometimes you have geographical data that are neither in tabular form or shapefile. In that situation, you ought to use the st_layers() function to identify, first the driver used to create the file and, second, the layer name you want to extract. Once you have identified the layer of interest, you can use the st_read() function to import the layer from the file. For example, we have the track file that was recorded with a GPS device. Let’s explore the layer it contains with the `st_layers() function. tracks = sf::st_layers(&quot;./tracks/Track-181204-075451.gpx&quot;) tracks Driver: GPX Available layers: layer_name geometry_type features fields 1 waypoints Point 1 19 2 routes Line String 0 12 3 tracks Multi Line String 1 12 4 route_points Point 0 21 5 track_points Point 1467 24 Once we print, it shows that i’s a GPX format with five layer’s name. We are only interested with the tracks and track_points layers. We can extract them with the st_read() function, by specifying the dsn and the layer. This can be written as; ## obtain track points track.points = sf::st_read(dsn =&quot;./tracks/Track-181204-075451.gpx&quot; ,layer = &quot;track_points&quot;) Reading layer `track_points&#39; from data source `E:\\bookdown\\spatil_r\\tracks\\Track-181204-075451.gpx&#39; using driver `GPX&#39; Simple feature collection with 1467 features and 24 fields geometry type: POINT dimension: XY bbox: xmin: 39.68927 ymin: -8.033337 xmax: 39.75059 ymax: -7.977127 epsg (SRID): 4326 proj4string: +proj=longlat +datum=WGS84 +no_defs ## drop other variable that are not needed track.points = track.points %&gt;% select(elevation = ele, time, speed) ## display track.points Simple feature collection with 1467 features and 3 fields geometry type: POINT dimension: XY bbox: xmin: 39.68927 ymin: -8.033337 xmax: 39.75059 ymax: -7.977127 epsg (SRID): 4326 proj4string: +proj=longlat +datum=WGS84 +no_defs First 10 features: elevation time speed 1 -3.7 2018-12-04 07:54:55 1.471591 2 -5.0 2018-12-04 07:54:58 1.479312 3 -5.6 2018-12-04 07:55:00 1.358867 4 -5.8 2018-12-04 07:55:02 1.530269 5 -5.9 2018-12-04 07:55:04 1.424751 6 -6.1 2018-12-04 07:55:05 1.381000 7 -6.2 2018-12-04 07:55:07 1.437619 8 -6.3 2018-12-04 07:55:09 0.994958 9 -6.3 2018-12-04 07:55:11 1.032018 10 -6.4 2018-12-04 07:55:13 1.369676 geometry 1 POINT (39.75051 -7.977127) 2 POINT (39.75052 -7.977157) 3 POINT (39.75052 -7.977185) 4 POINT (39.75052 -7.977215) 5 POINT (39.75052 -7.977243) 6 POINT (39.75052 -7.977262) 7 POINT (39.75052 -7.97729) 8 POINT (39.75051 -7.97731) 9 POINT (39.75051 -7.97733) 10 POINT (39.7505 -7.977353) track = sf::st_read(dsn =&quot;./tracks/Track-181204-075451.gpx&quot; ,layer = &quot;tracks&quot;) Reading layer `tracks&#39; from data source `E:\\bookdown\\spatil_r\\tracks\\Track-181204-075451.gpx&#39; using driver `GPX&#39; Simple feature collection with 1 feature and 12 fields geometry type: MULTILINESTRING dimension: XY bbox: xmin: 39.68927 ymin: -8.033337 xmax: 39.75059 ymax: -7.977127 epsg (SRID): 4326 proj4string: +proj=longlat +datum=WGS84 +no_defs track Simple feature collection with 1 feature and 12 fields geometry type: MULTILINESTRING dimension: XY bbox: xmin: 39.68927 ymin: -8.033337 xmax: 39.75059 ymax: -7.977127 epsg (SRID): 4326 proj4string: +proj=longlat +datum=WGS84 +no_defs name cmt 1 Tracking android:60fd0ef637a6eb1b &lt;NA&gt; desc src link1_href 1 Tracking recently started 12/4/18 07:54 &lt;NA&gt; &lt;NA&gt; link1_text link1_type link2_href link2_text link2_type number 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 10193 type geometry 1 &lt;NA&gt; MULTILINESTRING ((39.75051 ... 16.2 Introduction A satisfying and important aspect of geographic research is communicating the results in spatial context. With recent advance in technology from satellite, internet and mobile location services, the amount of geographical data has increased significantly. Plenty of data are generated daily with latitude and longitude coordinates attached to it both from satellite observation and social media. To be able to build up a good mental model of the spatial data, you need to invest considerable effort in making your maps as self-explanatory as possible. In this chapter, you’ll learn some of the tools that ggplot2 provides to make elegand maps and graphics (Wickham 2016). Maps are great way to understand patterns from data over space. They are scaled down versions of the physical world, and they’re everywhere. R has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile. ggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. The chapter also introduce you to some extended functionalities from sf (Pebesma 2018), cowplot (Wilke 2019), ggsn (Santos Baquero 2019), ggsci (Xiao 2018), metR (Campitelli 2019), ggrepel (Slowikowski 2018), gganimate (Pedersen and Robinson 2019) and egg (Auguie 2018) packages. Therefore, this chapter focuses on the tools you need to create good graphics. Rather than loading those extensions here, we’ll refer to their functions explicitly, using the :: notation. This will help make it clear which functions are built into ggplot2, and which come from other packages. Ensure you have these packages in your machine, otherwise install them with install.packages() if you don’t already have them. 16.3 Static Maps Static maps are the most common type of visual output from spatial objects. Fixed images for printed outputs, common formats for static maps include .png and .pdf, for raster and vector outputs, respectively. Initially static maps were the only type of map that R could produce. Things have advanced greatly since sp was released (see Bivand, Pebesma, and Gomez-Rubio (2013)). Many new techniques for map making have been developed since then. However, a decade later static plotting was still the emphasis of geographic data visualisation in R (Cheshire and Lovelace 2015). Despite the innovation of interactive mapping in R, static maps are still the foundation of mapping in R. The base plot() function is often the fastest way to create static maps from vector and raster spatial objects. Sometimes simplicity and speed are priorities, especially during the development phase of a project, and this is where plot() excels. The base R approach is also extensible, with plot() offering dozens of arguments. Another low-level approach is the grid package, which provides functions for low-level control of graphical outputs. This section, however, focus on how to make static maps with ggplot2, emphasizing the important aesthetic and layout options. require(tidyverse) 16.3.1 The bathmetry data ggplot2 works with data that are tidy—data frame arranged in such way that observations are in rows and variables are in columns and each value must have its own cell. But, the bathmetry data is from ETOPO1 and came in .asc format. First read the file with the raster::raster() function. ## read the ascii file tz.bath = raster::raster(&quot;e:/GIS/ROADMAP/Etopo 1/Tanzania_etopo1/tanz1_-3432.asc&quot;) tz.bath %&gt;% class() [1] &quot;RasterLayer&quot; attr(,&quot;package&quot;) [1] &quot;raster&quot; We notice that the file is raster and ggplot2 requires data.frame. To continue, we need to convert the data and tidy in format that is **plot2–readable. Specifically, we need to convert raster file into data frame with raster::as.data.frame(xy = TRUE) and specify the xy = TRUE argument. We then rename the x to lon, y to lat and convert bathmetry values from the double presion to integer and select values within the geographical extend of interest and depth between 0 and 1200 meter deep. ## convert raster to data frame tz.bath.df = tz.bath %&gt;% raster::as.data.frame(xy = TRUE) %&gt;% dplyr::as_tibble() ## rename the variable tz.bath.df = tz.bath.df %&gt;% dplyr::rename(lon = x, lat = y, depth = 3)%&gt;% dplyr::mutate(depth = as.integer(depth)) ## chop the area of interest off.kimbiji = tz.bath.df %&gt;% dplyr::filter(lon &gt; 38.5 &amp; lon &lt; 40 &amp; lat &gt; -7.2 &amp; lat &lt; - 6 &amp; depth &gt; -1200&amp; depth &lt; 0 ) The bathmetry file now contain three variables, the lon, lat and depth as seen in table 16.1 off.kimbiji %&gt;% dplyr::sample_n(10) %&gt;% knitr::kable(col.names = c(&quot;Longitude&quot;, &quot;Latitude&quot;, &quot;Depth (meters)&quot;), digits = 3, caption = &quot;Ten randomly selected points of bathmetry values off Kimbiji, Tanzania&quot;, align = &quot;c&quot;)%&gt;% kableExtra::column_spec(column = 2:3, width = &quot;3cm&quot;)%&gt;% kableExtra::add_header_above(c(&quot;Coordinate (Degree)&quot; = 2,&quot;&quot;)) Table 16.1: Ten randomly selected points of bathmetry values off Kimbiji, Tanzania Coordinate (Degree) Longitude Latitude Depth (meters) 39.983 -6.733 -512 39.133 -6.200 -1 39.017 -6.317 -4 39.067 -6.317 -2 39.333 -6.517 -18 39.767 -6.700 -504 39.650 -6.400 -310 39.117 -6.017 -6 39.150 -6.250 -4 39.900 -6.717 -419 16.4 Basemap We also need basemap—country boundary layer. We use the st_read() function from sf package to read the shapefile boundary layer. Since the layer is for the whole Africa, to reduce the processing time for ploting the map of africa, we use the st_crop() function to chop the area of interest. africa = sf::st_read(&quot;./data/africa.shp&quot;) Reading layer `africa&#39; from data source `E:\\bookdown\\spatil_r\\data\\africa.shp&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 59 features and 7 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -25.35875 ymin: -34.83983 xmax: 57.80085 ymax: 37.34962 epsg (SRID): 4326 proj4string: +proj=longlat +datum=WGS84 +no_defs kimbiji = africa %&gt;% sf::st_crop(xmin = 38.0, xmax = 40.5, ymin = -8, ymax = -5.5) 16.5 Creating contour map Once we have the data ready, we can tools in ggplot2 and add-on packages to create the bathmetry map off–Kimbiji located between longitude 38.5°E and 40.1°E and latitude 7.2°S and `r LatLabel(-6.0). The code block below was used to create figure 16.1 Figure 16.1: Map of Off-Kimbiji showing contour lines. The grey lines are contour at 50 m interval and the black line are contoured at 200 m intervals There are fourteen lined of codes in the chunk to make figure 16.1. That’s a lot! Don’t get intimidated, I will explain in detail how each line of code work together to make this figure. As before, you start plotting ggplot2 with the ggplot() function as the first line. Surprisingly, the ggplot() is empty without any argument specified. When mapping with geom_sf() function in ggplot2 package, you are advised to leave the ggplot() function empty. This will allow the geom_sf() to label the axes with the appropriate geographical labelling for longitude and latitude. The second line of gode add a simple feature with a geom_sf() function from sf package. Note however, I specified the geom_sf() to fill the boundary layer with grey of 90 shade and the stroke with black colour. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;) note that ggplot2 plot the map with default aesthetic settings. The plot background is filled with gray color and without stroke but the grids are white colored. The third line add the contour lines spaced at 50 meter intervals. Instead of using geom_contour() from ggplot2, the geom_contour2() from metR package was used. They both serve the same task. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;) Like the third line, the fourth line add contour lines, but instead of spacing them into meters, these are spaced at 200 meters interval and are black in color. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200) The fifth line add the label on contour spaced at 200 meter interval with geom_text_contour() function from metR package. Here is where you will find the useful of package like metR that extends the ggplot2, for which the current version (2.3.1.1) is unable. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200)+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200, rotate = FALSE) The sixth line zoom the map to only the geographical extent we are interested with using the coord_sf() function from sf package. We could also use the coord_cartesin() to limit the area. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200)+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200, rotate = FALSE)+ coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3)) We got a glimpse of the map now, let us use theme to make some changes. The background was set to white with panel.background = element_rect(fill = \"white\"), and removed grids with panel.grid = element_line(colour = NA) and change the font size of the axis label to 11 points with axis.text = element_text(size = 11). The theme_bw() just set the border of the plot to black with solid line. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200)+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200, rotate = FALSE)+ coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+ theme_bw()+ theme(panel.background = element_rect(fill = &quot;white&quot;), panel.grid = element_line(colour = NA), axis.text = element_text(size = 11)) The good thing to start making maps is with an understanding of the map elements. A static map can be composed of many different map elements. These include main map body, legend, title, scale indicator, orientation indicator, inset map and source or ancillary information. By increasing the font size of axis textual label to 11, the axes are cluttered. adding the scale can improve the labelling. scale_x_continuous(breaks = seq(39.2, 39.8, .2)) in line 9 force ggplot2 to label the x–axis four letter that are spaced with 0.2 latitude and scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4)) in line 10 label four digits of longitude. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200)+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200, rotate = FALSE)+ coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+ theme_bw()+ theme(panel.background = element_rect(fill = &quot;white&quot;), panel.grid = element_line(colour = NA), axis.text = element_text(size = 11))+ scale_x_continuous(breaks = seq(39.2, 39.8, .2))+ scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4)) Because the axes are abbreviated with longitude and latitude symbol, line 11 in the code remove the axes title label. Line 12 to 14 add textual label on the map with the annotate() function. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200)+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200, rotate = FALSE)+ coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+ theme_bw()+ theme(panel.background = element_rect(fill = &quot;white&quot;), panel.grid = element_line(colour = NA), axis.text = element_text(size = 11))+ scale_x_continuous(breaks = seq(39.2, 39.8, .2))+ scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4))+ labs(x = NULL, y = NULL)+ annotate(geom = &quot;text&quot;, x = 39.28, y = -6.48, label = &quot;Zanzibar \\nChannel&quot;)+ annotate(geom = &quot;text&quot;, x = 39.5, y = -6.37, label = &quot;Unguja \\nIsland&quot;)+ annotate(geom = &quot;text&quot;, x = 39.3, y = -6.91, label = &quot;Dar es Salaam&quot;) Close look of figure 16.1, the north arrrow and the scale bar are missing. The last two lines of our code inset the scalebar and north arrow on map using the ggsn::scalebar() from ggsn package and ggspatial::annotation_north_arrow() functions from ggspatial package. In a nutshell, making this map using ggplot2 and ancillary extensions used fiften line codes and hundred of arguments. This are very common task of making maps with the combination of tools from different packages. ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200)+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200, rotate = FALSE)+ coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+ theme_bw()+ theme(panel.background = element_rect(fill = &quot;white&quot;), panel.grid = element_line(colour = NA), axis.text = element_text(size = 11))+ scale_x_continuous(breaks = seq(39.2, 39.8, .2))+ scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4))+ labs(x = NULL, y = NULL)+ annotate(geom = &quot;text&quot;, x = 39.28, y = -6.48, label = &quot;Zanzibar \\nChannel&quot;)+ annotate(geom = &quot;text&quot;, x = 39.5, y = -6.37, label = &quot;Unguja \\nIsland&quot;)+ annotate(geom = &quot;text&quot;, x = 39.3, y = -6.91, label = &quot;Dar es Salaam&quot;)+ ggsn::scalebar(location = &quot;bottomleft&quot;, x.min = 39.2, x.max = 39.8, y.min = -6.98, y.max = -6.35, dist = 10, dist_unit = &quot;km&quot;, transform = TRUE, model = &quot;WGS84&quot;, st.dist = 0.03, st.size = 4, height = 0.03)+ # ggspatial::annotation_scale(height = unit(.35, &quot;cm&quot;), pad_x = unit(.5, &quot;cm&quot;), # tick_height = unit(3, &quot;cm&quot;), pad_y = unit(.5, &quot;cm&quot;), text_cex = .85)+ ggspatial::annotation_north_arrow(location = &quot;tr&quot;, width = unit(.75, &quot;cm&quot;), height = unit(1, &quot;cm&quot;)) metR package has geom_contour_fill() function that draw filled contour lines and geom_contour_tanaka(), which illunate contours with varying brithtness to create an illusion of relief. The code chunk to create highlighted filled contour using metR function can be written as; ggplot()+ metR::geom_contour_fill(data = off.kimbiji, aes(x = lon, y = lat, z=depth), na.fill = TRUE, show.legend = FALSE)+ metR::geom_contour_tanaka(data = off.kimbiji, aes(x = lon, y = lat, z=depth))+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), rotate = TRUE, check_overlap = TRUE, size = 3.0)+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+ theme_bw()+ theme(panel.background = element_rect(fill = &quot;white&quot;), panel.grid = element_line(colour = NA), axis.text = element_text(size = 11))+ scale_x_continuous(breaks = seq(39.2, 39.8, .2))+ scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4))+ scale_fill_gradientn(colours = oce::oce.colorsGebco(120))+ labs(x = NULL, y = NULL)+ annotate(geom = &quot;text&quot;, x = 39.28, y = -6.48, label = &quot;Zanzibar \\nChannel&quot;)+ annotate(geom = &quot;text&quot;, x = 39.5, y = -6.37, label = &quot;Unguja \\nIsland&quot;)+ annotate(geom = &quot;text&quot;, x = 39.3, y = -6.91, label = &quot;Dar es Salaam&quot;)+ ggsn::scalebar(location = &quot;bottomleft&quot;, x.min = 39.2, x.max = 39.8, y.min = -6.98, y.max = -6.35, dist = 10, dist_unit = &quot;km&quot;, transform = TRUE, model = &quot;WGS84&quot;, st.dist = 0.03, st.size = 4, height = 0.03)+ # ggspatial::annotation_scale(location = &quot;bl&quot;)+ ggspatial::annotation_north_arrow(location = &quot;tr&quot;, width = unit(.75, &quot;cm&quot;), height = unit(1.0, &quot;cm&quot;)) 16.6 Inset maps An inset map is a smaller map rendered within or next to the main map. It could serve many different purposes, including showing the relative position of the study area in regional area. In figure ?? is the map showing the contour interval off-kimbiji, Tanzania. The inset map show the area of Kimbiji in the Western Indian Ocean Region. The chunk below was used to create figure 16.2. In a nutshell, we assign the study area map as main.map and the regional map as inset.map and then we used function from the cowplot package to combine the two maps. main.map = ggplot()+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 50, col = &quot;grey&quot;)+ metR::geom_contour2(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200)+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), binwidth = 200, rotate = FALSE)+ coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+ theme_bw()+ theme(panel.background = element_rect(fill = &quot;white&quot;), panel.grid = element_line(colour = NA), axis.text = element_text(size = 11))+ scale_x_continuous(breaks = seq(39.2, 39.8, .2))+ scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4))+ labs(x = NULL, y = NULL)+ annotate(geom = &quot;text&quot;, x = 39.28, y = -6.48, label = &quot;Zanzibar \\nChannel&quot;)+ annotate(geom = &quot;text&quot;, x = 39.5, y = -6.37, label = &quot;Unguja \\nIsland&quot;)+ annotate(geom = &quot;text&quot;, x = 39.3, y = -6.91, label = &quot;Dar es Salaam&quot;)+ ggspatial::annotation_scale(location = &quot;bl&quot;)+ ggspatial::annotation_north_arrow(location = &quot;tr&quot;) world = spData::world aoi = data.frame(lon = c(38.5, 40, 40, 38.5, 38.5), lat = c(-8, -8, -6, -6, -8)) inset.map = ggplot()+ geom_sf(data = world, fill = &quot;grey90&quot;, col = 1)+ coord_sf(xlim = c(37, 45), ylim = c(-12,-1))+ geom_path(data = aoi, aes(x = lon, y = lat), size = 1.2)+ theme_bw()+ theme(plot.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_line(colour = &quot;white&quot;)) + labs(x = NULL, y = NULL) cowplot::ggdraw()+ cowplot::draw_plot(plot = main.map, x = 0, y = 0, width = 1, height = 1, scale = 1)+ cowplot::draw_plot(plot = inset.map, x = .558, y = .05, width = .3,height = .3) Figure 16.2: The main map with the inset map showing the positon of the study areas in the region main.map = ggplot()+ metR::geom_contour_fill(data = off.kimbiji, aes(x = lon, y = lat, z=depth), na.fill = TRUE, show.legend = FALSE)+ metR::geom_contour_tanaka(data = off.kimbiji, aes(x = lon, y = lat, z=depth))+ metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), rotate = TRUE, check_overlap = TRUE, size = 3.4)+ geom_sf(data = kimbiji, fill = &quot;grey90&quot;, col = &quot;grey40&quot;)+ coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+ theme_bw()+ theme(panel.background = element_rect(fill = &quot;white&quot;), panel.grid = element_line(colour = NA), axis.text = element_text(size = 11))+ scale_x_continuous(breaks = seq(39.2, 39.8, .2))+ scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4))+ scale_fill_gradientn(colours = oce::oce.colorsGebco(120))+ labs(x = NULL, y = NULL)+ annotate(geom = &quot;text&quot;, x = 39.28, y = -6.48, label = &quot;Zanzibar \\nChannel&quot;)+ annotate(geom = &quot;text&quot;, x = 39.5, y = -6.37, label = &quot;Unguja \\nIsland&quot;)+ annotate(geom = &quot;text&quot;, x = 39.3, y = -6.91, label = &quot;Dar es Salaam&quot;)+ ggspatial::annotation_scale(location = &quot;bl&quot;)+ ggspatial::annotation_north_arrow(location = &quot;tr&quot;, width = unit(.75, &quot;cm&quot;)) world = spData::world aoi = data.frame(lon = c(38.5, 40, 40, 38.5, 38.5), lat = c(-8, -8, -6, -6, -8)) inset.map = ggplot()+ geom_sf(data = world, fill = &quot;grey90&quot;, col = 1)+ coord_sf(xlim = c(37, 45), ylim = c(-12,-1))+ geom_path(data = aoi, aes(x = lon, y = lat), size = 1.2)+ theme_bw()+ theme(plot.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_line(colour = &quot;white&quot;)) + labs(x = NULL, y = NULL) cowplot::ggdraw()+ cowplot::draw_plot(plot = main.map, x = 0, y = 0, width = 1, height = 1, scale = 1)+ cowplot::draw_plot(plot = inset.map, x = .558, y = .05, width = .3,height = .3) Figure 16.3: The main map with the inset map showing the positon of the study areas in the region 16.6.1 Choropleth maps Chloropleth maps use color or shading on predefined areas to map values of a numeric or categorical variable in that area. For example we are interested to map the different coral reefs in Jibondo Island, Mafia, Tanzania. First we import the data into R using the st_read() function from sf package (Pebesma 2018) jibondo.reefs = sf::st_read(dsn = &quot;./data/jibondo_reefs.shp&quot;) Reading layer `jibondo_reefs&#39; from data source `E:\\bookdown\\spatil_r\\data\\jibondo_reefs.shp&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 16 features and 4 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: 39.58571 ymin: -8.135198 xmax: 39.83 ymax: -7.903451 epsg (SRID): 4326 proj4string: +proj=longlat +datum=WGS84 +no_defs The jibondo.reefs file is simple feature (equivalent to shapefile) with sixteeen polygons in four groups—coastal-shallow areas, islands, reef flat and submerged reefs. We use the variable type to map the different coastal features in Jibondo. The code used to make figure 16.4 is written as: require(RColorBrewer) ggplot()+ geom_sf(data = jibondo.reefs, aes(fill = type)) + coord_sf(xlim = c(39.57, 39.88), ylim = c(-8.15,-7.88)) + geom_sf_text(data = jibondo.reefs, aes(label = mwamba), check_overlap = TRUE) + theme_bw() %+% theme(axis.text = element_text(size = 11), legend.position = c(.8,.18)) + scale_fill_brewer(palette = &quot;Accent&quot;) + metR::scale_x_longitude(ticks = 0.1) + metR::scale_y_latitude(ticks = 0.08) + guides(fill = guide_legend(title = &quot;Reef Type&quot;, title.position = &quot;top&quot;, keywidth = 1.1, ncol = 1)) Figure 16.4: Reefs and non-reeef features in Jibondo Island, Mafia The variable used to make Figure 16.4 is a categorical, but we can also map continuous variables. For this case, we want to map the catch per unit effort (CPUE) of octopus at each reef to identify octopus catches at different reefs as seen in figure require(RColorBrewer) ggplot()+ geom_sf(data = jibondo.reefs, aes(fill = cpue %&gt;%round(2) %&gt;% as.factor()))+ coord_sf(xlim = c(39.57, 39.88), ylim = c(-8.15,-7.88))+ geom_sf_text(data = jibondo.reefs, aes(label = mwamba), check_overlap = TRUE) + theme_bw() %+% theme(axis.text = element_text(size = 11), legend.position = c(.9,.25)) + # scale_fill_brewer(palette = &quot;Accent&quot;) + ggsci::scale_fill_d3()+ metR::scale_x_longitude(ticks = 0.1) + metR::scale_y_latitude(ticks = 0.08)+ guides(fill = guide_legend(title.position = &quot;top&quot;, keywidth = 1.1, ncol = 1, title = &quot;CPUE&quot;)) Figure 16.5: Catches of Octopus at Jibondo Island Finally let’s map the the spatial patterns of sea surface temperature anomaly. We plot the departure of sea surface temperature from zonal average mean. Let’s import the dataset from the workspace. For more information on how to compute the zonal departure see chapter… temperature.anomaly = read_csv(&quot;./data/sst_anomaly.csv&quot;) Coordinate (Degree) Latitude Longitude Depth Anomaly 70.5 -11.5 0 -0.18 -70.5 -90.5 0 -0.07 -15.5 -1.5 0 -3.81 -40.5 -168.5 0 0.81 -55.5 34.5 0 -2.13 -10.5 119.5 0 1.32 55.5 -45.5 0 -0.53 -60.5 -11.5 0 -1.73 70.5 -13.5 0 -0.56 55.5 0.5 0 3.26 ggplot() + metR::geom_contour_fill(data = temperature.anomaly, aes(x = lon, y = lat, z = anomaly), na.fill = T) + geom_sf(data = spData::world, col = NA, fill = &quot;grey40&quot;)+ coord_sf(xlim = c(-180,178), ylim = c(-90,85), clip = &quot;on&quot;, expand = FALSE)+ scale_fill_gradientn(colours = oce::oce.colors9A(120), breaks = seq(-6,6,2))+ theme_bw() + theme(legend.position = &quot;right&quot;, panel.background = element_blank(), axis.text = element_text(size = 11, colour = &quot;black&quot;), legend.text = element_text(size = 10), legend.title = element_text(size = 11))+ guides(fill = guide_colorbar(title = expression(Temperature~anomaly~(degree*C)), title.position = &quot;right&quot;, title.hjust = 0.5, title.theme = element_text(angle = 90), label.theme = element_text(size = 10), direction = &quot;vertical&quot;, reverse = FALSE, raster = FALSE, barwidth = unit(.4, &quot;cm&quot;), barheight = unit(6.5, &quot;cm&quot;)))+ metR::scale_y_latitude(ticks = 30) + metR::scale_x_longitude(ticks = 45)+ labs(x = NULL, y = NULL) Figure 16.6: center 16.7 Animated maps 16.8 Interactive maps References "],
["animation.html", "Chapter 17 Animation 17.1 Animation 17.2 isosurface 17.3 checking for proper interpolation methods", " Chapter 17 Animation require(tidyverse) require(magrittr) require(gganimate) load addition functions source(&quot;e:/Data Manipulation/semba_functions.R&quot;) 17.1 Animation In the previous chapter we saw how to make static graphs in R with ggplot2. In this chapter we focus on creating animation. Animations is very important part of visualization. Animated charts are visually appealing and catches the attention of audiences. We will use gganimate package for creating animated plots. Its an extension of ggplot2—a popular package for making graphics in R. Note that, current gganimate does not support ggrob created with geoms from metR package. You need to replace metR geoms with those of ggplot2 before you animate the plot. ggplot(data = ctd %&gt;% filter(pressure ==5), aes(x = lon, y = lat)) + geom_point()+ metR::scale_x_longitude(ticks = 0.5)+ metR::scale_y_latitude(ticks = 1) map = ggplot(data = ctd %&gt;% filter(pressure ==5), aes(x = lon, y = lat)) + geom_point(size = 4) + metR::scale_x_longitude(ticks = 0.5)+ metR::scale_y_latitude(ticks = 1)+ # geom_text(aes(y = lat+.25,label = station))+ theme_minimal()+ theme(axis.text = element_text(size = 14))+ # labs(x = NULL, y = NULL, title = &quot;The cast was done on {frame_time}&quot;) + labs(x = NULL, y = NULL, title = &quot;CTD Instrument casted at {closest_state}&quot;) + # gganimate::transition_time(time = time)+ gganimate::transition_states(states = time, transition_length = 1,wrap = TRUE, state_length = 2)+ gganimate::view_follow(fixed_y = TRUE, fixed_x = FALSE)+ gganimate::ease_aes(&quot;cubic-in-out&quot;)+ # gganimate::ease_aes(y = &#39;bounce-in&#39;)+ gganimate::shadow_mark() map Frame 1 (1%) Frame 2 (2%) Frame 3 (3%) Frame 4 (4%) Frame 5 (5%) Frame 6 (6%) Frame 7 (7%) Frame 8 (8%) Frame 9 (9%) Frame 10 (10%) Frame 11 (11%) Frame 12 (12%) Frame 13 (13%) Frame 14 (14%) Frame 15 (15%) Frame 16 (16%) Frame 17 (17%) Frame 18 (18%) Frame 19 (19%) Frame 20 (20%) Frame 21 (21%) Frame 22 (22%) Frame 23 (23%) Frame 24 (24%) Frame 25 (25%) Frame 26 (26%) Frame 27 (27%) Frame 28 (28%) Frame 29 (29%) Frame 30 (30%) Frame 31 (31%) Frame 32 (32%) Frame 33 (33%) Frame 34 (34%) Frame 35 (35%) Frame 36 (36%) Frame 37 (37%) Frame 38 (38%) Frame 39 (39%) Frame 40 (40%) Frame 41 (41%) Frame 42 (42%) Frame 43 (43%) Frame 44 (44%) Frame 45 (45%) Frame 46 (46%) Frame 47 (47%) Frame 48 (48%) Frame 49 (49%) Frame 50 (50%) Frame 51 (51%) Frame 52 (52%) Frame 53 (53%) Frame 54 (54%) Frame 55 (55%) Frame 56 (56%) Frame 57 (57%) Frame 58 (58%) Frame 59 (59%) Frame 60 (60%) Frame 61 (61%) Frame 62 (62%) Frame 63 (63%) Frame 64 (64%) Frame 65 (65%) Frame 66 (66%) Frame 67 (67%) Frame 68 (68%) Frame 69 (69%) Frame 70 (70%) Frame 71 (71%) Frame 72 (72%) Frame 73 (73%) Frame 74 (74%) Frame 75 (75%) Frame 76 (76%) Frame 77 (77%) Frame 78 (78%) Frame 79 (79%) Frame 80 (80%) Frame 81 (81%) Frame 82 (82%) Frame 83 (83%) Frame 84 (84%) Frame 85 (85%) Frame 86 (86%) Frame 87 (87%) Frame 88 (88%) Frame 89 (89%) Frame 90 (90%) Frame 91 (91%) Frame 92 (92%) Frame 93 (93%) Frame 94 (94%) Frame 95 (95%) Frame 96 (96%) Frame 97 (97%) Frame 98 (98%) Frame 99 (99%) Frame 100 (100%) Finalizing encoding... done! ## frames per second (fps) It is the amount of time spend on each frame per second. You can use parameter fps in animate() function. By default, it is 10 frames per seconds. Example the cast location animation map is faster and visually difficult to follow. We can alter the speed by changing the nframes = 40 and fps = 1 in animate() function as the code show in the chunk: gganimate::animate(plot = map, nframes=50, fps = 1) Frame 1 (2%) Frame 2 (4%) Frame 3 (6%) Frame 4 (8%) Frame 5 (10%) Frame 6 (12%) Frame 7 (14%) Frame 8 (16%) Frame 9 (18%) Frame 10 (20%) Frame 11 (22%) Frame 12 (24%) Frame 13 (26%) Frame 14 (28%) Frame 15 (30%) Frame 16 (32%) Frame 17 (34%) Frame 18 (36%) Frame 19 (38%) Frame 20 (40%) Frame 21 (42%) Frame 22 (44%) Frame 23 (46%) Frame 24 (48%) Frame 25 (50%) Frame 26 (52%) Frame 27 (54%) Frame 28 (56%) Frame 29 (58%) Frame 30 (60%) Frame 31 (62%) Frame 32 (64%) Frame 33 (66%) Frame 34 (68%) Frame 35 (70%) Frame 36 (72%) Frame 37 (74%) Frame 38 (76%) Frame 39 (78%) Frame 40 (80%) Frame 41 (82%) Frame 42 (84%) Frame 43 (86%) Frame 44 (88%) Frame 45 (90%) Frame 46 (92%) Frame 47 (94%) Frame 48 (96%) Frame 49 (98%) Frame 50 (100%) Finalizing encoding... done! 17.2 isosurface depth = c(10,50,100,200) algoa = NULL for (i in seq_along(depth)){ strata = ctd %&gt;% filter(lon &gt; 39.3 &amp; lon &lt; 40.5 &amp; lat &gt;= -10 &amp; pressure == depth[i]) %&gt;% select(3:9) Lon = strata %&gt;% pull(lon) Lat = strata %&gt;% pull(lat) data = strata %&gt;% select(4:7) for (j in seq_along(data)){ algoa.interp = strata %$% oce::interpBarnes(x = Lon, y = Lat, z = data[j]%&gt;% pull()) algoa.tb = algoa.interp %$% matrix_tb(x = xg, y = yg, data = zg) %&gt;% rename(lon = x, lat = y) %&gt;% mutate(variable = colnames(data[j]), pressure = depth[i]) algoa = algoa %&gt;% bind_rows(algoa.tb) } } # algoa %&gt;% group_by(pressure, variable) %&gt;% summarise(average = mean(value, na.rm = TRUE)) salinity = ggplot()+ geom_raster(data = algoa %&gt;% filter(variable == &quot;salinity&quot;) %&gt;% mutate(pressure = pressure %&gt;% as.factor()), aes(x = lon, y = lat, fill = value))+ # metR::geom_contour_fill(data = algoa %&gt;% filter(variable == &quot;salinity&quot;) %&gt;% # mutate(pressure = pressure %&gt;% as.factor()), # aes(x = lon, y = lat, z = value), na.fill = TRUE, bins = 20)+ scale_fill_gradientn(colours = oce::oceColors9A(120))+ scale_y_continuous(breaks = seq(-8.5,-6,2.5), labels = metR::LatLabel(seq(-8.5,-6,2.5)))+ scale_x_continuous(breaks = seq(39.55,40.25,length.out = 3) %&gt;% round(2), labels = metR::LonLabel(seq(39.55,40.25,length.out = 3)))+ labs(x = NULL, y = NULL, title = &#39;Salinity at depth of {closest_state} meters&#39;)+ theme_bw()+ theme(axis.text = element_text(size = 16, colour = &quot;black&quot;), title = element_text(size = 16))+ coord_cartesian(expand = FALSE)+ guides(fill = guide_colorbar(title =expression(Salinity), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 14), label.theme = element_text(size = 14), barheight = 15, barwidth = .95))+ gganimate::transition_states(pressure, transition_length = 1, state_length = 2)+ gganimate::view_step()+ gganimate::ease_aes(&quot;cubic-in-out&quot;) # Slow start and end for a smoother look salinity Frame 1 (1%) Frame 2 (2%) Frame 3 (3%) Frame 4 (4%) Frame 5 (5%) Frame 6 (6%) Frame 7 (7%) Frame 8 (8%) Frame 9 (9%) Frame 10 (10%) Frame 11 (11%) Frame 12 (12%) Frame 13 (13%) Frame 14 (14%) Frame 15 (15%) Frame 16 (16%) Frame 17 (17%) Frame 18 (18%) Frame 19 (19%) Frame 20 (20%) Frame 21 (21%) Frame 22 (22%) Frame 23 (23%) Frame 24 (24%) Frame 25 (25%) Frame 26 (26%) Frame 27 (27%) Frame 28 (28%) Frame 29 (29%) Frame 30 (30%) Frame 31 (31%) Frame 32 (32%) Frame 33 (33%) Frame 34 (34%) Frame 35 (35%) Frame 36 (36%) Frame 37 (37%) Frame 38 (38%) Frame 39 (39%) Frame 40 (40%) Frame 41 (41%) Frame 42 (42%) Frame 43 (43%) Frame 44 (44%) Frame 45 (45%) Frame 46 (46%) Frame 47 (47%) Frame 48 (48%) Frame 49 (49%) Frame 50 (50%) Frame 51 (51%) Frame 52 (52%) Frame 53 (53%) Frame 54 (54%) Frame 55 (55%) Frame 56 (56%) Frame 57 (57%) Frame 58 (58%) Frame 59 (59%) Frame 60 (60%) Frame 61 (61%) Frame 62 (62%) Frame 63 (63%) Frame 64 (64%) Frame 65 (65%) Frame 66 (66%) Frame 67 (67%) Frame 68 (68%) Frame 69 (69%) Frame 70 (70%) Frame 71 (71%) Frame 72 (72%) Frame 73 (73%) Frame 74 (74%) Frame 75 (75%) Frame 76 (76%) Frame 77 (77%) Frame 78 (78%) Frame 79 (79%) Frame 80 (80%) Frame 81 (81%) Frame 82 (82%) Frame 83 (83%) Frame 84 (84%) Frame 85 (85%) Frame 86 (86%) Frame 87 (87%) Frame 88 (88%) Frame 89 (89%) Frame 90 (90%) Frame 91 (91%) Frame 92 (92%) Frame 93 (93%) Frame 94 (94%) Frame 95 (95%) Frame 96 (96%) Frame 97 (97%) Frame 98 (98%) Frame 99 (99%) Frame 100 (100%) Finalizing encoding... done! # gganimate::anim_save(filename = &quot;salinity_animation.gif&quot;, animation = salinity) oxygen = ggplot()+ geom_raster(data = algoa %&gt;% filter(variable == &quot;oxygen&quot;) %&gt;% mutate(pressure = pressure %&gt;% as.factor()), aes(x = lon, y = lat, fill = value))+ scale_fill_gradientn(colours = oce::oceColors9A(120))+ scale_y_continuous(breaks = seq(-8.5,-6,2.5), labels = metR::LatLabel(seq(-8.5,-6,2.5)))+ scale_x_continuous(breaks = seq(39.55,40.25,length.out = 3) %&gt;% round(2), labels = metR::LonLabel(seq(39.55,40.25,length.out = 3)))+ labs(x = NULL, y = NULL, title = &#39;Dissolved oxygen at depth of {closest_state} meters&#39;)+ theme_bw()+ theme(axis.text = element_text(size = 16, colour = &quot;black&quot;), title = element_text(size = 16))+ coord_cartesian(expand = FALSE)+ guides(fill = guide_colorbar(title =expression(Dissolved~Oxygen~(mgL)), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 14), label.theme = element_text(size = 14), barheight = 15, barwidth = .95))+ gganimate::transition_states(pressure, transition_length = 1, state_length = 2)+ gganimate::view_step()+ gganimate::ease_aes(&quot;cubic-in-out&quot;) # Slow start and end for a smoother look oxygen Frame 1 (1%) Frame 2 (2%) Frame 3 (3%) Frame 4 (4%) Frame 5 (5%) Frame 6 (6%) Frame 7 (7%) Frame 8 (8%) Frame 9 (9%) Frame 10 (10%) Frame 11 (11%) Frame 12 (12%) Frame 13 (13%) Frame 14 (14%) Frame 15 (15%) Frame 16 (16%) Frame 17 (17%) Frame 18 (18%) Frame 19 (19%) Frame 20 (20%) Frame 21 (21%) Frame 22 (22%) Frame 23 (23%) Frame 24 (24%) Frame 25 (25%) Frame 26 (26%) Frame 27 (27%) Frame 28 (28%) Frame 29 (29%) Frame 30 (30%) Frame 31 (31%) Frame 32 (32%) Frame 33 (33%) Frame 34 (34%) Frame 35 (35%) Frame 36 (36%) Frame 37 (37%) Frame 38 (38%) Frame 39 (39%) Frame 40 (40%) Frame 41 (41%) Frame 42 (42%) Frame 43 (43%) Frame 44 (44%) Frame 45 (45%) Frame 46 (46%) Frame 47 (47%) Frame 48 (48%) Frame 49 (49%) Frame 50 (50%) Frame 51 (51%) Frame 52 (52%) Frame 53 (53%) Frame 54 (54%) Frame 55 (55%) Frame 56 (56%) Frame 57 (57%) Frame 58 (58%) Frame 59 (59%) Frame 60 (60%) Frame 61 (61%) Frame 62 (62%) Frame 63 (63%) Frame 64 (64%) Frame 65 (65%) Frame 66 (66%) Frame 67 (67%) Frame 68 (68%) Frame 69 (69%) Frame 70 (70%) Frame 71 (71%) Frame 72 (72%) Frame 73 (73%) Frame 74 (74%) Frame 75 (75%) Frame 76 (76%) Frame 77 (77%) Frame 78 (78%) Frame 79 (79%) Frame 80 (80%) Frame 81 (81%) Frame 82 (82%) Frame 83 (83%) Frame 84 (84%) Frame 85 (85%) Frame 86 (86%) Frame 87 (87%) Frame 88 (88%) Frame 89 (89%) Frame 90 (90%) Frame 91 (91%) Frame 92 (92%) Frame 93 (93%) Frame 94 (94%) Frame 95 (95%) Frame 96 (96%) Frame 97 (97%) Frame 98 (98%) Frame 99 (99%) Frame 100 (100%) Finalizing encoding... done! gganimate::anim_save(filename = &quot;oxygen_animation.gif&quot;, animation = oxygen) Frame 1 (1%) Frame 2 (2%) Frame 3 (3%) Frame 4 (4%) Frame 5 (5%) Frame 6 (6%) Frame 7 (7%) Frame 8 (8%) Frame 9 (9%) Frame 10 (10%) Frame 11 (11%) Frame 12 (12%) Frame 13 (13%) Frame 14 (14%) Frame 15 (15%) Frame 16 (16%) Frame 17 (17%) Frame 18 (18%) Frame 19 (19%) Frame 20 (20%) Frame 21 (21%) Frame 22 (22%) Frame 23 (23%) Frame 24 (24%) Frame 25 (25%) Frame 26 (26%) Frame 27 (27%) Frame 28 (28%) Frame 29 (29%) Frame 30 (30%) Frame 31 (31%) Frame 32 (32%) Frame 33 (33%) Frame 34 (34%) Frame 35 (35%) Frame 36 (36%) Frame 37 (37%) Frame 38 (38%) Frame 39 (39%) Frame 40 (40%) Frame 41 (41%) Frame 42 (42%) Frame 43 (43%) Frame 44 (44%) Frame 45 (45%) Frame 46 (46%) Frame 47 (47%) Frame 48 (48%) Frame 49 (49%) Frame 50 (50%) Frame 51 (51%) Frame 52 (52%) Frame 53 (53%) Frame 54 (54%) Frame 55 (55%) Frame 56 (56%) Frame 57 (57%) Frame 58 (58%) Frame 59 (59%) Frame 60 (60%) Frame 61 (61%) Frame 62 (62%) Frame 63 (63%) Frame 64 (64%) Frame 65 (65%) Frame 66 (66%) Frame 67 (67%) Frame 68 (68%) Frame 69 (69%) Frame 70 (70%) Frame 71 (71%) Frame 72 (72%) Frame 73 (73%) Frame 74 (74%) Frame 75 (75%) Frame 76 (76%) Frame 77 (77%) Frame 78 (78%) Frame 79 (79%) Frame 80 (80%) Frame 81 (81%) Frame 82 (82%) Frame 83 (83%) Frame 84 (84%) Frame 85 (85%) Frame 86 (86%) Frame 87 (87%) Frame 88 (88%) Frame 89 (89%) Frame 90 (90%) Frame 91 (91%) Frame 92 (92%) Frame 93 (93%) Frame 94 (94%) Frame 95 (95%) Frame 96 (96%) Frame 97 (97%) Frame 98 (98%) Frame 99 (99%) Frame 100 (100%) Finalizing encoding... done! temperature = ggplot()+ geom_raster(data = algoa %&gt;% filter(variable == &quot;temperature&quot;) %&gt;% mutate(pressure = pressure %&gt;% as.factor()), aes(x = lon, y = lat, fill = value))+ scale_fill_gradientn(colours = oce::oceColors9A(120))+ scale_y_continuous(breaks = seq(-8.5,-6,2.5), labels = metR::LatLabel(seq(-8.5,-6,2.5)))+ scale_x_continuous(breaks = seq(39.55,40.25,length.out = 3) %&gt;% round(2), labels = metR::LonLabel(seq(39.55,40.25,length.out = 3)))+ labs(x = NULL, y = NULL, title = &#39;Temperature at depth of {closest_state} meters&#39;)+ theme_bw()+ theme(axis.text = element_text(size = 16, colour = &quot;black&quot;), title = element_text(size = 16))+ coord_cartesian(expand = FALSE)+ guides(fill = guide_colorbar(title =expression(Temperature~(degree*C)), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 14), label.theme = element_text(size = 14), barheight = 15, barwidth = .95))+ gganimate::transition_states(pressure, transition_length = 1, state_length = 2)+ gganimate::view_step()+ gganimate::ease_aes(&quot;cubic-in-out&quot;) # Slow start and end for a smoother look temperature Frame 1 (1%) Frame 2 (2%) Frame 3 (3%) Frame 4 (4%) Frame 5 (5%) Frame 6 (6%) Frame 7 (7%) Frame 8 (8%) Frame 9 (9%) Frame 10 (10%) Frame 11 (11%) Frame 12 (12%) Frame 13 (13%) Frame 14 (14%) Frame 15 (15%) Frame 16 (16%) Frame 17 (17%) Frame 18 (18%) Frame 19 (19%) Frame 20 (20%) Frame 21 (21%) Frame 22 (22%) Frame 23 (23%) Frame 24 (24%) Frame 25 (25%) Frame 26 (26%) Frame 27 (27%) Frame 28 (28%) Frame 29 (29%) Frame 30 (30%) Frame 31 (31%) Frame 32 (32%) Frame 33 (33%) Frame 34 (34%) Frame 35 (35%) Frame 36 (36%) Frame 37 (37%) Frame 38 (38%) Frame 39 (39%) Frame 40 (40%) Frame 41 (41%) Frame 42 (42%) Frame 43 (43%) Frame 44 (44%) Frame 45 (45%) Frame 46 (46%) Frame 47 (47%) Frame 48 (48%) Frame 49 (49%) Frame 50 (50%) Frame 51 (51%) Frame 52 (52%) Frame 53 (53%) Frame 54 (54%) Frame 55 (55%) Frame 56 (56%) Frame 57 (57%) Frame 58 (58%) Frame 59 (59%) Frame 60 (60%) Frame 61 (61%) Frame 62 (62%) Frame 63 (63%) Frame 64 (64%) Frame 65 (65%) Frame 66 (66%) Frame 67 (67%) Frame 68 (68%) Frame 69 (69%) Frame 70 (70%) Frame 71 (71%) Frame 72 (72%) Frame 73 (73%) Frame 74 (74%) Frame 75 (75%) Frame 76 (76%) Frame 77 (77%) Frame 78 (78%) Frame 79 (79%) Frame 80 (80%) Frame 81 (81%) Frame 82 (82%) Frame 83 (83%) Frame 84 (84%) Frame 85 (85%) Frame 86 (86%) Frame 87 (87%) Frame 88 (88%) Frame 89 (89%) Frame 90 (90%) Frame 91 (91%) Frame 92 (92%) Frame 93 (93%) Frame 94 (94%) Frame 95 (95%) Frame 96 (96%) Frame 97 (97%) Frame 98 (98%) Frame 99 (99%) Frame 100 (100%) Finalizing encoding... done! gganimate::anim_save(filename = &quot;temperature_animation.gif&quot;, animation = temperature) Frame 1 (1%) Frame 2 (2%) Frame 3 (3%) Frame 4 (4%) Frame 5 (5%) Frame 6 (6%) Frame 7 (7%) Frame 8 (8%) Frame 9 (9%) Frame 10 (10%) Frame 11 (11%) Frame 12 (12%) Frame 13 (13%) Frame 14 (14%) Frame 15 (15%) Frame 16 (16%) Frame 17 (17%) Frame 18 (18%) Frame 19 (19%) Frame 20 (20%) Frame 21 (21%) Frame 22 (22%) Frame 23 (23%) Frame 24 (24%) Frame 25 (25%) Frame 26 (26%) Frame 27 (27%) Frame 28 (28%) Frame 29 (29%) Frame 30 (30%) Frame 31 (31%) Frame 32 (32%) Frame 33 (33%) Frame 34 (34%) Frame 35 (35%) Frame 36 (36%) Frame 37 (37%) Frame 38 (38%) Frame 39 (39%) Frame 40 (40%) Frame 41 (41%) Frame 42 (42%) Frame 43 (43%) Frame 44 (44%) Frame 45 (45%) Frame 46 (46%) Frame 47 (47%) Frame 48 (48%) Frame 49 (49%) Frame 50 (50%) Frame 51 (51%) Frame 52 (52%) Frame 53 (53%) Frame 54 (54%) Frame 55 (55%) Frame 56 (56%) Frame 57 (57%) Frame 58 (58%) Frame 59 (59%) Frame 60 (60%) Frame 61 (61%) Frame 62 (62%) Frame 63 (63%) Frame 64 (64%) Frame 65 (65%) Frame 66 (66%) Frame 67 (67%) Frame 68 (68%) Frame 69 (69%) Frame 70 (70%) Frame 71 (71%) Frame 72 (72%) Frame 73 (73%) Frame 74 (74%) Frame 75 (75%) Frame 76 (76%) Frame 77 (77%) Frame 78 (78%) Frame 79 (79%) Frame 80 (80%) Frame 81 (81%) Frame 82 (82%) Frame 83 (83%) Frame 84 (84%) Frame 85 (85%) Frame 86 (86%) Frame 87 (87%) Frame 88 (88%) Frame 89 (89%) Frame 90 (90%) Frame 91 (91%) Frame 92 (92%) Frame 93 (93%) Frame 94 (94%) Frame 95 (95%) Frame 96 (96%) Frame 97 (97%) Frame 98 (98%) Frame 99 (99%) Frame 100 (100%) Finalizing encoding... done! fluorescence = ggplot()+ geom_raster(data = algoa %&gt;% filter(variable == &quot;fluorescence&quot;) %&gt;% mutate(pressure = pressure %&gt;% as.factor()), aes(x = lon, y = lat, fill = value))+ scale_fill_gradientn(colours = oce::oceColors9A(120))+ scale_y_continuous(breaks = seq(-8.5,-6,2.5), labels = metR::LatLabel(seq(-8.5,-6,2.5)))+ scale_x_continuous(breaks = seq(39.55,40.25,length.out = 3) %&gt;% round(2), labels = metR::LonLabel(seq(39.55,40.25,length.out = 3)))+ labs(x = NULL, y = NULL, title = &#39;fluorescence at depth of {closest_state} meters&#39;)+ theme_bw()+ theme(axis.text = element_text(size = 16, colour = &quot;black&quot;), title = element_text(size = 16))+ coord_cartesian(expand = FALSE)+ guides(fill = guide_colorbar(title =expression(fluorescence~concentration~(mgm^{-3})), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 14), label.theme = element_text(size = 14), barheight = 15, barwidth = .95))+ gganimate::transition_states(pressure, transition_length = 1, state_length = 2)+ gganimate::view_step()+ gganimate::ease_aes(&quot;cubic-in-out&quot;) # Slow start and end for a smoother look fluorescence Frame 1 (1%) Frame 2 (2%) Frame 3 (3%) Frame 4 (4%) Frame 5 (5%) Frame 6 (6%) Frame 7 (7%) Frame 8 (8%) Frame 9 (9%) Frame 10 (10%) Frame 11 (11%) Frame 12 (12%) Frame 13 (13%) Frame 14 (14%) Frame 15 (15%) Frame 16 (16%) Frame 17 (17%) Frame 18 (18%) Frame 19 (19%) Frame 20 (20%) Frame 21 (21%) Frame 22 (22%) Frame 23 (23%) Frame 24 (24%) Frame 25 (25%) Frame 26 (26%) Frame 27 (27%) Frame 28 (28%) Frame 29 (29%) Frame 30 (30%) Frame 31 (31%) Frame 32 (32%) Frame 33 (33%) Frame 34 (34%) Frame 35 (35%) Frame 36 (36%) Frame 37 (37%) Frame 38 (38%) Frame 39 (39%) Frame 40 (40%) Frame 41 (41%) Frame 42 (42%) Frame 43 (43%) Frame 44 (44%) Frame 45 (45%) Frame 46 (46%) Frame 47 (47%) Frame 48 (48%) Frame 49 (49%) Frame 50 (50%) Frame 51 (51%) Frame 52 (52%) Frame 53 (53%) Frame 54 (54%) Frame 55 (55%) Frame 56 (56%) Frame 57 (57%) Frame 58 (58%) Frame 59 (59%) Frame 60 (60%) Frame 61 (61%) Frame 62 (62%) Frame 63 (63%) Frame 64 (64%) Frame 65 (65%) Frame 66 (66%) Frame 67 (67%) Frame 68 (68%) Frame 69 (69%) Frame 70 (70%) Frame 71 (71%) Frame 72 (72%) Frame 73 (73%) Frame 74 (74%) Frame 75 (75%) Frame 76 (76%) Frame 77 (77%) Frame 78 (78%) Frame 79 (79%) Frame 80 (80%) Frame 81 (81%) Frame 82 (82%) Frame 83 (83%) Frame 84 (84%) Frame 85 (85%) Frame 86 (86%) Frame 87 (87%) Frame 88 (88%) Frame 89 (89%) Frame 90 (90%) Frame 91 (91%) Frame 92 (92%) Frame 93 (93%) Frame 94 (94%) Frame 95 (95%) Frame 96 (96%) Frame 97 (97%) Frame 98 (98%) Frame 99 (99%) Frame 100 (100%) Finalizing encoding... done! gganimate::anim_save(filename = &quot;fluorescence_animation.gif&quot;, animation = fluorescence) Frame 1 (1%) Frame 2 (2%) Frame 3 (3%) Frame 4 (4%) Frame 5 (5%) Frame 6 (6%) Frame 7 (7%) Frame 8 (8%) Frame 9 (9%) Frame 10 (10%) Frame 11 (11%) Frame 12 (12%) Frame 13 (13%) Frame 14 (14%) Frame 15 (15%) Frame 16 (16%) Frame 17 (17%) Frame 18 (18%) Frame 19 (19%) Frame 20 (20%) Frame 21 (21%) Frame 22 (22%) Frame 23 (23%) Frame 24 (24%) Frame 25 (25%) Frame 26 (26%) Frame 27 (27%) Frame 28 (28%) Frame 29 (29%) Frame 30 (30%) Frame 31 (31%) Frame 32 (32%) Frame 33 (33%) Frame 34 (34%) Frame 35 (35%) Frame 36 (36%) Frame 37 (37%) Frame 38 (38%) Frame 39 (39%) Frame 40 (40%) Frame 41 (41%) Frame 42 (42%) Frame 43 (43%) Frame 44 (44%) Frame 45 (45%) Frame 46 (46%) Frame 47 (47%) Frame 48 (48%) Frame 49 (49%) Frame 50 (50%) Frame 51 (51%) Frame 52 (52%) Frame 53 (53%) Frame 54 (54%) Frame 55 (55%) Frame 56 (56%) Frame 57 (57%) Frame 58 (58%) Frame 59 (59%) Frame 60 (60%) Frame 61 (61%) Frame 62 (62%) Frame 63 (63%) Frame 64 (64%) Frame 65 (65%) Frame 66 (66%) Frame 67 (67%) Frame 68 (68%) Frame 69 (69%) Frame 70 (70%) Frame 71 (71%) Frame 72 (72%) Frame 73 (73%) Frame 74 (74%) Frame 75 (75%) Frame 76 (76%) Frame 77 (77%) Frame 78 (78%) Frame 79 (79%) Frame 80 (80%) Frame 81 (81%) Frame 82 (82%) Frame 83 (83%) Frame 84 (84%) Frame 85 (85%) Frame 86 (86%) Frame 87 (87%) Frame 88 (88%) Frame 89 (89%) Frame 90 (90%) Frame 91 (91%) Frame 92 (92%) Frame 93 (93%) Frame 94 (94%) Frame 95 (95%) Frame 96 (96%) Frame 97 (97%) Frame 98 (98%) Frame 99 (99%) Frame 100 (100%) Finalizing encoding... done! ggplot(data = ctd %&gt;% filter(pressure ==5), aes(x = lon, y = lat)) + geom_point()+ metR::scale_x_longitude(ticks = 0.5)+ metR::scale_y_latitude(ticks = 1) transect1 = ctd %&gt;% filter(lat &gt; -6&amp; pressure &lt; 205)%&gt;%select(lon,lat, pressure, temperature, salinity, oxygen, fluorescence) %&gt;% mutate(transect = &quot;transect1&quot;) transect2 = ctd %&gt;% filter( lat &gt;= -8 &amp;lat &lt; -6 &amp; pressure &lt; 205)%&gt;%select(lon,lat, pressure, temperature, salinity, oxygen, fluorescence) %&gt;% mutate(transect = &quot;transect2&quot;) transect3 = ctd %&gt;% filter( lat &gt;= -10 &amp;lat &lt; -8 &amp; pressure &lt; 205)%&gt;%select(lon,lat, pressure, temperature, salinity, oxygen, fluorescence) %&gt;% mutate(transect = &quot;transect3&quot;) transect4 = ctd %&gt;% filter(lat &lt; -10 &amp; pressure &lt; 205)%&gt;%select(lon,lat, pressure, temperature, salinity, oxygen, fluorescence) %&gt;% mutate(transect = &quot;transect4&quot;) 17.3 checking for proper interpolation methods The figure below comes from the metR::geom_contour_fill() with an argument na.fill = TRUE to impute missing values transects = transect1 %&gt;% bind_rows(transect2, transect3, transect4) %&gt;% tidyr::gather(key = &quot;variable&quot;, value = &quot;value&quot;, 4:7) ggplot(data = transects %&gt;% filter(transect == &quot;transect4&quot; &amp; variable == &quot;fluorescence&quot;),aes(x = lon, y = pressure, z = value))+ metR::geom_contour_fill(na.fill = TRUE, bins = 30)+ # geom_contour_tanaka()+ metR::scale_x_longitude(ticks = .15)+ scale_y_reverse()+ scale_fill_gradientn(colors = oce::oce.colors9A(120))+ coord_cartesian(expand = FALSE) The figure below comes from the metR::geom_contour_fill() with an argument na.fill = FALSE not to impute missing values as they were interpolated with oce::interpBarnes() function. It look very different from the above and the spatial pattern does not come well. So, we need to rethink about oce::interpBarnes() algorithm for interpolating the missing values. transects %&gt;% filter(transect == &quot;transect4&quot; &amp; variable == &quot;fluorescence&quot;) %$% oce::interpBarnes(x = lon, y = pressure, z = value) %$% matrix_tb(x = xg, y = yg, data = zg) %&gt;% ggplot()+ # geom_raster(aes(x = x, y = y, fill = value), interpolate = FALSE)+ metR::geom_contour_fill(aes(x = x, y = y, z = value), na.fill = FALSE, bins = 30)+ # geom_contour_tanaka(aes(x = x, y = y, z = value))+ scale_y_reverse()+ metR::scale_x_longitude(ticks = .25)+ scale_fill_gradientn(colors = oce::oce.colors9A(120))+ coord_cartesian(expand = FALSE) The figure below comes from the metR::geom_contour_fill() with an argument na.fill = FALSE not to impute missing values as they were interpolated with metR::Interpolate() function. It look very similar from the before above and the spatial pattern does come well. So, we I think of quiting using oce::interpBarnes() algorithm and instead use metR::Interpolate() for interpolating the missing values before plotting with metR::geom_contour_fill(). transects %&gt;% filter(transect == &quot;transect4&quot; &amp; variable == &quot;fluorescence&quot;) %$% metR::Interpolate(formula = value ~ lon+pressure, x.out = seq(min(lon), max(lon), length.out = 100), y.out = seq(0,200,5)) %&gt;% ggplot() + # geom_raster(aes(x = lon, y = pressure, fill = value), interpolate = FALSE)+ metR::geom_contour_fill(aes(x = lon, y = pressure, z = value),na.fill = TRUE, bins = 30)+ # geom_contour_tanaka(aes(x = x, y = y, z = value))+ scale_y_reverse()+ metR::scale_x_longitude(ticks = .25)+ scale_fill_gradientn(colors = oce::oce.colors9A(120))+ coord_cartesian(expand = FALSE) I created a light version of metR::Interpolate() called interpolate2() with three arguments: x, y, and z and then you specify how many spacing you want the function to do for you for gridding. The figure below was mapped using the output from interpolate2() function and it resembles with the above figure that used the ouput from metR::Interpolate() function. transects %&gt;% filter(transect == &quot;transect4&quot; &amp; variable == &quot;fluorescence&quot;) %$% interpolate2(x = lon, y = pressure, z = value, n = 100) %&gt;% ggplot() + metR::geom_contour_fill(aes(x = x, y = y, z = z),na.fill = TRUE, bins = 30)+ scale_y_reverse()+ metR::scale_x_longitude(ticks = .25)+ scale_fill_gradientn(colors = oce::oce.colors9A(120))+ coord_cartesian(expand = FALSE) What about akima transects %&gt;% filter(transect == &quot;transect4&quot; &amp; variable == &quot;fluorescence&quot;)%&gt;% na.omit() %$% akima::interp(x = lon, y = pressure, z = value, nx = 50, ny = 50) %&gt;% akima::interp2xyz() %&gt;% as_tibble() %&gt;% ggplot() + metR::geom_contour_fill(aes(x = x, y = y, z = z),na.fill = TRUE, bins = 30)+ scale_y_reverse()+ metR::scale_x_longitude(ticks = .25)+ scale_fill_gradientn(colors = oce::oce.colors9A(120))+ coord_cartesian(expand = FALSE) ## get unique transects names sects = transects%&gt;% distinct(transect) %&gt;% pull() ## get unique variable names var = transects%&gt;% distinct(variable) %&gt;% pull() ## make and label Latitude Lat.label = transects %&gt;% group_by(transect) %&gt;% summarise(Lat = median(lat) %&gt;% round(4) %&gt;% metR::LatLabel()) ## create a dummy object test.tb = NULL ## iterate with for loop for (i in seq_along(sects)){ #index for transects for (j in seq_along(var)){ #index for variable # extract the variable of interest and assign a name data data = transects %&gt;% filter(transect == sects[i] &amp; variable == var[j]) # obtain the range of longitude at each transect and variable lon.range = data %$% range(lon) # fill the missing value with the interpolate function from metR test = data %&gt;% na.omit() %$% ## uncheck the comment below if you want to use this function # metR::Interpolate(formula = value ~ lon+pressure, # x.out = seq(from = lon.range[1], # to = lon.range[2], # length.out = 20), # y.out = seq(from = 5, to = 200, by = 5)) %&gt;% ## check the three lines below of akima if you want to use the checked function above akima::interp(x = lon, y = pressure, z = value) %&gt;% akima::interp2xyz() %&gt;% as_tibble() %&gt;% rename(lon = x, pressure = y, value = z)%&gt;% mutate(transect = sects[i], variable = var[j], Lat= Lat.label$Lat[j]) # The data frame of estimated variable at a particular section is binded rowise test.tb = test.tb %&gt;% bind_rows(test) } } test.tb$transect = as.factor(test.tb$transect) test.tb$variable = as.factor(test.tb$variable) test.tb$Lat = as.factor(test.tb$Lat) # ## evaluate unreliable minimum value to replace with NA # test.tb %&gt;% filter(value &gt; 0) %&gt;% group_by(transect, variable) %&gt;% summarise(weka = min(value, na.rm = T)) %&gt;% arrange(variable) ## replace the unreliable value identified with the NA test.tb = test.tb %&gt;% filter(variable == &quot;temperature&quot;) %&gt;% mutate(value = replace(value,value &lt; 14, NA)) %&gt;% bind_rows(test.tb %&gt;% filter(variable == &quot;fluorescence&quot;) %&gt;% mutate(value = replace(value,value &lt; 0.064, NA)), test.tb %&gt;% filter(variable == &quot;salinity&quot;) %&gt;% mutate(value = replace(value,value &lt; 34.7, NA)), test.tb %&gt;% filter(variable == &quot;oxygen&quot;) %&gt;% mutate(value = replace(value,value &lt; 2, NA))) 17.3.1 Fluorescence section ggplot(data = test.tb %&gt;% filter(variable == &quot;fluorescence&quot;), #&amp; transect == &quot;transect4&quot; aes(x = lon, y = pressure, z = value)) + metR::geom_contour_fill(na.fill = TRUE, bins = 20)+ # not supported with gganimate, use geom_raster() instead # geom_raster() + metR::scale_x_longitude(ticks = .3)+ scale_y_reverse(breaks = seq(40,190,40))+ coord_cartesian(expand = FALSE)+ scale_fill_gradientn(colours = oce::oceColors9A(120), na.value = &quot;white&quot;)+ # theme_bw()+ theme(axis.text = element_text(size = 12, colour = &quot;black&quot;), title = element_text(size = 12))+ guides(fill = guide_colorbar(title =expression(fluorescence~concentration~(mgm^{-3})), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 12), label.theme = element_text(size = 11), barheight = 15, barwidth = 1.1))+ facet_wrap(~transect, scales = &quot;free_x&quot;) fluorescence.section = ggplot(data = test.tb %&gt;% filter(variable == &quot;fluorescence&quot; ), #&amp; transect == &quot;transect4&quot; aes(x = lon, y = pressure, fill = value)) + # metR::geom_contour_fill(na.fill = TRUE)+ # not supported with gganimate, use geom_raster() instead geom_raster() + metR::scale_x_longitude(ticks = .2)+ scale_y_reverse(breaks = seq(40,190,40))+ coord_cartesian(expand = FALSE)+ scale_fill_gradientn(colours = oce::oceColors9A(120), na.value = &quot;white&quot;)+ # theme_bw()+ theme(axis.text = element_text(size = 16, colour = &quot;black&quot;), title = element_text(size = 16))+ guides(fill = guide_colorbar(title =expression(fluorescence~concentration~(mgm^{-3})), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 16), label.theme = element_text(size = 15), barheight = 20, barwidth = 1.5))+ labs(x = NULL, y = &quot;Pressure [m]&quot;, title = &#39;Section along {closest_state}&#39;)+ gganimate::transition_states(transect, transition_length = 1, state_length = 2)+ # gganimate::view_step()+ gganimate::view_follow(fixed_y = TRUE, fixed_x = FALSE)+ gganimate::ease_aes(&quot;cubic-in-out&quot;) # Slow start and end for a smoother look gganimate::animate(fluorescence.section, nframes = 10, fps = 1) Frame 1 (10%) Frame 2 (20%) Frame 3 (30%) Frame 4 (40%) Frame 5 (50%) Frame 6 (60%) Frame 7 (70%) Frame 8 (80%) Frame 9 (90%) Frame 10 (100%) Finalizing encoding... done! # gganimate::anim_save(filename = &quot;fluorescenceSection_animation.gif&quot;, animation = fluorescence.section) 17.3.2 Temperature section ggplot(data = test.tb %&gt;% filter(variable == &quot;temperature&quot;), #&amp; transect == &quot;transect4&quot; aes(x = lon, y = pressure, z = value)) + metR::geom_contour_fill(na.fill = TRUE, bins = 30)+ # not supported with gganimate, use geom_raster() instead # geom_raster() + metR::scale_x_longitude(ticks = .3)+ scale_y_reverse(breaks = seq(40,190,40))+ coord_cartesian(expand = FALSE)+ scale_fill_gradientn(colours = oce::oceColors9A(120), na.value = &quot;white&quot;)+ # theme_bw()+ theme(axis.text = element_text(size = 12, colour = &quot;black&quot;), title = element_text(size = 12))+ guides(fill = guide_colorbar(title =expression(fluorescence~concentration~(mgm^{-3})), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 12), label.theme = element_text(size = 11), barheight = 15, barwidth = 1.1))+ labs(x = NULL, y = &quot;Pressure [m]&quot;)+ facet_wrap(~transect, scales = &quot;free_x&quot;) temperature.section = ggplot(data = test.tb %&gt;% filter(variable == &quot;temperature&quot;), #&amp; transect == &quot;transect4&quot; aes(x = lon, y = pressure, fill = value)) + # metR::geom_contour_fill(na.fill = TRUE)+ # not supported with gganimate, use geom_raster() instead geom_raster() + metR::scale_x_longitude(ticks = .2)+ scale_y_reverse(breaks = seq(40,190,40))+ coord_cartesian(expand = FALSE)+ scale_fill_gradientn(colours = oce::oceColors9A(120), na.value = &quot;white&quot;)+ # theme_bw()+ theme(axis.text = element_text(size = 16, colour = &quot;black&quot;), title = element_text(size = 16))+ guides(fill = guide_colorbar(title =expression(fluorescence~concentration~(mgm^{-3})), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 16), label.theme = element_text(size = 15), barheight = 20, barwidth = 1.5))+ labs(x = NULL, y = &quot;Pressure [m]&quot;, title = &#39;Section along {closest_state}&#39;)+ gganimate::transition_states(transect, transition_length = 1, state_length = 2)+ # gganimate::view_step()+ gganimate::view_follow(fixed_y = TRUE, fixed_x = FALSE)+ gganimate::ease_aes(&quot;cubic-in-out&quot;) # Slow start and end for a smoother look temperature.section Frame 1 (1%) Frame 2 (2%) Frame 3 (3%) Frame 4 (4%) Frame 5 (5%) Frame 6 (6%) Frame 7 (7%) Frame 8 (8%) Frame 9 (9%) Frame 10 (10%) Frame 11 (11%) Frame 12 (12%) Frame 13 (13%) Frame 14 (14%) Frame 15 (15%) Frame 16 (16%) Frame 17 (17%) Frame 18 (18%) Frame 19 (19%) Frame 20 (20%) Frame 21 (21%) Frame 22 (22%) Frame 23 (23%) Frame 24 (24%) Frame 25 (25%) Frame 26 (26%) Frame 27 (27%) Frame 28 (28%) Frame 29 (29%) Frame 30 (30%) Frame 31 (31%) Frame 32 (32%) Frame 33 (33%) Frame 34 (34%) Frame 35 (35%) Frame 36 (36%) Frame 37 (37%) Frame 38 (38%) Frame 39 (39%) Frame 40 (40%) Frame 41 (41%) Frame 42 (42%) Frame 43 (43%) Frame 44 (44%) Frame 45 (45%) Frame 46 (46%) Frame 47 (47%) Frame 48 (48%) Frame 49 (49%) Frame 50 (50%) Frame 51 (51%) Frame 52 (52%) Frame 53 (53%) Frame 54 (54%) Frame 55 (55%) Frame 56 (56%) Frame 57 (57%) Frame 58 (58%) Frame 59 (59%) Frame 60 (60%) Frame 61 (61%) Frame 62 (62%) Frame 63 (63%) Frame 64 (64%) Frame 65 (65%) Frame 66 (66%) Frame 67 (67%) Frame 68 (68%) Frame 69 (69%) Frame 70 (70%) Frame 71 (71%) Frame 72 (72%) Frame 73 (73%) Frame 74 (74%) Frame 75 (75%) Frame 76 (76%) Frame 77 (77%) Frame 78 (78%) Frame 79 (79%) Frame 80 (80%) Frame 81 (81%) Frame 82 (82%) Frame 83 (83%) Frame 84 (84%) Frame 85 (85%) Frame 86 (86%) Frame 87 (87%) Frame 88 (88%) Frame 89 (89%) Frame 90 (90%) Frame 91 (91%) Frame 92 (92%) Frame 93 (93%) Frame 94 (94%) Frame 95 (95%) Frame 96 (96%) Frame 97 (97%) Frame 98 (98%) Frame 99 (99%) Frame 100 (100%) Finalizing encoding... done! # gganimate::anim_save(filename = &quot;temperatureSection_animation.gif&quot;, animation = temperature.section) 17.3.3 Dissolved Oxygen section ggplot(data = test.tb %&gt;% filter(variable == &quot;oxygen&quot; ), #&amp; transect == &quot;transect4&quot; aes(x = lon, y = pressure, z = value)) + metR::geom_contour_fill(na.fill = TRUE, bins = 30)+ # not supported with gganimate, use geom_raster() instead # geom_raster() + metR::scale_x_longitude(ticks = .3)+ scale_y_reverse(breaks = seq(40,190,40))+ coord_cartesian(expand = FALSE)+ scale_fill_gradientn(colours = oce::oceColors9A(120), na.value = &quot;white&quot;)+ # theme_bw()+ theme(axis.text = element_text(size = 12, colour = &quot;black&quot;), title = element_text(size = 12))+ guides(fill = guide_colorbar(title =expression(Dissolved~Oxygen~(mgL)), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 12), label.theme = element_text(size = 11), barheight = 12, barwidth = 1.1))+ labs(x = NULL, y = &quot;Pressure [m]&quot;)+ facet_wrap(~transect, scales = &quot;free_x&quot;) oxygen.section = ggplot(data = test.tb %&gt;% filter(variable == &quot;oxygen&quot; ), #&amp; transect == &quot;transect4&quot; aes(x = lon, y = pressure, fill = value)) + # metR::geom_contour_fill(na.fill = TRUE)+ # not supported with gganimate, use geom_raster() instead geom_raster() + metR::scale_x_longitude(ticks = .2)+ scale_y_reverse(breaks = seq(40,190,40))+ coord_cartesian(expand = FALSE)+ scale_fill_gradientn(colours = oce::oceColors9A(120), na.value = &quot;white&quot;)+ # theme_bw()+ theme(axis.text = element_text(size = 16, colour = &quot;black&quot;), title = element_text(size = 16))+ guides(fill = guide_colorbar(title =expression(Dissolved~Oxygen~(mgL)), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 16), label.theme = element_text(size = 15), barheight = 20, barwidth = 1.5))+ labs(x = NULL, y = &quot;Pressure [m]&quot;, title = &#39;Section along {closest_state}&#39;)+ gganimate::transition_states(transect, transition_length = 1, state_length = 2)+ # gganimate::view_step()+ gganimate::view_follow(fixed_y = TRUE, fixed_x = FALSE)+ gganimate::ease_aes(&quot;cubic-in-out&quot;) # Slow start and end for a smoother look oxygen.section Frame 1 (1%) Frame 2 (2%) Frame 3 (3%) Frame 4 (4%) Frame 5 (5%) Frame 6 (6%) Frame 7 (7%) Frame 8 (8%) Frame 9 (9%) Frame 10 (10%) Frame 11 (11%) Frame 12 (12%) Frame 13 (13%) Frame 14 (14%) Frame 15 (15%) Frame 16 (16%) Frame 17 (17%) Frame 18 (18%) Frame 19 (19%) Frame 20 (20%) Frame 21 (21%) Frame 22 (22%) Frame 23 (23%) Frame 24 (24%) Frame 25 (25%) Frame 26 (26%) Frame 27 (27%) Frame 28 (28%) Frame 29 (29%) Frame 30 (30%) Frame 31 (31%) Frame 32 (32%) Frame 33 (33%) Frame 34 (34%) Frame 35 (35%) Frame 36 (36%) Frame 37 (37%) Frame 38 (38%) Frame 39 (39%) Frame 40 (40%) Frame 41 (41%) Frame 42 (42%) Frame 43 (43%) Frame 44 (44%) Frame 45 (45%) Frame 46 (46%) Frame 47 (47%) Frame 48 (48%) Frame 49 (49%) Frame 50 (50%) Frame 51 (51%) Frame 52 (52%) Frame 53 (53%) Frame 54 (54%) Frame 55 (55%) Frame 56 (56%) Frame 57 (57%) Frame 58 (58%) Frame 59 (59%) Frame 60 (60%) Frame 61 (61%) Frame 62 (62%) Frame 63 (63%) Frame 64 (64%) Frame 65 (65%) Frame 66 (66%) Frame 67 (67%) Frame 68 (68%) Frame 69 (69%) Frame 70 (70%) Frame 71 (71%) Frame 72 (72%) Frame 73 (73%) Frame 74 (74%) Frame 75 (75%) Frame 76 (76%) Frame 77 (77%) Frame 78 (78%) Frame 79 (79%) Frame 80 (80%) Frame 81 (81%) Frame 82 (82%) Frame 83 (83%) Frame 84 (84%) Frame 85 (85%) Frame 86 (86%) Frame 87 (87%) Frame 88 (88%) Frame 89 (89%) Frame 90 (90%) Frame 91 (91%) Frame 92 (92%) Frame 93 (93%) Frame 94 (94%) Frame 95 (95%) Frame 96 (96%) Frame 97 (97%) Frame 98 (98%) Frame 99 (99%) Frame 100 (100%) Finalizing encoding... done! # gganimate::anim_save(filename = &quot;oxygenSection_animation.gif&quot;, animation = oxygen.section) 17.3.4 Salinity section ggplot(data = test.tb %&gt;% filter(variable == &quot;salinity&quot; &amp; value &gt; 34.7), #&amp; transect == &quot;transect4&quot; aes(x = lon, y = pressure, z = value)) + metR::geom_contour_fill(na.fill = TRUE, bins = 30)+ # not supported with gganimate, use geom_raster() instead # geom_raster() + metR::scale_x_longitude(ticks = .2)+ scale_y_reverse(breaks = seq(40,190,40))+ coord_cartesian(expand = FALSE)+ scale_fill_gradientn(colours = oce::oceColors9A(120), na.value = &quot;white&quot;)+ # theme_bw()+ theme(axis.text = element_text(size = 12, colour = &quot;black&quot;), title = element_text(size = 12))+ guides(fill = guide_colorbar(title =expression(Salinity), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 16), label.theme = element_text(size = 12), barheight = 12, barwidth = 1.1))+ facet_wrap(~transect, scales = &quot;free_x&quot;) salinity.section = ggplot(data = test.tb %&gt;% filter(variable == &quot;salinity&quot;), #&amp; transect == &quot;transect4&quot; aes(x = lon, y = pressure, fill = value)) + # metR::geom_contour_fill(na.fill = TRUE)+ # not supported with gganimate, use geom_raster() instead geom_raster() + metR::scale_x_longitude(ticks = .2)+ scale_y_reverse(breaks = seq(40,190,40))+ coord_cartesian(expand = FALSE)+ scale_fill_gradientn(colours = oce::oceColors9A(120), na.value = &quot;white&quot;)+ # theme_bw()+ theme(axis.text = element_text(size = 16, colour = &quot;black&quot;), title = element_text(size = 16))+ guides(fill = guide_colorbar(title =expression(Salinity), title.position = &quot;right&quot;, title.hjust = .5, raster = FALSE, title.theme = element_text(angle = 90, size = 16), label.theme = element_text(size = 15), barheight = 20, barwidth = 1.5))+ labs(x = NULL, y = &quot;Pressure [m]&quot;, title = &#39;Section along {closest_state}&#39;)+ gganimate::transition_states(transect, transition_length = 1, state_length = 2)+ # gganimate::view_step()+ gganimate::view_follow(fixed_y = TRUE, fixed_x = FALSE)+ gganimate::ease_aes(&quot;cubic-in-out&quot;) # Slow start and end for a smoother look salinity.section Frame 1 (1%) Frame 2 (2%) Frame 3 (3%) Frame 4 (4%) Frame 5 (5%) Frame 6 (6%) Frame 7 (7%) Frame 8 (8%) Frame 9 (9%) Frame 10 (10%) Frame 11 (11%) Frame 12 (12%) Frame 13 (13%) Frame 14 (14%) Frame 15 (15%) Frame 16 (16%) Frame 17 (17%) Frame 18 (18%) Frame 19 (19%) Frame 20 (20%) Frame 21 (21%) Frame 22 (22%) Frame 23 (23%) Frame 24 (24%) Frame 25 (25%) Frame 26 (26%) Frame 27 (27%) Frame 28 (28%) Frame 29 (29%) Frame 30 (30%) Frame 31 (31%) Frame 32 (32%) Frame 33 (33%) Frame 34 (34%) Frame 35 (35%) Frame 36 (36%) Frame 37 (37%) Frame 38 (38%) Frame 39 (39%) Frame 40 (40%) Frame 41 (41%) Frame 42 (42%) Frame 43 (43%) Frame 44 (44%) Frame 45 (45%) Frame 46 (46%) Frame 47 (47%) Frame 48 (48%) Frame 49 (49%) Frame 50 (50%) Frame 51 (51%) Frame 52 (52%) Frame 53 (53%) Frame 54 (54%) Frame 55 (55%) Frame 56 (56%) Frame 57 (57%) Frame 58 (58%) Frame 59 (59%) Frame 60 (60%) Frame 61 (61%) Frame 62 (62%) Frame 63 (63%) Frame 64 (64%) Frame 65 (65%) Frame 66 (66%) Frame 67 (67%) Frame 68 (68%) Frame 69 (69%) Frame 70 (70%) Frame 71 (71%) Frame 72 (72%) Frame 73 (73%) Frame 74 (74%) Frame 75 (75%) Frame 76 (76%) Frame 77 (77%) Frame 78 (78%) Frame 79 (79%) Frame 80 (80%) Frame 81 (81%) Frame 82 (82%) Frame 83 (83%) Frame 84 (84%) Frame 85 (85%) Frame 86 (86%) Frame 87 (87%) Frame 88 (88%) Frame 89 (89%) Frame 90 (90%) Frame 91 (91%) Frame 92 (92%) Frame 93 (93%) Frame 94 (94%) Frame 95 (95%) Frame 96 (96%) Frame 97 (97%) Frame 98 (98%) Frame 99 (99%) Frame 100 (100%) Finalizing encoding... done! # gganimate::anim_save(filename = &quot;salinitySection_animation.gif&quot;, animation = salinity.section) "],
["forecast-time-series.html", "Chapter 18 Forecast time series 18.1 introduction 18.2 Data 18.3 Data processing 18.4 Forecasting 18.5 Visualize the forecasted values 18.6 conclusion", " Chapter 18 Forecast time series 18.1 introduction Time series analysis comprises methods for predicting the future based on the historical in order to extract meaningful statistics and other characteristics of the data. In other words, time series forecasting is the use of a model to predict future values based on previously observed values. Time series are widely used for non-stationary data, like economic, weather, stock price, and retail sales in this post. Scientists all over the globe are also working to predict the future increase of environmenal variables like surface current, wind speed and direction, sea surface temperature and marine productivity. This chapter demonstrate how to predict chlorophyll concentration in the Pemba channel using prophet package developed by Sean Taylor and Ben Letham (2019). Let’s get started by loading the package we need to import the data, analyse and plotting: require(tidyverse) require(lubridate) require(magrittr) require(prophet) 18.2 Data We use a time series of chlorophyll concentration at the Pemba Channel, Tanzania. We extracted these data using the xtracto_3D() function from xtractomatic package (Mendelssohn 2018). The .csv format of this file is available here. First we import the data into our workspace: chl.tb = read_csv(&quot;./chl_tb.csv&quot;) The file contain four variables described as: date the month of records longitude the eastings geographical coordinates values latitude the northings geographical cooridnates values chlorophyll the value of chlorophyll measured at that location and time Table 18.1: Randomly selected twelve observations of thedataset Geographical coordinates Date Longitude Latitude Chlorophyll (mg/m3) 2005-10-16 40.10 -5.65 0.14 2005-05-16 38.90 -5.48 NaN 2011-12-16 39.19 -5.60 0.27 2017-11-16 39.27 -5.56 0.20 2018-03-16 39.73 -5.65 0.13 2004-12-16 39.98 -5.44 0.08 2007-03-16 39.19 -5.65 0.37 2004-05-16 39.35 -5.65 NaN 2017-05-16 39.77 -5.65 0.17 2017-08-16 39.94 -5.44 0.20 18.3 Data processing This step includes removing longitude and latitude value from the dataset and aggregate chlorophyll-value by date with function from tidyverse (Wickham 2017). The average chlorophyll concentration was computed and rename the variables to ds for date and date time and y for a variable containing the values to match the data frame format that is compartible with the prophet package (Taylor and Letham 2019). chl.mean = chl.tb %&gt;% group_by(date) %&gt;% summarise(chl = mean(chl, na.rm = TRUE)) %&gt;% ungroup() %&gt;% rename(ds = date, y = chl) 18.4 Forecasting Once we have organized the data frame in the format that prophet package understand, We can fit the data frame of into the model. m = prophet(df = chl.mean, weekly.seasonality=TRUE, daily.seasonality=TRUE) We then used the fitted model to create a data frame of the future. For this case we want to predict chlorophyll value for the next ten years, therefore we specify the number of periods to 120 and the frequency to month. The make_future_dataframe() function takes the model object and a number of periods to forecast and produce a suitable dataframe, but also include the historical dates. future = make_future_dataframe(m = m, periods = 120, freq = &quot;month&quot;) now you have created a future time and have the historical trend, we can predict the value of chlorophyll for pemba channel for the coming years with the predict() function. It output future.chl data frame object with column yhat containing the predicted values (table 18.2. It has other columns for uncertainity and seasonal components. future.chl = stats::predict(m, future) Table 18.2: Randomly selected twelve observations of the predicted chlorophyll dataset Historical value Predicted value ds trend trend_lower trend_upper yhat yhat_lower yhat_upper 2018-01-16 0.40 0.40 0.40 0.24 0.18 0.30 2010-07-16 0.36 0.36 0.36 0.26 0.20 0.33 2014-03-16 0.38 0.38 0.38 0.15 0.09 0.22 2005-07-16 0.39 0.39 0.39 0.30 0.24 0.36 2027-07-16 0.45 0.43 0.47 0.35 0.29 0.42 2014-06-16 0.38 0.38 0.38 0.27 0.20 0.33 2013-12-16 0.38 0.38 0.38 0.16 0.10 0.22 2023-06-16 0.43 0.42 0.43 0.30 0.24 0.36 2022-04-16 0.42 0.42 0.43 0.27 0.21 0.33 2010-12-16 0.36 0.36 0.36 0.14 0.08 0.20 18.5 Visualize the forecasted values The generic plot() function can be ussed to plot the predicted chlorophyll value. Note that the model must be supplied in as the first argument and the predicted as the second argument. The nice things is that it uses the ggplot2 framework of grammar of graphics to make this plot. Hence, we can take full control of the rich function of ggplot2 (Wickham 2016), to customize the plot as seen in figure 18.1. plot(m,future.chl) + # theme_bw()+ theme(axis.text = element_text(size = 12), axis.title = element_text(size = 14))+ scale_y_continuous(breaks = seq(0.1, 0.6, 0.1))+ labs(x = NULL, y = expression(Chlorophyll~concentration~(mgm^{-3}))) Figure 18.1: Predicted time series of chlorophyll concentration in the Pemba Channel We can visualize the components of predicted chlorophyll value shown in figure 18.2 with the prophet_plot_components() function: prophet_plot_components(m = m, fcst = future.chl) Figure 18.2: Time series of historical and predicted chlorophyll values decomposed into yearly, week, day and hours trends We can make an interactive plot of the predicted and historical concentration of chlorophyll as in figure 18.3 with the dyplot.prophet() function written as; dyplot.prophet(x = m, fcst = future.chl,uncertainty = TRUE ) Figure 18.3: Interactive plot showing the historical and predicted chlorophyll-value 18.6 conclusion We have seen that with few command line of function from prophet package, we can automatically forecast time series data based on additive models where non-linear trends are unfit. The package works well with time series from the Western Indian Ocean where there is strong effects of monsoon seasons caused by trande winds. References "],
["reading-netcdf.html", "Chapter 19 Reading NetCDF 19.1 Introduction 19.2 Types of netCDF file 19.3 Rading NetCDF with ncdf4 package", " Chapter 19 Reading NetCDF conflicted::conflict_prefer(&quot;print&quot;, &quot;raster&quot;) 19.1 Introduction Network Common Data Form (NetCDF) is a widely used format for storing array–based data as variables. NetCDF are developed and maintained by Unidata was originally developed for storing and distributiing climate data , such as those generatd by climate simulation or reanalysis models. It has also been adopted in other fields, particularly in oceanography, where large mutidimensional arrays of data are generatted from satellite observation systems. The NetCDF format is a platform-independent because can be transeerred among servers and coputers that are running different operating systems, without a need to convert the file that fit a particular sytem. The NetCDF is also self-describing—contains metadata that describe the what is contained in the file, like the dimensions of longitude and latitude of the grid, the names and units of variables in the dataset, and attributes that describe thingks like themissing values codesr, or offsets and scale facators that may have been used to compress the data. 19.2 Types of netCDF file There are two version of netCDF. The first is netCDF version 3 (netCDF3), which is widely used, but has some size and performance limitations. The other is netCDF version 4 (netCDF4), which supports larger dataset and includes addtional capabilities like to compress the file and reduce the file size. 19.3 Rading NetCDF with ncdf4 package The ncdf4 package allows reading, writing, and creation of netCDF files, either netCDF version 3 or (optionally) netCDF version 4. Let’s first load the packages we need. Note that the ncddf4 package must already be installed on your machine. require(ncdf4) require(magrittr) require(tidyverse) Once you have loaded the package, then use a function nc_open() to read an existing netCDF file # nc.file = nc_open(&quot;./data/ml_depth__0.2deg_mean_grid_all_global_locean.nc&quot;) nc.file = nc_open(&quot;e:/MatlabWorking/Altimetry/old staff/wio_ssh_july_2015.nc&quot;) Once we have opened the file, we can print it to have a glimpse of some basic information about the file nc.file File e:/MatlabWorking/Altimetry/old staff/wio_ssh_july_2015.nc (NC_FORMAT_CLASSIC): 2 variables (excluding dimension variables): int crs[] comment: This is a container variable that describes the grid_mapping used by the data in this file. This variable does not contain any data; only information about the geographic coordinate system. grid_mapping_name: latitude_longitude inverse_flattening: 298.257 semi_major_axis: 6378136.3 _CoordinateTransformType: Projection _CoordinateAxisTypes: GeoX GeoY int adt[lon,lat,time] _CoordinateAxes: lon lat time lat lon _FillValue: -2147483647 coordinates: lon lat grid_mapping: crs long_name: Absolute Dynamic Topography scale_factor: 1e-04 standard_name: sea_surface_height_above_geoid units: m 3 dimensions: time Size:31 axis: T long_name: Time standard_name: time units: days since 1950-01-01 00:00:00 _CoordinateAxisType: Time lat Size:420 axis: Y bounds: lat_bnds long_name: Latitude standard_name: latitude units: degrees_north _CoordinateAxisType: Lat lon Size:401 axis: X bounds: lon_bnds long_name: Longitude standard_name: longitude units: degrees_east _CoordinateAxisType: Lon 13 global attributes: title: NRT merged all satellites Global Ocean Gridded Absolute Dynamic Topography L4 product institution: CNES, CLS references: http://www.aviso.altimetry.fr source: Altimetry measurements Conventions: CF-1.0 history: Data extracted from dataset http://opendap.aviso.altimetry.fr/thredds/dodsC/dataset-duacs-nrt-over30d-global-allsat-madt-h time_min: 23922 time_max: 23952 julian_day_unit: days since 1950-01-01 00:00:00 latitude_min: -74.875 latitude_max: 29.875 longitude_min: 19.875 longitude_max: 119.875 By simply printing this nc.file file which is the returned object of nc_open() gave us a lot of information that is ovewhelming to grasp. This information describe about the file and is very important to understand the basic information embedded in it as we will require them later for extraction of the varibles. In summary, the metadata tells us about the; filename, which is a character string holding the name of the file; for our case is File ./data/ml_depth__0.2deg_mean_grid_all_global_locean.nc ndims, which is an integer holding the number of dimensions in the file; in our file there are three dimension of time',latandlon` nvars, which is an integer holding the number of the variables in the file that are NOT coordinate variables (aka dimensional variables); the printed file showed there are seven variables krig_std_dev, mask, med_dev, mld, mld_raw, mld_smth, n_profiles natts, which is an integer holding the number of global attributes; unlimdimid, which is an integer holding the dimension id of the unlimited dimension, or -1 if there is none; 6 dim, which is a list of objects of class ncdim4; var, which is a list of objects of class ncvar4; writable, which is TRUE or FALSE, depending on whether the file was opened with write=TRUE or write=FALSE. 19.3.1 Extracting the dimensions Based on the metadata, I can customize it to print only the information I need. I used the for loop to customize only the variable information I need shown. Here is the code that will print the dimensions in the files together with additional information for (j in 1:nc.file$ndims){ a = nc.file$dim[[j]] print(paste(&quot; Here is information on variable number&quot;,j)) print(paste(&quot; Name: &quot;,a$name)) print(paste(&quot; Units:&quot;,a$units)) print(paste(&quot; Length:&quot;,a$len)) print(paste(&quot; Values:&quot;)) print(a$vals) } [1] &quot; Here is information on variable number 1&quot; [1] &quot; Name: time&quot; [1] &quot; Units: days since 1950-01-01 00:00:00&quot; [1] &quot; Length: 31&quot; [1] &quot; Values:&quot; [1] 23922 23923 23924 23925 23926 23927 23928 23929 23930 23931 [11] 23932 23933 23934 23935 23936 23937 23938 23939 23940 23941 [21] 23942 23943 23944 23945 23946 23947 23948 23949 23950 23951 [31] 23952 [1] &quot; Here is information on variable number 2&quot; [1] &quot; Name: lat&quot; [1] &quot; Units: degrees_north&quot; [1] &quot; Length: 420&quot; [1] &quot; Values:&quot; [1] -74.875 -74.625 -74.375 -74.125 -73.875 -73.625 -73.375 [8] -73.125 -72.875 -72.625 -72.375 -72.125 -71.875 -71.625 [15] -71.375 -71.125 -70.875 -70.625 -70.375 -70.125 -69.875 [22] -69.625 -69.375 -69.125 -68.875 -68.625 -68.375 -68.125 [29] -67.875 -67.625 -67.375 -67.125 -66.875 -66.625 -66.375 [36] -66.125 -65.875 -65.625 -65.375 -65.125 -64.875 -64.625 [43] -64.375 -64.125 -63.875 -63.625 -63.375 -63.125 -62.875 [50] -62.625 -62.375 -62.125 -61.875 -61.625 -61.375 -61.125 [57] -60.875 -60.625 -60.375 -60.125 -59.875 -59.625 -59.375 [64] -59.125 -58.875 -58.625 -58.375 -58.125 -57.875 -57.625 [71] -57.375 -57.125 -56.875 -56.625 -56.375 -56.125 -55.875 [78] -55.625 -55.375 -55.125 -54.875 -54.625 -54.375 -54.125 [85] -53.875 -53.625 -53.375 -53.125 -52.875 -52.625 -52.375 [92] -52.125 -51.875 -51.625 -51.375 -51.125 -50.875 -50.625 [99] -50.375 -50.125 -49.875 -49.625 -49.375 -49.125 -48.875 [106] -48.625 -48.375 -48.125 -47.875 -47.625 -47.375 -47.125 [113] -46.875 -46.625 -46.375 -46.125 -45.875 -45.625 -45.375 [120] -45.125 -44.875 -44.625 -44.375 -44.125 -43.875 -43.625 [127] -43.375 -43.125 -42.875 -42.625 -42.375 -42.125 -41.875 [134] -41.625 -41.375 -41.125 -40.875 -40.625 -40.375 -40.125 [141] -39.875 -39.625 -39.375 -39.125 -38.875 -38.625 -38.375 [148] -38.125 -37.875 -37.625 -37.375 -37.125 -36.875 -36.625 [155] -36.375 -36.125 -35.875 -35.625 -35.375 -35.125 -34.875 [162] -34.625 -34.375 -34.125 -33.875 -33.625 -33.375 -33.125 [169] -32.875 -32.625 -32.375 -32.125 -31.875 -31.625 -31.375 [176] -31.125 -30.875 -30.625 -30.375 -30.125 -29.875 -29.625 [183] -29.375 -29.125 -28.875 -28.625 -28.375 -28.125 -27.875 [190] -27.625 -27.375 -27.125 -26.875 -26.625 -26.375 -26.125 [197] -25.875 -25.625 -25.375 -25.125 -24.875 -24.625 -24.375 [204] -24.125 -23.875 -23.625 -23.375 -23.125 -22.875 -22.625 [211] -22.375 -22.125 -21.875 -21.625 -21.375 -21.125 -20.875 [218] -20.625 -20.375 -20.125 -19.875 -19.625 -19.375 -19.125 [225] -18.875 -18.625 -18.375 -18.125 -17.875 -17.625 -17.375 [232] -17.125 -16.875 -16.625 -16.375 -16.125 -15.875 -15.625 [239] -15.375 -15.125 -14.875 -14.625 -14.375 -14.125 -13.875 [246] -13.625 -13.375 -13.125 -12.875 -12.625 -12.375 -12.125 [253] -11.875 -11.625 -11.375 -11.125 -10.875 -10.625 -10.375 [260] -10.125 -9.875 -9.625 -9.375 -9.125 -8.875 -8.625 [267] -8.375 -8.125 -7.875 -7.625 -7.375 -7.125 -6.875 [274] -6.625 -6.375 -6.125 -5.875 -5.625 -5.375 -5.125 [281] -4.875 -4.625 -4.375 -4.125 -3.875 -3.625 -3.375 [288] -3.125 -2.875 -2.625 -2.375 -2.125 -1.875 -1.625 [295] -1.375 -1.125 -0.875 -0.625 -0.375 -0.125 0.125 [302] 0.375 0.625 0.875 1.125 1.375 1.625 1.875 [309] 2.125 2.375 2.625 2.875 3.125 3.375 3.625 [316] 3.875 4.125 4.375 4.625 4.875 5.125 5.375 [323] 5.625 5.875 6.125 6.375 6.625 6.875 7.125 [330] 7.375 7.625 7.875 8.125 8.375 8.625 8.875 [337] 9.125 9.375 9.625 9.875 10.125 10.375 10.625 [344] 10.875 11.125 11.375 11.625 11.875 12.125 12.375 [351] 12.625 12.875 13.125 13.375 13.625 13.875 14.125 [358] 14.375 14.625 14.875 15.125 15.375 15.625 15.875 [365] 16.125 16.375 16.625 16.875 17.125 17.375 17.625 [372] 17.875 18.125 18.375 18.625 18.875 19.125 19.375 [379] 19.625 19.875 20.125 20.375 20.625 20.875 21.125 [386] 21.375 21.625 21.875 22.125 22.375 22.625 22.875 [393] 23.125 23.375 23.625 23.875 24.125 24.375 24.625 [400] 24.875 25.125 25.375 25.625 25.875 26.125 26.375 [407] 26.625 26.875 27.125 27.375 27.625 27.875 28.125 [414] 28.375 28.625 28.875 29.125 29.375 29.625 29.875 [1] &quot; Here is information on variable number 3&quot; [1] &quot; Name: lon&quot; [1] &quot; Units: degrees_east&quot; [1] &quot; Length: 401&quot; [1] &quot; Values:&quot; [1] 19.875 20.125 20.375 20.625 20.875 21.125 21.375 [8] 21.625 21.875 22.125 22.375 22.625 22.875 23.125 [15] 23.375 23.625 23.875 24.125 24.375 24.625 24.875 [22] 25.125 25.375 25.625 25.875 26.125 26.375 26.625 [29] 26.875 27.125 27.375 27.625 27.875 28.125 28.375 [36] 28.625 28.875 29.125 29.375 29.625 29.875 30.125 [43] 30.375 30.625 30.875 31.125 31.375 31.625 31.875 [50] 32.125 32.375 32.625 32.875 33.125 33.375 33.625 [57] 33.875 34.125 34.375 34.625 34.875 35.125 35.375 [64] 35.625 35.875 36.125 36.375 36.625 36.875 37.125 [71] 37.375 37.625 37.875 38.125 38.375 38.625 38.875 [78] 39.125 39.375 39.625 39.875 40.125 40.375 40.625 [85] 40.875 41.125 41.375 41.625 41.875 42.125 42.375 [92] 42.625 42.875 43.125 43.375 43.625 43.875 44.125 [99] 44.375 44.625 44.875 45.125 45.375 45.625 45.875 [106] 46.125 46.375 46.625 46.875 47.125 47.375 47.625 [113] 47.875 48.125 48.375 48.625 48.875 49.125 49.375 [120] 49.625 49.875 50.125 50.375 50.625 50.875 51.125 [127] 51.375 51.625 51.875 52.125 52.375 52.625 52.875 [134] 53.125 53.375 53.625 53.875 54.125 54.375 54.625 [141] 54.875 55.125 55.375 55.625 55.875 56.125 56.375 [148] 56.625 56.875 57.125 57.375 57.625 57.875 58.125 [155] 58.375 58.625 58.875 59.125 59.375 59.625 59.875 [162] 60.125 60.375 60.625 60.875 61.125 61.375 61.625 [169] 61.875 62.125 62.375 62.625 62.875 63.125 63.375 [176] 63.625 63.875 64.125 64.375 64.625 64.875 65.125 [183] 65.375 65.625 65.875 66.125 66.375 66.625 66.875 [190] 67.125 67.375 67.625 67.875 68.125 68.375 68.625 [197] 68.875 69.125 69.375 69.625 69.875 70.125 70.375 [204] 70.625 70.875 71.125 71.375 71.625 71.875 72.125 [211] 72.375 72.625 72.875 73.125 73.375 73.625 73.875 [218] 74.125 74.375 74.625 74.875 75.125 75.375 75.625 [225] 75.875 76.125 76.375 76.625 76.875 77.125 77.375 [232] 77.625 77.875 78.125 78.375 78.625 78.875 79.125 [239] 79.375 79.625 79.875 80.125 80.375 80.625 80.875 [246] 81.125 81.375 81.625 81.875 82.125 82.375 82.625 [253] 82.875 83.125 83.375 83.625 83.875 84.125 84.375 [260] 84.625 84.875 85.125 85.375 85.625 85.875 86.125 [267] 86.375 86.625 86.875 87.125 87.375 87.625 87.875 [274] 88.125 88.375 88.625 88.875 89.125 89.375 89.625 [281] 89.875 90.125 90.375 90.625 90.875 91.125 91.375 [288] 91.625 91.875 92.125 92.375 92.625 92.875 93.125 [295] 93.375 93.625 93.875 94.125 94.375 94.625 94.875 [302] 95.125 95.375 95.625 95.875 96.125 96.375 96.625 [309] 96.875 97.125 97.375 97.625 97.875 98.125 98.375 [316] 98.625 98.875 99.125 99.375 99.625 99.875 100.125 [323] 100.375 100.625 100.875 101.125 101.375 101.625 101.875 [330] 102.125 102.375 102.625 102.875 103.125 103.375 103.625 [337] 103.875 104.125 104.375 104.625 104.875 105.125 105.375 [344] 105.625 105.875 106.125 106.375 106.625 106.875 107.125 [351] 107.375 107.625 107.875 108.125 108.375 108.625 108.875 [358] 109.125 109.375 109.625 109.875 110.125 110.375 110.625 [365] 110.875 111.125 111.375 111.625 111.875 112.125 112.375 [372] 112.625 112.875 113.125 113.375 113.625 113.875 114.125 [379] 114.375 114.625 114.875 115.125 115.375 115.625 115.875 [386] 116.125 116.375 116.625 116.875 117.125 117.375 117.625 [393] 117.875 118.125 118.375 118.625 118.875 119.125 119.375 [400] 119.625 119.875 The output now give us the basic information of the variable name, the units, the length of the variable and the values. This is very simplified version and make more sense to grasp easily the information contained in the file. we can now extract the dimensions. 19.3.2 Extracting time The dimensions are extracted with the ncvar_get() function and parse along the arguments required. But before we extract let’s run the loop to have a glimpse again of the structure of the time dimension [1] &quot; Here is information on variable number 1&quot; [1] &quot; Name: time&quot; [1] &quot; Units: days since 1950-01-01 00:00:00&quot; [1] &quot; Length: 31&quot; [1] &quot; Values:&quot; [1] 23922 23923 23924 23925 23926 23927 23928 23929 23930 23931 [11] 23932 23933 23934 23935 23936 23937 23938 23939 23940 23941 [21] 23942 23943 23944 23945 23946 23947 23948 23949 23950 23951 [31] 23952 time = ncvar_get(nc = nc.file, varid = &quot;time&quot;) We notice that time is in numerical number—julian days. Using the original time 1950-01-01 00:00:00, we can transform julian date into gregorian calender.Since the time we are given is in julian, We need to convert the original time to julian to ensure its in the same format with time. Then to get the real time, we add up the original time to time and convert them from julian to gregorian. We used JD function from insol package for conversion of date between Julian and Gregorian (Corripio 2014) to = insol::JD(lubridate::ymd_hms(&quot;1950-01-01 00:00:00&quot;, tz = &quot;&quot;)) jd = time + to date = insol::JD(x = jd, inverse = TRUE) 19.3.3 Extracting Latitude Similary, we can have a glimpse of latidude with the loop code as; [1] &quot; Here is information on variable number 2&quot; [1] &quot; Name: lat&quot; [1] &quot; Units: degrees_north&quot; [1] &quot; Length: 420&quot; We notice the latitude values span from 88°S to 89.5°N.We extract with the ncget_var() function; lat = ncvar_get(nc = nc.file, varid = &quot;lat&quot;) 19.3.4 Extracting Longitude [1] &quot; Here is information on variable number 3&quot; [1] &quot; Name: lon&quot; [1] &quot; Units: degrees_east&quot; [1] &quot; Length: 401&quot; Unlike the convention range of longitude which span from 180°W to 180°, here we see that longitud range from 0 to 360. Let’s extract the longitude values with the ncvar_get() function. lon = ncvar_get(nc = nc.file, varid = &quot;lon&quot;) 19.3.5 Extracting the variable As we have seen, obtaining the information from the metadata printout more information that we need, we can customize how we want the information to be printed with a loop. For instance the code of lines in the chunk below show how to printout information about the variables embbed in the file. Here is the code for( i in 1:nc.file$nvars ) { v &lt;- nc.file$var[[i]] print(paste(&quot; Here is information on variable number&quot;,i)) print(paste(&quot; Name: &quot;,v$name)) print(paste(&quot; Units:&quot;,v$units)) print(paste(&quot; Missing value:&quot;,v$missval)) print(paste(&quot;# dimensions :&quot;,v$ndims)) print(paste(&quot; Variable size:&quot;,v$varsize)) } [1] &quot; Here is information on variable number 1&quot; [1] &quot; Name: crs&quot; [1] &quot; Units: &quot; [1] &quot; Missing value: NA&quot; [1] &quot;# dimensions : 0&quot; [1] &quot; Variable size: &quot; [1] &quot; Here is information on variable number 2&quot; [1] &quot; Name: adt&quot; [1] &quot; Units: m&quot; [1] &quot; Missing value: -2147483647&quot; [1] &quot;# dimensions : 3&quot; [1] &quot; Variable size: 401&quot; &quot; Variable size: 420&quot; [3] &quot; Variable size: 31&quot; We notice also there are two variables. here we are only interested with the variable 2, which is the sea surface height in meters. Let’s extract the variable adt.array = ncvar_get(nc = nc.file, varid = &quot;adt&quot;) Once we have extracted the time, lat ,lon and mld, we need to verify their dimension. The dimension of atomic vector lon,lat and time must correspond to the dimension of the mld array. We can check the length of the lon, lat, time with the length() length(lon);length(lat);length(time) [1] 401 [1] 420 [1] 31 The output display that there are 180 longitude, 90 latitude and 12 months. To check for dimension of the array we use the dim() function instead of the length() dim(adt.array) [1] 401 420 31 That’s perfect, the length of the dimensions matches the dimension of the mld array 19.3.6 Replace FillValue with NA In a netCDF file, values of a variable that are either missing or simply not available (i.e. ocean grid points in a terrestrial data set) are flagged using specific “fill values” (_FillValue) or missing values (missing_value), the values of which are set as attributes of a variable. In R, such unavailable data are indicated using the “NA” value. We can explore the value of the mld with the hist() adt.array %&gt;% hist() The histogram display only two bars, which indicated the skewness of the data. Looking back on the metadata, we observed that there the value 1e+09 was used to mask the land. Therefore, in R, we need to set all pixel with the 1e+09 values to NA. The following code fragment illustrates how to replace the netCDF variable’s fill values with R NA’s . adt.array[adt.array == v$missval] = NA We can explore again the value of the mld with the hist() functions and the values now look good adt.array %&gt;% hist() 19.3.7 Obtain a slice NetCDF variables are read and written as one-dimensional vectors (e.g. longitudes), two-dimensional arrays or matrices (raster “slices”), or multi-dimensional arrays (raster “bricks”). In such data structures, the coordinate values for each grid point are implicit, inferred from the marginal values of, for example, longitude, latitude and time. In contrast, in R, the principal data structure for a variable is the data.frame.For instance, the adt.array data file is the multidimensional array consist of longitude, latitude and 12 columns of long-term means for each month, with the full data set thus consisting of 5221020 rows (401 by length(lat)) and 12 columns. adt.array %&gt;% dim() [1] 401 420 31 180*90 [1] 16200 In the kinds of data sets usually stored as netCDF files, each row in the data frame will contain the data for an individual grid point, with each column representing a particular variable, including explicit values for longitude and latitude (and perhaps time). This particular structure of this data set can be illustrated by selecting a single slice from the adt.array “brick”. Therefore, you need to convert extract matrices from array by indexing. For instance, we can extract the january matrix by typing first = adt.array[,,1] once we have the mld for january, we can map the spatial pattern of the mixed layer depth with the imagep() function from oce package. Note that the values were normalized with the `inverse_hyperbolic(), function. oce::imagep(x = lon,y = lat, z = first, xlim = c(37,52), ylim = c(-17,2), zlim = c(0.55,1.3), zclip = TRUE, filledContour = FALSE, col = oce::oceColors9A(120), # at = seq(3.5,6.5,.5), # labels = seq(20,500, length.out = 7), xlab = &quot;Longitude&quot;, ylab = &quot;Latitude&quot;, zlab = &quot;Sea Surface Height Anomaly (m)&quot;, zlabPosition = &quot;side&quot;) ## expand the longitude and latitude lon.lat = expand.grid(lon,lat) ## convert the january matrix to a vector first.vector = first %&gt;% as.vector() ## combine the expanded lon.lat with vectorized matrix of january first.df = data.frame(lon.lat, first.vector) %&gt;% tibble::as_tibble() %&gt;% dplyr::rename(lon = 1, lat = 2, ssha= 3) wio = spData::world %&gt;% sf::st_crop(xmin = 20, ymin = -80,xmax = 130, ymax =30) ssha.eacc = first.df %&gt;% filter(lon &gt; 30 &amp; lon &lt; 53 &amp; lat &gt; -19 &amp; lat &lt; 3) ggplot()+ geom_sf(data =wio) + metR::geom_contour_fill(data = ssha.eacc, aes(x = lon, y = lat, z =ssha), bins = 28, na.fill = TRUE) + scale_fill_gradientn(colors = oce::oce.colors9A(120))+ geom_sf(data =wio) + coord_sf(xlim = c(37,51), ylim = c(-17,2), datum = sf::st_crs(4326)) + metR::scale_x_latitude(ticks = 5) + metR::scale_x_longitude(ticks = 4)+ labs(x = NULL, y = NULL, subtitle = paste(&quot;Sea surface height of &quot;, date[1]))+ theme_bw() %+% theme(axis.text = element_text(size = 11)) + guides(fill = guide_colorbar(barheight = 15, barwidth = .85, raster = FALSE, title = &quot;Sea surface height (m)&quot;, title.position = &quot;right&quot;, title.hjust = .5, title.theme = element_text(angle = 90))) ## expand the longitude and latitude lon.lat = expand.grid(lon,lat) ## preallocate ssha = list() for (j in 1:length(date)){ ## chop the blick data = adt.array[,,j] ## create a data frame ssha[[j]] = data.frame(lon.lat, first.vector) %&gt;% tibble::as_tibble() %&gt;% dplyr::rename(lon = 1, lat = 2, ssha= 3) %&gt;% dplyr::mutate(date = date[j]) } ssha = ssha %&gt;% dplyr::bind_rows() %&gt;% dplyr::filter(lon &gt; 30 &amp; lon &lt; 53 &amp; lat &gt; -19 &amp; lat &lt; 3) %&gt;% dplyr::mutate(day = lubridate::day(date), jd = lubridate::yday(date) %&gt;% as.integer(), week = lubridate::week(date)) References "],
["packages-2.html", "Chapter 20 packages 20.1 One Sample t Test 20.2 Paired t test 20.3 Levene’s Test", " Chapter 20 packages Rsquared Academy uses R packages for teaching business analytics/ data science courses. In the process, we created a few packages and would like to share them with the community in the hope that others will find it useful in teaching and learning data science with R. knitr::include_graphics(&quot;./images/rsquared_academy_packages.png&quot;) rsquare.pkgs = c(&quot;olsrr&quot;, &quot;descriptr&quot;, &quot;inferr&quot;, &quot;blorr&quot;, &quot;vistributions&quot;, &quot;rbin&quot;, &quot;xplorerr&quot;) install.packages(rsquare.pkgs) devtools::install_github(&quot;rsquaredacademy/pkginfo&quot;) 20.1 One Sample t Test A one sample t-test is used to determine whether a sample of observations comes from a population with a specific mean. The observations must be continuous, independent of each other, approximately distributed and should not contain any outliers. Example sst.season %&gt;% na.omit %&gt;% inferr::infer_os_t_test(x = sst, mu = 27, alternative = &quot;all&quot;) One-Sample Statistics ----------------------------------------------------------------------------------- Variable Obs Mean Std. Err. Std. Dev. [95% Conf. Interval] ----------------------------------------------------------------------------------- sst 24112 28.164 0.0091 1.4207 28.1462 28.1818 ----------------------------------------------------------------------------------- Ho: mean(sst) ~=27 Ha: mean &lt; 27 Ha: mean ~= 27 Ha: mean &gt; 27 t = 127.912 t = 127.912 t = 127.912 P &lt; t = 1.0000 P &gt; |t| = 0.0000 P &gt; t = 0.0000 20.2 Paired t test A paired (samples) t-test is used when you want to compare the means between two related groups of observations on some continuous dependent variable. In a paired sample test, each subject or entity is measured twice. It can be used to evaluate the effectiveness of training programs or treatments. If the dependent variable is dichotomous, use the McNemar test. sst.season %&gt;% dplyr::filter(date &gt;= lubridate::dmy(010115) &amp; date &lt; lubridate::dmy(010119)) %&gt;% dplyr::mutate(year = lubridate::year(date)) %&gt;% dplyr::group_by(year,month) %&gt;% dplyr::summarise(sst = mean(sst, na.rm = TRUE)) %&gt;% tidyr::spread(key = &quot;year&quot;, value = &quot;sst&quot;) %&gt;% dplyr::rename(first = 2, last = 5) %&gt;% inferr::infer_ts_paired_ttest(x = first, y = last, alternative = &quot;less&quot;) Paired Samples Statistics ------------------------------------------------------------------------------ Variables Obs Mean Std. Err. Std. Dev. [95% Conf. Interval] ------------------------------------------------------------------------------ first 12 27.94286 0.3613807 1.251859 27.15 28.74 last 12 27.98805 0.3923361 1.359092 27.12 28.85 ------------------------------------------------------------------------------ diff 12 -0.04519391 0.1444083 0.500245 -0.36 0.27 ------------------------------------------------------------------------------ Paired Samples Correlations ------------------------------------------- Variables Obs Correlation Sig. first &amp; last 12 0.93 0 ------------------------------------------- Paired Samples Test ------------------- Ho: mean(first - last) = 0 Ha: mean(first - last) &lt; 0 -------------------------------------- Variables t df Sig. -------------------------------------- first - last -0.313 11 0.380 -------------------------------------- Two Independent Sample t Test An independent samples t-test is used to compare the means of a normally distributed continuous dependent variable for two unrelated groups. The dependent variable must be approximately normally distributed and the cases/subjects in the two groups must be different i.e. a subject in one group cannot also be a subject of the other group. It can be used to answer whether: average number of products produced by two machines differ significantly? average salaries of graduate students differ based on gender? sst.season %&gt;% na.omit() %&gt;% dplyr::filter(date &gt;= lubridate::dmy(010115) &amp; date &lt; lubridate::dmy(010119) &amp; season != &quot;IN&quot;) %&gt;% dplyr::mutate(season = as.factor(season)) %&gt;% inferr::infer_ts_ind_ttest(x = season, y = sst, alternative = &quot;all&quot;) Group Statistics ------------------------------------------------------------------------------- Group Obs Mean Std. Err. Std. Dev. [95% Conf. Interval] ------------------------------------------------------------------------------- NE 8916 28.924 0.009 0.830 28.90636 28.94164 SE 8940 26.867 0.010 0.922 26.84740 26.88660 ------------------------------------------------------------------------------- combined 17856 27.894 0.01 1.352 27.8744 27.9136 ------------------------------------------------------------------------------- diff 17856 2.057 0.013 0.877 2.03126 2.08274 ------------------------------------------------------------------------------- Independent Samples Test ------------------------ Ho: mean(NE) - mean(SE) = diff = 0 Ha: diff &lt; 0 Ha: diff ~= 0 Ha: diff &gt; 0 Pooled ------------------------------------------------------------------------ t = 156.6619 t = 156.6619 t = 156.6619 P &lt; t = 1.0000 P &gt; |t| = 0.0000 P &gt; t = 0.0000 Satterthwaite ------------------------------------------------------------------------ t = 156.6839 t = 156.6839 t = 156.6839 P &lt; t = 1.0000 P &gt; |t| = 0.0000 P &gt; t = 0.0000 Test for Equality of Variances -------------------------------------------------------------- Variable Method Num DF Den DF F Value P &gt; F -------------------------------------------------------------- sst Folded F 8915 8939 0.8104 0 -------------------------------------------------------------- One sample test of proportion compares proportion in one group to a specified population proportion. sst.season %&gt;% na.omit() %&gt;% dplyr::filter(date &gt;= lubridate::dmy(010115) &amp; date &lt; lubridate::dmy(010119) &amp; season != &quot;IN&quot;) %&gt;% inferr::infer_os_prop_test(variable = season, pro = 0.5) Test Statistics ------------------------- Sample Size 17856 Exp Prop 0.5 Obs Prop 0.5007 z 0.1871 Pr(|Z| &gt; |z|) 0.8516 ----------------------------------------------------------------- Category Observed Expected % Deviation Std. Residuals ----------------------------------------------------------------- 0 8915.5008 8928 -0.14 -0.13 1 8940.4992 8928 0.14 0.13 ----------------------------------------------------------------- ANOVA The one-way analysis of variance (ANOVA) is used to determine whether there are any statistically significant differences between the means of two or more independent (unrelated) groups. It tests the null hypothesis that samples in two or more groups are drawn from populations with the same mean values. It cannot tell you which specific groups were statistically significantly different from each other but only that at least two groups were different and can be used only for numerical data. Examples Using the sst.season, test whether the mean of sst differs between the three monsoon seasons. sst.season %&gt;% na.omit() %&gt;% dplyr::filter(date &gt;= lubridate::dmy(010115) &amp; date &lt; lubridate::dmy(010119)) %&gt;% dplyr::mutate(season = as.factor(season)) %&gt;% inferr::infer_oneway_anova(x = sst, y = season) ANOVA -------------------------------------------------------------------------- Sum of Squares DF Mean Square F Sig. -------------------------------------------------------------------------- Between Groups 20270.069 2 10135.034 9570.381 0.0000 Within Groups 22700.163 21431 1.059 Total 42970.232 21433 -------------------------------------------------------------------------- Report ------------------------------------------ Category N Mean Std. Dev. ------------------------------------------ IN 3578 28.575 1.583 NE 8916 28.924 0.830 SE 8940 26.867 0.922 ------------------------------------------ Number of obs = 21434 R-squared = 0.4717 Root MSE = 1.0292 Adj R-squared = 0.4717 20.3 Levene’s Test Levene’s test is used to determine if k samples have equal variances. It is less sensitive to departures from normality and is an alternative to Bartlett’s test. This test returns Levene’s robust test statistic and the two statistics proposed by Brown and Forsythe that replace the mean in Levene’s formula with alternative location estimators. The first alternative replaces the mean with the median and the second alternative replaces the mean with the 10% trimmed mean. Examples Using the sst.season data, test whether variance in sst is same across seasons sst.season %&gt;% na.omit() %&gt;% dplyr::filter(date &gt;= lubridate::dmy(010115) &amp; date &lt; lubridate::dmy(010119)) %&gt;% dplyr::mutate(season = as.factor(season)) %&gt;% inferr::infer_levene_test(x = sst, group_var = season) Summary Statistics Levels Frequency Mean Std. Dev ----------------------------------------- IN 3578 28.58 1.58 NE 8916 28.92 0.83 SE 8940 26.87 0.92 ----------------------------------------- Total 21434 28.01 1.42 ----------------------------------------- Test Statistics ---------------------------------------------------------------------------- Statistic Num DF Den DF F Pr &gt; F ---------------------------------------------------------------------------- Brown and Forsythe 2 21431 3112.855 0 Levene 2 21431 2271.514 0 Brown and Forsythe (Trimmed Mean) 2 21431 3065.65 0 ---------------------------------------------------------------------------- inferr::hsb id female race ses schtyp prog read write math science socst 1 70 0 4 1 1 1 57 52 41 47 57 2 121 1 4 2 1 3 68 59 53 63 61 3 86 0 4 3 1 1 44 33 54 58 31 4 141 0 4 3 1 3 63 44 47 53 56 5 172 0 4 2 1 2 47 52 57 53 61 6 113 0 4 2 1 2 44 52 51 63 61 7 50 0 3 2 1 1 50 59 42 53 61 8 11 0 1 2 1 2 34 46 45 39 36 9 84 0 4 2 1 1 63 57 54 58 51 10 48 0 3 2 1 2 57 55 52 50 51 11 75 0 4 2 1 3 60 46 51 53 61 12 60 0 4 2 1 2 57 65 51 63 61 13 95 0 4 3 1 2 73 60 71 61 71 14 104 0 4 3 1 2 54 63 57 55 46 15 38 0 3 1 1 2 45 57 50 31 56 16 115 0 4 1 1 1 42 49 43 50 56 17 76 0 4 3 1 2 47 52 51 50 56 18 195 0 4 2 2 1 57 57 60 58 56 19 114 0 4 3 1 2 68 65 62 55 61 20 85 0 4 2 1 1 55 39 57 53 46 21 167 0 4 2 1 1 63 49 35 66 41 22 143 0 4 2 1 3 63 63 75 72 66 23 41 0 3 2 1 2 50 40 45 55 56 24 20 0 1 3 1 2 60 52 57 61 61 25 12 0 1 2 1 3 37 44 45 39 46 26 53 0 3 2 1 3 34 37 46 39 31 27 154 0 4 3 1 2 65 65 66 61 66 28 178 0 4 2 2 3 47 57 57 58 46 29 196 0 4 3 2 2 44 38 49 39 46 30 29 0 2 1 1 1 52 44 49 55 41 31 126 0 4 2 1 1 42 31 57 47 51 32 103 0 4 3 1 2 76 52 64 64 61 33 192 0 4 3 2 2 65 67 63 66 71 34 150 0 4 2 1 3 42 41 57 72 31 35 199 0 4 3 2 2 52 59 50 61 61 36 144 0 4 3 1 1 60 65 58 61 66 37 200 0 4 2 2 2 68 54 75 66 66 38 80 0 4 3 1 2 65 62 68 66 66 39 16 0 1 1 1 3 47 31 44 36 36 40 153 0 4 2 1 3 39 31 40 39 51 41 176 0 4 2 2 2 47 47 41 42 51 42 177 0 4 2 2 2 55 59 62 58 51 43 168 0 4 2 1 2 52 54 57 55 51 44 40 0 3 1 1 1 42 41 43 50 41 45 62 0 4 3 1 1 65 65 48 63 66 46 169 0 4 1 1 1 55 59 63 69 46 47 49 0 3 3 1 3 50 40 39 49 47 48 136 0 4 2 1 2 65 59 70 63 51 49 189 0 4 2 2 2 47 59 63 53 46 50 7 0 1 2 1 2 57 54 59 47 51 51 27 0 2 2 1 2 53 61 61 57 56 52 128 0 4 3 1 2 39 33 38 47 41 53 21 0 1 2 1 1 44 44 61 50 46 54 183 0 4 2 2 2 63 59 49 55 71 55 132 0 4 2 1 2 73 62 73 69 66 56 15 0 1 3 1 3 39 39 44 26 42 57 67 0 4 1 1 3 37 37 42 33 32 58 22 0 1 2 1 3 42 39 39 56 46 59 185 0 4 2 2 2 63 57 55 58 41 60 9 0 1 2 1 3 48 49 52 44 51 61 181 0 4 2 2 2 50 46 45 58 61 62 170 0 4 3 1 2 47 62 61 69 66 63 134 0 4 1 1 1 44 44 39 34 46 64 108 0 4 2 1 1 34 33 41 36 36 65 197 0 4 3 2 2 50 42 50 36 61 66 140 0 4 2 1 3 44 41 40 50 26 67 171 0 4 2 1 2 60 54 60 55 66 68 107 0 4 1 1 3 47 39 47 42 26 69 81 0 4 1 1 2 63 43 59 65 44 70 18 0 1 2 1 3 50 33 49 44 36 71 155 0 4 2 1 1 44 44 46 39 51 72 97 0 4 3 1 2 60 54 58 58 61 73 68 0 4 2 1 2 73 67 71 63 66 74 157 0 4 2 1 1 68 59 58 74 66 75 56 0 4 2 1 3 55 45 46 58 51 76 5 0 1 1 1 2 47 40 43 45 31 77 159 0 4 3 1 2 55 61 54 49 61 78 123 0 4 3 1 1 68 59 56 63 66 79 164 0 4 2 1 3 31 36 46 39 46 80 14 0 1 3 1 2 47 41 54 42 56 81 127 0 4 3 1 2 63 59 57 55 56 82 165 0 4 1 1 3 36 49 54 61 36 83 174 0 4 2 2 2 68 59 71 66 56 84 3 0 1 1 1 2 63 65 48 63 56 85 58 0 4 2 1 3 55 41 40 44 41 86 146 0 4 3 1 2 55 62 64 63 66 87 102 0 4 3 1 2 52 41 51 53 56 88 117 0 4 3 1 3 34 49 39 42 56 89 133 0 4 2 1 3 50 31 40 34 31 90 94 0 4 3 1 2 55 49 61 61 56 [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 110 rows ] "],
["references.html", "References", " References "]
]
